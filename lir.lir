---
title: 'Reproducible computing with `lir`'
author: Boris Vassilev
version: 0.3
<<:eval>>=
echo "date: $(date --utc --iso-8601)"
@
rights: (c) 2015 Boris Vassilev, University of Helsinki
...

This is a collection of tools that support programming language-agnostic reproducible computing.

This document is self-hosting: the implementation presented here can be used to produce the final human-readable version of itself.
This makes informal validation trivial ("does the final document look right?"), but it also means that the only guarantee is that it implements enough to be able to compile itself.

# Installation
These are a few paths that would probably be different for each computer.
Those have to be redefined before compiling this document.

## Library paths
<<Path to `lir`>>=
"$HOME"/lib/lir
<<Path to `lir` binaries>>=
"$HOME"/bin
<<Path to `noweb`>>=
"$HOME"/lib/noweb
@

## Making and installing
<<build.sh>>=
<<Bash header>>

echo "*** Making and Installing..."
notangle -t8 -filter emptydefn -R"Makefile" lir.lir > Makefile
export LIRPATH=<<Path to `lir`>>
export LIRBINPATH=<<Path to `lir` binaries>>
export NOWEBPATH=<<Path to `noweb`>>

make uninstall
make clean
make
make install
echo "*** Done!"

rm -rf .lir
echo "*** Using lir to tangle and weave lir..."
lir tangle lir.lir \
    && lir make lir.lir \
    && lir weave lir.lir
echo "*** Weaved document is in lir.html!"
@

## Makefile
<<Makefile>>=
nwpipemodules = driver.pl nwpipe.pl lirhtml.pl yaml.pl

nwpipesources = nwpipe-pandoc.pl $(nwpipemodules)

tangled = lir lir-weave $(nwpipesources) lir.css

all : lir.lir
	$(NOWEBPATH)/markup -t lir.lir \
	    | $(NOWEBPATH)/emptydefn \
	    | $(NOWEBPATH)/mnt -t $(tangled)
	swipl --goal=main -o nwpipe-pandoc -c nwpipe-pandoc.pl
	chmod u+x lir lir-weave

install :
	mkdir --parents $(LIRPATH)
	cp --verbose --preserve \
	    lir.css nwpipe-pandoc lir-weave $(LIRPATH)
	mkdir --parents $(LIRBINPATH)
	cp --verbose --preserve \
	    lir $(LIRBINPATH)
.PHONY : install

clean :
	-rm $(tangled) nwpipe-pandoc
.PHONY : clean

uninstall :
	-rm -r $(LIRPATH)
	-rm $(LIRBINPATH)/lir
.PHONY : uninstall
@

## Bash scripts
The portable [[<<Bash header>>]] is, according to [Stack Overflow](http://stackoverflow.com/questions/10376206/preferred-bash-shebang):
<<Bash header>>=
#! /usr/bin/env bash
@
Furthermore, we set the scripts to immediately exit with error on failure, consider failure in a pipe a failure, and to consider "empty" (unset) variables an error:
<<Bash header>>=
set -o errexit
set -o pipefail
set -o nounset
@

# Overview
The tools provided by `lir` support two goals:

1. Generating results by running the code in a `lir` source file.
2. Presenting all code and possibly results in a human-readable form.

This is usually done in three independent steps: _tangling_, _making_, and _weaving_.

  * **Tangling**: Extract all source code from the `lir` source file and bring in external dependencies.

    This might be scripts that are immediately executable, or code that needs to be compiled.

    For externally maintained files to be managed by `lir`, they need to be explicitly declared in the `lir` source file.
    Example use cases would be input data files, or a bibliography file maintained by a reference manager.

  * **Making**: Generate all results.

    This is a step that usually will include resolving dependencies.
    In the case of intermediate analysis steps, a final result will depend on an intermediate result.
    For example, a figure might be generated from the results of an analysis of the input data.

    In the case of programs that need to be compiled, a compilation step will have to occur before actually running the program.

  * **Weaving**: Compile the final documentation.

    Once all results and figures have been generated, the original `lir` source file is processed to obtain a valid Pandoc markdown source file.
    At that point, Pandoc is used to compile this to a final human readable file with any results and figures generated in the previous step.

## The wrapper script
The program is installed as a script that can be invoked with one of the three actions, `tangle`, `make`, or `weave`, as the first argument.
<<lir>>=
<<lir.bash>>
<<lir.bash>>=
<<Bash header>>
<<Define functions used by `lir`>>

if [ "$#" == "0" ]
then
    :
else

LIR_SOURCE="$2"
LIR_DIR=.lir
MORE_OPTS="${@:2}"
BASENAME=$(basename "$LIR_SOURCE" .lir)

case "$1" in
tangle)
    mkdir --verbose --parents "$LIR_DIR"
    tmpdir=$(mktemp --directory -p "$LIR_DIR")

    STATE_SOURCE="$LIR_DIR"/"$LIR_SOURCE"
    awk '<<Nameless code chunks as continuations .awk>>' "$LIR_SOURCE" \
        | awk '<<Close all codechunks with `@` .awk>>' \
        | awk -v evalcmd="bash -o errexit -o pipefail -o nounset" \
              '<<Evaluate code chunk contents .awk>>' \
        > "$STATE_SOURCE"

    noroots "$STATE_SOURCE" \
        | <<Drop double angle brackets>> \
        | <<Remove empty lines>> \
        > "$tmpdir"/rootchunks
    <<Remove special chunknames>> "$tmpdir"/rootchunks \
        > "$tmpdir"/filechunks
    <<Get source chunknames>> "$tmpdir"/rootchunks \
        > "$tmpdir"/sourcechunks
    <<Get sink chunknames>> "$tmpdir"/rootchunks \
        > "$tmpdir"/sinkchunks
    
    < "$tmpdir"/filechunks lir-mtangle "$STATE_SOURCE" "$LIR_DIR"
    < "$tmpdir"/sourcechunks lir-use "$LIR_DIR"

    if grep --quiet --line-regexp ':make' "$tmpdir"/rootchunks
    then
        notangle -t8 -R':make' "$STATE_SOURCE" \
            > "$LIR_DIR"/.makedag
    else
        echo '' > "$LIR_DIR"/.makedag
    fi

    echo -n 'all: ' > "$LIR_DIR"/.makeall
    < "$tmpdir"/sinkchunks tr '\n' ' ' >> "$LIR_DIR"/.makeall
    echo ";" >> "$LIR_DIR"/.makeall

    echo -n "$BASENAME.html: $BASENAME.lir " > "$LIR_DIR"/.makehtml
    < "$tmpdir"/sinkchunks tr '\n' ' ' >> "$LIR_DIR"/.makehtml
    echo ";" >> "$LIR_DIR"/.makehtml
    echo -e "\t<<Path to `lir`>>"'/lir-weave < $< > $@' >> "$LIR_DIR"/.makehtml

    rm -r "$tmpdir"
    ;;
make)
    make --jobs --directory="$LIR_DIR" \
        --makefile=.makeall --makefile=.makedag \
        all
    ;;
weave)
    make --directory="$LIR_DIR" \
        --makefile=.makehtml \
        "$BASENAME".html
    cp --verbose --update ".lir/$BASENAME.html" .
    ;;
purge)
    rm --recursive --one-file-system "$LIR_DIR"
    ;;
*)
    echo "ERROR: unknown command $1"
    ;;
esac

fi # if [ "$#" == "0" ]
@

## Tangling
<<Define functions used by `lir`>>=
<<lir-use>>
<<lir-tangle>>
<<lir-mtangle>>
<<lir-cmpcp>>
@

<<:source lir.bib>>=
@

A helper function [[<<lir-tangle>>]] accepts three arguments, in this order: (1) the name of a source file; (2) the name of a chunk; and (3) the name of the file that the chunk will be extracted to.
It uses `noweb`'s `notangle` to extract from (1) the chunk (2) to a temporary file, compares it to (3) and only writes (2) to (3) if (3) does not exist, or is different from the temporary file.
<<lir-tangle>>=
lir-tangle () {
    tmpfile=$(mktemp)
    notangle -t8 -R"$2" "$1" > "$tmpfile"
    lir-cmpcp "$tmpfile" "$3"
    rm "$tmpfile"
}
@

The function used, [[<<lir-cmpcp>>]], overwrites a file with a newer version of a file after comparing their contents.
It takes two arguments: (1) the new file, and (2) the old file.
The standard command line tool `cmp` used for the comparison will exit with status 0 (success) if the two files have identical contents; with status 1 if they are different, and with status 2 if there is some trouble: for example, if one of the files does not exist.
<<lir-cmpcp>>=
lir-cmpcp () {
if ! cmp --silent "$1" "$2"
then
    cp --verbose "$1" "$2"
fi
}
@

With the help of [[<<lir-tangle>>]] we implement [[<<lir-mtangle>>]] ("multiple" tangle).
It reads from standard input a list of chunk names, one per line, and tangles each chunk from the literate source file in (1) to a file of the same name as the chunk in the directory in (2).
<<lir-mtangle>>=
lir-mtangle () {
while IFS='' read -r chunk
do
    lir-tangle "$1" "$chunk" "$2/$chunk"
done
}
@

Not all data in an analysis naturally fits into the `lir` source file.
Examples already mentioned above are the input data, or an externally maintained file like a bibliography.

Taking in use such a file means simply making a symbolic link to the file in the working direcory.
_However_, keep in mind that the externally maintained file still needs to be in the project directory, and its name should on a path relative to the project directory.
The `make` utility will check the timestamp on the _target of the link_, so when a "used" file changes, this will be taken into consideration.

The argument to the function is the directory in which the symbolic links will be made.
The file names are read from standard input, one per line.
<<lir-use>>=
lir-use () {
while IFS='' read -r used
do
    useddir=$(dirname "$used")
    mkdir  --verbose --parents "$1/$useddir"
    original=$(readlink --canonicalize "$used")
    if [ -f "$original" ]
    then
        ln --symbolic --force "$original" "$1/$used"
    else
        >&2 echo "LIR ERROR: File not found! ($original)"
        exit 1
    fi
done
}
@

<<Drop double angle brackets>>=
sed -n 's/^@<<\(.*\)@>>$/\1/p'
<<Remove empty lines>>=
sed -n '/^\s*$/!p'
<<Remove special chunknames>>=
sed -n '/^:/!p'
<<Get source chunknames>>=
sed -n 's/^:source \(.\+\)$/\1/p'
<<Get sink chunknames>>=
sed -n 's/^:\(figure\|table\|listing\|result\) \(.\+\)/\2/p'
@

## Source transformations
<<Nameless code chunks as continuations .awk>>=
BEGIN {
    <<Setup `awk` to treat lines as single-field records>>
    last_name = "@<<>>="
}
/^@<<>>=$/ { $0 = last_name }
/^@<<.+>>=$/ { last_name = $0 }
{ print $0 }
<<Close all codechunks with `@` .awk>>=
BEGIN {
    <<Setup `awk` to treat lines as single-field records>>
    in_chunk = 0
}
/<<Begin code `awk` regex>>/ {
    if (in_chunk) print "@"
    in_chunk = 1
}
/<<End code `awk` regex>>/ { in_chunk = 0 }
{ print $0 }
@

The contents of each code chunk named `:eval` will be evaluated by an invocation of `bash`.
Whatever is written to standard output will be **pasted as it is** in place of the code chunk before anything else is done with the source file.
<<Evaluate code chunk contents .awk>>=
BEGIN {
    <<Setup `awk` to treat lines as single-field records>>
}
/^@<<:eval@>>=$/ {
    <<Execute contents as standard input of [[evalcmd]]>>
    <<Write output of [[evalcmd]] to awk's output>>
    next
}
{ print $0 }    
@

To make `awk` treat lines of input as records with one field each, set both the record and the field separator to the newline character.
With these settings, an empty line (nothing but a new line) will have 0 fields (`NF == 0`) and a length of 0 (`length($0) == 0`).
<<Setup `awk` to treat lines as single-field records>>=
RS="\n"
FS="\n"
@

<<Execute contents as standard input of [[evalcmd]]>>=
while (getline > 0 && $0 !~ /<<End code `awk` regex>>/)
    print $0 |& evalcmd
close(evalcmd, "to")
<<Write output of [[evalcmd]] to awk's output>>=
save_rs = RS
RS = "^$"
evalcmd |& getline x
close(evalcmd)
printf "%s", x
RS = save_rs
<<Begin code `awk` regex>>=
^@<<.*@>>=$
<<End code `awk` regex>>=
^@
@

## Weaving
Weaving means generating the final human readable documentation from the `lir` source file.
The original source is transformed by several filters before passed to Pandoc for generating an HTML document that can be viewed in a browser.
Everything is wrapped as a bash script that will be installed, along with [[<<lir>>]], in `LIR_BINPATH`.
It reads the `lir` source from standard input and writes the HTML to standard output.
<<lir-weave>>=
<<Bash header>>
export PANDOC_OPTS="${@:2}"
<<Weave `lir` source to HTML>>
<<Weave `lir` source to HTML>>=
<<Source to `noweb` pipeline representation>> \
    | <<`Noweb` pipeline representation to Pandoc source>> \
    | <<Pandoc source to HTML>>
@

A custom `noweb` pipeline is used, as we definitely want to be able to use "anonymous" chunks as continuations of the previous chunk.
Since tabs can be significant, for example in Makefiles, those are preserved (the `-t` option to `markup`).
The `-delay` option to `noidx` makes sure that the index of code chunks is emitted before the last document chunk.
This is necessary to be able to generate a bibliography with Pandoc.
<<Source to `noweb` pipeline representation>>=
"<<Path to `noweb`>>"/markup -t \
    | "<<Path to `noweb`>>"/noidx -delay
@

To make it marginally easier to deal with the pipeline representation, we completely remove any empty text tokens from it before tranforming it to Pandoc source.
We then use [[<<nwpipe-pandoc.pl>>]] to transform the `noweb` pipeline representation of the `lir` source file to a valid Pandoc source file.
<<`Noweb` pipeline representation to Pandoc source>>=
sed -n '/^@text $/!p' \
    | "<<Path to `lir`>>"/nwpipe-pandoc
@

<<Pandoc source to HTML>>=
pandoc "$@" \
    --preserve-tabs \
    --from=markdown \
    --table-of-contents \
    --standalone \
    --smart \
    --to=html5 \
    --css="<<Path to `lir`>>"/lir.css \
    --output=-
@

# `Lir` to Pandoc
The program [[<<nwpipe-pandoc.pl>>]] is written in Prolog, using the SWI-Prolog implementation [@Wielemaker2012].
It is written as a stand-alone command line program: once compiled, can be run from the command line as a part of a pipe.
It uses the module [[<<driver.pl>>]] to do the actual work.
<<nwpipe-pandoc.pl>>=
<<No banner or informational messages>>
<<Turn on error stack trace>>
<<Expand meta-predicates at compile time>>
<<Succeed once without choice points>>

:- use_module(driver, [nwpipeline_pandoc/1]).

main :-
    current_prolog_flag(argv, Argv), % command line arguments
    goal_is_det(
        nwpipeline_pandoc(Argv)),
    halt. % exit with success
main :-
    throw(error(evaluation_error("nwpipe-pandoc failed"),_)),
    halt(1). % otherwise, exit with failure (1)
@

<<Turn on error stack trace>>=
@

During development, it is good to know if the program has left behind any unintentional choice points -- they are certainly errors, as the program is designed to succeed deterministically.
They would be "hidden" by the `halt/0` used at the end of the main program.
<<Succeed once without choice points>>=
goal_is_det(Goal) :-
    setup_call_cleanup(true, Goal, Det = true),
    (   Det == true
    ->  true
    ;   !,
        throw(error(mode_error(notdet),_))
    ).
@


Prolog programs are usually used from the "top level" (the Prolog REPL), not as command-line tools.
We need to explicitly turn off the REPL banner and informational messages written to standard output.
<<No banner or informational messages>>=
:- set_prolog_flag(verbose, silent).
@

Meta-predicates (predicates that take other predicates as arguments) are fundamentally slow: they need to evaluate the passed predicate dynamically.
This can be avoided for the more commonly used meta-predicates (`maplist/1+N`, `forall/2` etc) by treating them as macros and expanding them at compile time.
<<Expand meta-predicates at compile time>>=
:- use_module(library(apply_macros)).
@

The driver uses [[<<nwpipe.pl>>]] to parse its input to native Prolog terms and [[<<lirhtml.pl>>]] to emit a valid Pandoc source document with embedded HTML fragments.
<<driver.pl>>=
:- module(driver, [nwpipeline_pandoc/1]).

:- use_module(nwpipe, [nwpipe_term/2]).
:- use_module(lirhtml, [emit_html/2]).

nwpipeline_pandoc(_) :-
    I = user_input,
    read_string(I, _, S),
    split_string(S, "\r\n", "\r\n", Str_Ls),
    maplist(string_codes, Str_Ls, Ls),
    nwpipe_term(Ls, T),
    emit_html(user_output, T).
@

## Parse the `noweb` pipeline
This module is used by the [[<<driver.pl>>]] to convert the list of lines representing the line-oriented `noweb` pipeline to a Prolog term.
This is done in two steps.

#. Each line in the list is converted to a term;
#. The flat list of terms is parsed to obtain a term that represents the structure of the document.
<<nwpipe.pl>>=
:- module(nwpipe, [nwpipe_term/2]).

nwpipe_term(Pipe, Doc) :-
    maplist(nwtoken_term, Pipe, Terms),
    phrase(lir_doc(Doc), Terms).

<<`Noweb` token $\rightarrow$ Prolog term>>
<<Flat list of terms $\rightarrow$ structured term>>
@

A `noweb` token in the `noweb` pipeline representation [@Ramsey1992] is parsed to become a Prolog term.
Here, we use an approach to parsing them described in "The Craft of Prolog" [@OKeefe1990].
We use the keyword at the beginning of each token to look up (deterministically) in a table the list of items that this token contains.
Then, each item is converted to a Prolog term using a mini-interpreter for the "language" of `noweb` tokens.
<<`Noweb` token $\rightarrow$ Prolog term>>=
:- use_module(library(dcg/basics), [nonblanks//1, integer//1]).

nwtoken_term([0'@|Token], Term) :-
    phrase(token(Back, Term), Token, Back).

token(Back, Term) -->
    nonblanks(NB),
    {   atom_codes(Key, NB),
        keyword(Key, Items, Back, Term)
    },
    items(Items).
<<Keyword table>>
<<Mini-interpreter for items>>
@

<<Keyword table>>=
<<Structural keywords>>
<<Tagging keywords>>

<<Structural keywords>>=
keyword(begin, [space, chunk_kind(K)], _, K).
keyword(end, [], _, end).
keyword(text, [space, string_rest(S, Rest)], Rest, text(S)).
keyword(nl, [], [], nl).
keyword(defn, [space, string_rest(S, Rest)], Rest, defn(S)).
keyword(use, [space, string_rest(S, Rest)], Rest, use(S)).
keyword(quote, [], [], quote).
keyword(endquote, [], [], endquote).

<<Tagging keywords>>=
keyword(file, [space, atom_rest(File, Rest)], Rest, file(File)).
keyword(xref, [space, xref(XRef, Rest)], Rest, XRef).
keyword(index, [space, index(Index, Rest)], Rest, Index).

<<Cross-referencing keyword table>>
<<Index table>>

<<Cross-referencing keyword table>>=
<<Basic cross-referencing>>
<<Linking previous and next definitions of a code chunk>>
<<Continued definitions of the current chunk>>
<<The list of all code chunks>>
<<Chunks where the code is used>>

<<Index table>>=
index(beginindex, [], [], index_beginindex).
index(endindex, [], [], index_endindex).

<<Basic cross-referencing>>=
xref(label, [space, atom_rest(L, Rest)], Rest, xref_label(L)).
xref(ref, [space, atom_rest(L, Rest)], Rest, xref_ref(L)).

<<Linking previous and next definitions of a code chunk>>=
xref(prevdef, [space, atom_rest(L, Rest)], Rest, xref_prevdef(L)).
xref(nextdef, [space, atom_rest(L, Rest)], Rest, xref_nextdef(L)).

<<Continued definitions of the current chunk>>=
xref(begindefs, [], [], xref_begindefs).
xref(defitem, [space, atom_rest(L, Rest)], Rest, xref_defitem(L)).
xref(enddefs, [], [], xref_enddefs).

<<Chunks where the code is used>>=
xref(beginuses, [], [], xref_beginuses).
xref(useitem, [space, atom_rest(L, Rest)], Rest, xref_useitem(L)).
xref(enduses, [], [], xref_enduses).
xref(notused, [space, string_rest(Name, Rest)], Rest, xref_notused(Name)).

<<The list of all code chunks>>=
xref(beginchunks, [], [], xref_beginchunks).
xref(chunkbegin,
    [space, atom(L), space, string_rest(Name, Rest)],
    Rest,
    xref_chunkbegin(L, Name)).
xref(chunkuse, [space, atom_rest(L, Rest)], Rest, xref_chunkuse(L)).
xref(chunkdefn, [space, atom_rest(L, Rest)], Rest, xref_chunkdefn(L)).
xref(chunkend, [], [], xref_chunkend).
xref(endchunks, [], [], xref_endchunks).
@

Using the tables above, we have looked up the exact items that we expect in a `noweb` token.
This is a small interpreter that takes the list of items and converts each item to the corresponding Prolog term.
<<Mini-interpreter for items>>=
items([]) --> [].
items([I|Is]) -->
    item(I),
    items(Is).

<<Individual items>>
<<Individual items>>=
<<Atoms and text>>
<<"Space">>
<<Chunk number>>
<<Chunk kind>>
<<Cross-reference>>
@

Identifiers are converted to atoms, while "text" (text and names) are converted to strings:
<<Atoms and text>>=
item(atom(A)) -->
    nonblanks(Codes),
    {   atom_codes(A, Codes)
    }.

% Using when/2 here is probably an ugly hack.
% Could not figure out a better way to deal with
% "rest of line" situations.
item(atom_rest(A, Rest)) -->
    {   when(ground(Rest), atom_codes(A, Rest))
    }.

item(string_rest(Str, Rest)) -->
    {   when(ground(Rest), string_codes(Str, Rest))
    }.
@

We assume that [[<<"Space">>]] is represented by a single "space" character; the "Hacker's Guide" is not explicit about this, but so far it has always worked.
<<"Space">>=
item(space) --> [0'\s]. % could it be another "white"?...
@

Integers are used to enumerate the code and documentation:
<<Chunk number>>=
item(chunk_number(N)) --> integer(N).
@

Code and documentation chunks are delimited by the same keywords; the [[<<Chunk kind>>]] is encoded in a secondary keyword:
<<Chunk kind>>=
item(chunk_kind(CK)) -->
    nonblanks(Codes),
    {   atom_codes(CK, Codes)
    }.
@

Finally, [[<<Cross-reference>>]].
Note that it employs quite a few secondary keywords collected in their own look up tables, [[<<Cross-referencing keyword table>>]] and [[<<Index table>>]].
<<Cross-reference>>=
item(xref(XRef, Rest)) -->
    nonblanks(Codes),
    {   atom_codes(X, Codes),
        xref(X, Items, Rest, XRef)
    },
    items(Items).
item(index(Index, Rest)) -->
    nonblanks(Codes),
    {   atom_codes(X, Codes),
        index(X, Items, Rest, Index)
    },
    items(Items).
@

## Convert the pipeline to a term
The structure of the document is implicitly present in the `noweb` pipeline representation: documentation and code chunks are delimited by a start and an end token, new lines and other formatting are contained in text tokens, and so on.
The pipeline representation also contains cross-referencing information, using labels and references to them.
It is however a bit inconvenient to use this representation for generating markup.
This is because the order of tokens in the pipeline representation exactly mirrors the layout of the final human-readable documentation, as generated by `noweb`, and `lir` uses a slightly different layout.

At that point, we have a list of Prolog terms.
Those are easier to deal with in the context of Prolog, as it is much easier to make the rules deterministic.
The general approach is to consume the next input and use it as the first argument to a rule, thus taking advantage of Prolog's first-argument indexing.
Then, the rules defined below describe a state machine where each rule is a state and each rule clause is a transition that depends on the last input.

On the highest level, the `noweb` pipeline is made of a sequence of files.
Note, however, that since in `lir` the input is standard input (and not a list of files), there will be only one, unnamed file in the pipeline.
<<Flat list of terms $\rightarrow$ structured term>>=
lir_doc(L) --> [file('')],
    lir_rest(L).

lir_rest(L) --> [X], !,
    lir_file(X, L).
lir_rest([]) --> [].

<<Parse a file>>
@

A file is a sequence of documentation and code chunks.
It will contain a list of chunks and an index.
To parse the contents of display item meta-data, we use the [[<<yaml.pl>>]] module.
<<Parse a file>>=
:- use_module(yaml, [yaml_to_dict/2]).

lir_file(docs, L) --> [X],
    docs(X, L).
lir_file(code, [Code|L]) -->
    code(C, Name, Label, M),
    {   (   Name = name(N)
        ->  Code = code_name_label_meta(C, N, Label, M)
        ;   Name = display(KW, FN_str)
        ->  code_codelist(C, Codes),
            yaml_to_dict(Codes, Display_meta),
            Code = display(KW, FN_str, Display_meta, Label)
        )
    },
    lir_rest(L).
lir_file(nl, [nl|L]) -->
    lir_rest(L).
lir_file(xref_beginchunks, [chunks_list(Cs)|L]) --> [X],
    xref_chunks(X, Cs),
    lir_rest(L).
lir_file(index_beginindex, [index(I)|L]) --> [X],
    index_list(X, I),
    lir_rest(L).

code_codelist(C, L) :-
    maplist(code_atomic, C, L0),
    atomics_to_string(L0, S0),
    string_codes(S0, L).

code_atomic(text(Text), Text).
code_atomic(nl, "\n").

<<Parse a documentation chunk>>
<<Parse a code chunk>>
<<Parse the list of chunks>>
<<Parse the index>>
@

A module that allows the user to parse a codelist containing YAML data to a Prolog dictionary containing a native representation of the YAML data.
This module makes use of SWI-Prolog's library [`dcg/basics`](http://eu.swi-prolog.org/pldoc/doc/swi/library/dcg/basics.pl).
<<yaml.pl>>=
:- module(yaml, [yaml_to_dict/2]).

:- use_module(library(dcg/basics), [white//0,
                                    whites//0,
                                    string_without//2]).

yaml_to_dict(Codes, Dict) :-
    phrase(yaml_es(Es), Codes),
    dict_create(Dict, yaml, Es).

<<Make a list of YAML key-value pairs>>
@

<<Make a list of YAML key-value pairs>>=
yaml_es([K-V|Es]) --> yaml_key_val(K, V), !, yaml_es(Es).
yaml_es([]) --> [].

<<Parse one YAML key-value pair>>
@

The keyword starts at the very beginning of the line, does not contain any blank characters (space, tab, newline...), and is ended by a colon followed by a single space.
The character that follows that space, here called `C`, determines if this is a block scalar or not.
<<Parse one YAML key-value pair>>=
yaml_key_val(K, V) --> yaml_key_codes(KCs), ": ", !,
    {   atom_codes(K, KCs)
    },
    [C],
    yaml_val(C, V).

<<Get YAML key>>
<<Get YAML value>>
@


Any graph character will do, but no blanks of any kind allowed.
<<Get YAML key>>=
yaml_key_codes([C|Cs]) --> [C],
    {   code_type(C, graph)
    },
    yaml_key_codes(Cs).
yaml_key_codes([]) --> [].
@

The only recognized block notation is literal style, indicated by a "`|`".
<<Get YAML value>>=
yaml_val(0'|, V) --> "\n", !,
    yaml_indented_block(V_codes),
    {   string_codes(V, V_codes)
    }.
yaml_val(C, V_str) --> string_without("\n", Cs), "\n",
    {   string_codes(V_str, [C|Cs])
    }.

<<Read YAML indented block>>
@

All indenting is removed, but all newlines are preserved.
<<Read YAML indented block>>=
yaml_indented_block(Block) --> white, whites, !,
    indented_lines(Block).
yaml_indented_block([]) --> [].

indented_lines([C|Cs]) --> [C],
    {   C \== 0'\n
    },
    !,
    indented_lines(Cs).
indented_lines([0'\n|Block]) --> [0'\n], !,
    yaml_indented_block(Block).
@

<<:make>>=
test-display: lir.lir
	ls -l -h lir.lir > test-display

@
<<:listing test-display>>=
title: A test for display environments
caption: |
    This is just a small example,
    nothing more.
@

A documentation chunk can contain text, new lines, and quoted code.
<<Parse a documentation chunk>>=
docs(end, L) -->
    lir_rest(L).
docs(text(T), [text(Text)|L]) -->
    docs_text(Ts),
    {   atomics_to_string([T|Ts], Text)
    },
    [X],
    docs(X, L).
docs(nl, [nl|L]) --> [X],
    docs(X, L).
docs(quote, [quote(Q)|L]) --> [X],
    quote(X, Q),
    [Y],
    docs(Y, L).

<<Parse quoted code>>
@

Consequitive text tokens that are not interrupted by any other structural contents are collected and concatenated.
<<>>=
docs_text([T|Ts]) --> [text(T)], !,
    docs_text(Ts).
docs_text(["\n"|Ts]) --> [nl], !,
    eol(Ts).
docs_text([]) --> [].

eol([T|Ts]) --> [text(T)], !,
    docs_text(Ts).
eol([]) --> [].
@

<<Parse quoted code>>=
quote(text(T), [text(T)|Q]) --> [X],
    quote(X, Q).
quote(xref_ref(L), [quote_use(N, L)|Q]) --> [use(N), X],
    quote(X, Q).
quote(endquote, []) --> [].
@

<<Parse a code chunk>>=
code(Cs, Defn, L, M) -->
    [X],
    defn(X, M_pairs0),
    {   selectchk(label(L), M_pairs0, M_pairs1),
        selectchk(defn(Defn), M_pairs1, M_pairs),
        dict_create(M, code, M_pairs)
    },
    code_content(Cs).

<<Parse the code chunk header>>
<<Parse the code chunk contents>>
@

<<Parse the code chunk header>>=
defn(nl, []) --> [].
defn(xref_label(L), [label(L)|M]) --> [X],
    defn(X, M).
defn(xref_ref(L), [ref(L)|M]) --> [X],
    defn(X, M).
defn(defn(N), [defn(Name)|M]) -->
    {   (   display_item(N, KW, FN_str)
        ->  Name = display(KW, FN_str)
        ;   Name = name(N)
        )
    },
    [X],
    defn(X, M).
defn(xref_notused(_N), [uses(notused)|M]) --> [X],
    defn(X, M).
defn(xref_beginuses, [uses(Us)|M]) --> [X],
    uses(X, Us, Us),
    [Y],
    defn(Y, M).
defn(xref_prevdef(L), [prev(L)|M]) --> [X],
    defn(X, M).
defn(xref_nextdef(L), [next(L)|M]) --> [X],
    defn(X, M).
defn(language(L), [language(L)|M]) --> [X],
    defn(X, M).
defn(xref_begindefs, [defs(Ds)|M]) --> [X],
    defs(X, Ds, Ds),
    [Y],
    defn(Y, M).

<<Treat display items differently>>
<<Parse defs and uses>>
@

<<Treat display items differently>>=
display_item(Name, KW, FN_str) :-
    sub_string(Name, 0, 1, _, ":"),
    once( sub_string(Name, Before_sep, 1, After_sep, " ") ),
    Length_kw is Before_sep - 1,
    sub_string(Name, 1, Length_kw, _After_kw, KW_str),
    display_item_keyword(KW_str, KW),
    Before_fn is Before_sep + 1,
    sub_string(Name, Before_fn, After_sep, 0, FN_str).

/* Seems this is deterministic when 1st argument is a string */
display_item_keyword("source", source).
display_item_keyword("result", result).
display_item_keyword("figure", figure).
display_item_keyword("listing", listing).
display_item_keyword("table", table).
@

<<Parse the code chunk contents>>=
code_content([text(Text)|Cs]) -->
    text_token(T), !,
    code_text(Ts),
    {   atomics_to_string([T|Ts], Text)
    },
    code_content(Cs).
code_content([code_use(L, R, N)|Cs]) -->
    [xref_label(L), xref_ref(R), use(N)], !,
    code_content(Cs).
code_content([]) --> [end].

text_token("\n") --> [nl].
text_token(T) --> [text(T)].
    
code_text([T|Ts]) -->
    text_token(T), !,
    code_text(Ts).
code_text([]) --> [].
@

<<Parse defs and uses>>=
uses(xref_enduses, _, []) --> [].
uses(xref_useitem(L), Us, [L|Us0]) --> [X],
    uses(X, Us, Us0).

defs(xref_enddefs, _, []) --> [].
defs(xref_defitem(L), Ds, [L|Ds0]) --> [X],
    defs(X, Ds, Ds0).
@

<<Parse the list of chunks>>=
xref_chunks(xref_endchunks, []) --> [].
xref_chunks(xref_chunkbegin(L, N), [chunk(L, N, Us, Ds)|Cs]) --> [X],
    xref_chunk(X, Us, Ds),
    [Y],
    xref_chunks(Y, Cs).

xref_chunk(xref_chunkend, [], []) --> [].
xref_chunk(xref_chunkuse(U), [chunkuse(U)|Us], Ds) --> [X],
    xref_chunk(X, Us, Ds).
xref_chunk(xref_chunkdefn(D), Us, [chunkdefn(D)|Ds]) --> [X],
    xref_chunk(X, Us, Ds).
@

We don't have an index for now.
<<Parse the index>>=
index_list(index_endindex, []) --> [].
@

## Emit Pandoc HTML
<<lirhtml.pl>>=
:- module(lirhtml, [emit_html/2]).

:- use_module(library(http/html_write)).

emit_html(Out, L) :-
    counters_db(L, DB),
    phrase(lir_pandoc(P, DB), L),
    phrase(html(P), H),
    print_html(Out, H).

<<`counters_db/2`: Generate counters, add them to a database>>
<<`lir_pandoc//2`: Lir to Pandoc HTML>>
@

Add to a database all numbered items: code chunks, display items (`figure`, `listing`, `table`), as well as sources and sinks of the DAG representing the dataflow.
For the code chunks, we also format the names.
This is necessary because code chunk names are inside HTML tags by the time they make it to Pandoc, and are not considered for converting from markdown: instead, we do this here.
<<`counters_db/2`: Generate counters, add them to a database>>=
counters_db(L, db{code:code{name:CC_names, nr:CC_nrs},
                  source:source{namestr:"S", nr:DSNrs},
                  result:result{namestr:"Result ", nr:DRNrs},
                  listing:listing{namestr:"Listing ", nr:DLNrs},
                  figure:figure{namestr:"Figure ", nr:DFNrs},
                  table:table{namestr:"Table ", nr:DTNrs}}) :-
    include(is_code_name_label_meta, L, CNLMs),
    maplist(code_name_label_meta_Name_Label, CNLMs, Ns, Ls),
    fmt_names_fmtd(html5, Ns, FNs),
    
    pairs_keys_values(LFs, Ls, FNs),
    dict_create(CC_names, name, LFs),
    findall(Label-Nr, nth1(Nr, Ls, Label), LNrs),
    dict_create(CC_nrs, nr, LNrs),

    maplist(display_dict(L),
            [source, result, listing, figure, table],
            [DSNrs, DRNrs, DLNrs, DFNrs, DTNrs]),

    dict_create(DB, db,
                [code:code{name:CC_names, nr:CC_nrs},
                 source:source{namestr:"S", nr:DSNrs},
                 result:result{namestr:"Result ", nr:DRNrs},
                 listing:listing{namestr:"Listing ", nr:DLNrs},
                 figure:figure{namestr:"Figure ", nr:DFNrs},
                 table:table{namestr:"Table ", nr:DTNrs}]).

is_code_name_label_meta(code_name_label_meta(_,_,_,_)).
code_name_label_meta_Name_Label(code_name_label_meta(_,N,L,_), N,L).

display_dict(L, Item, Dict) :-
    include(is_display_item(Item), L, Ls),
    findall(Label-Nr,
            nth1(Nr, Ls, display(Item, _, _, Label)),
            DNrs),
    dict_create(Dict, nr, DNrs).

is_display_item(Item, display(Item,_,_,_)).
    
<<Use Pandoc to pre-format chunk names>>
@

<<Use Pandoc to pre-format chunk names>>=
:- use_module(library(process), [process_create/3]).
:- use_module(library(sgml), [load_structure/3]).

fmt_names_fmtd(html5, Ns, FNs) :-
    atomics_to_string(Ns, "\n\n", Ns_str),
    process_create(path(pandoc),
                   ["--to=html5"],
                   [stdin(pipe(Pandoc_in)),
                    stdout(pipe(Pandoc_out))]),
    format(Pandoc_in, "~s", [Ns_str]),
    close(Pandoc_in),
    load_structure(Pandoc_out, DOM, [dialect(xml)]),
    close(Pandoc_out),
    maplist(paragraph_fmtd, DOM, FNs).

:- use_module(library(sgml_write), [xml_write/3]).

paragraph_fmtd(element(p, [], StrDOM), Fmtd) :-
    with_output_to(string(Fmtd),
                   xml_write(current_output,
                             StrDOM,
                             [header(false), layout(false)])).
@

<<`lir_pandoc//2`: Lir to Pandoc HTML>>=
lir_pandoc(P, DB) -->
    [X], !,
    lir_pandoc(X, P, DB).
lir_pandoc([], _DB) --> [].

<<Lir to Pandoc: individual items>>
<<HTML for common symbols>>
@

<<Lir to Pandoc: individual items>>=
<<Lir documentation to Pandoc>>
<<Lir code chunks to Pandoc>>
<<Lir display items to Pandoc>>
<<Ignore the list of chunks and the index>>
<<Individual display items>>
@

<<Lir documentation to Pandoc>>=
lir_pandoc(nl, ["\n"|P], DB) --> lir_pandoc(P, DB).
lir_pandoc(text(T), [\[T]|P], DB) --> lir_pandoc(P, DB).
lir_pandoc(quote(Q), [span(class(quote), QC)|P], DB) -->
    {   phrase(lir_pandoc(QC, DB), Q)
    },
    lir_pandoc(P, DB).
lir_pandoc(quote_use(Name, Label),
            [span(class("quoteduse"),
                  [\openparen, \thinnbsp,
                   a(href("#~a"-Label), Name),
                   \thinnbsp, \closeparen])|P], DB) -->
    lir_pandoc(P, DB).
@

<<Lir code chunks to Pandoc>>=
lir_pandoc(code_name_label_meta(C, _, L, M),
           [div(class("codechunk"),
                [\chunk_defn(M, L, DB),
                 \chunk_uses(M, DB),
                 \chunk_defs(M, DB),
                 \chunk_prev(M, DB),
                 pre(\chunk_content(C, DB)),
                 \chunk_next(M, DB)])|P], DB) -->
    lir_pandoc(P, DB).
@

<<Lir display items to Pandoc>>=
lir_pandoc(display(KW, FN_str, Meta, Label),
           [div([class("lirdisplay"),id(Label)],
                [\display(KW, FN_str, Meta, Label, DB)])|P], DB) -->
    lir_pandoc(P, DB).

@

<<Ignore the list of chunks and the index>>=
lir_pandoc(chunks_list(_Cs), P, DB) -->
    lir_pandoc(P, DB).
lir_pandoc(index(_), P, DB) --> [], lir_pandoc(P, DB).
@

<<HTML for common symbols>>=
nbsp --> html(&(nbsp)).
thinnbsp --> html(span(style("white-space:nowrap"), &(0x2009))).
openparen --> html(&('Lang')). % Lang
closeparen --> html(&('Rang')). % Rang
prevsym --> html(&(0x21E7)). % UPWARDS WHITE ARROW
nextsym --> html(&(0x21E9)). % DOWNWARDS WHITE ARROW
contsym --> html(&(darr)).
contmoresym --> html(&(0x21E3)). % DOWNWARDS DASHED ARROW
defeq --> html(&(equiv)).
defend --> html(&(0x25FC)). % Black square
@

<<Individual display items>>=
:- discontiguous display//5.

display(source, FN, _, Label, DB) -->
    display_file(source, FN, Label, DB).
display(result, FN, _, Label, DB) -->
    display_file(result, FN, Label, DB).

display_file(Item, FN, Label, DB) -->
    {   absolute_file_name(FN, Absolute_FN)
    },
    html(div(class("lir~a"-Item),
             [span(class("~aname"-Item),
                   ["~s~d:"-[DB.Item.namestr,
                             DB.Item.nr.Label]]),
              \nbsp,
              span(class("~afile"-Item),
                   a(href(Absolute_FN), code(FN)))])).

display(listing, FN, Meta, Label, DB) -->
    {   file_contents(FN, FC)
    },
    html(div(class("lirlisting"),
             [\display_header(listing, FN, Label, DB),
              div(class("listingcontents"), pre([FC])),
              \display_title_caption(listing, Meta)])).

file_contents(FN, FC) :-
    setup_call_cleanup(open(FN, read, In),
                       read_string(In, _, FC),
                       close(In)).

display_header(Item, FN, Label, DB) -->
    {   absolute_file_name(FN, Absolute_FN)
    },
    html(div(class("~aheader"-Item),
             [span(class("~aname"-Item),
                   ["~s~d:"-[DB.Item.namestr,
                             DB.Item.nr.Label]]),
              \nbsp,
              span(class("~afile"-Item),
                   a(href(Absolute_FN), code(FN)))])).

display_title_caption(Item, M) -->
    html(div(class("~ameta"-Item),
             [span(class("~atitle"-Item), [M.title]),
              \display_optional_caption(Item, M)])).

display_optional_caption(Item, M) -->
    {   yaml{caption:Caption} :< M
    },
    !,
    html([" ", span(class("~acaption"-Item), [Caption])]).
display_optional_caption(_, _) --> [].

display(table, FN, Meta, Label, DB) -->
    {   file_contents(FN, FC)
    },
    html(div(class("lirtable"),
             [\display_header(table, FN, Label, DB),
              table([\[FC]]),
              \display_title_caption(table, Meta)])).

display(figure, FN, Meta, Label, DB) -->
    html(div(class("lirfigure"),
             [\display_header(figure, FN, Label, DB),
              \figure_contents(FN),
              \display_title_caption(figure, Meta)])).

figure_contents(FN) -->
    html(div(class("figurecontents"),
             [img(src(".lir/~s"-FN))])).

@

Chunk contents:
<<lirhtml.pl>>=
chunk_content([], _DB) --> [].
chunk_content([C|Cs], DB) -->
    chunk_content_(C, Cs, DB).

chunk_content_(nl, Cs, DB) -->
    html("\n"),
    chunk_content(Cs, DB).
chunk_content_(text(T), Cs, DB) -->
    html(T),
    chunk_content(Cs, DB).
chunk_content_(code_use(L, R, _), Cs, DB) -->
    html(span(class("embeddeduse"),
              [\openparen, \thinnbsp,
               a([id(L), href("#~a"-R)],
                 [\[DB.code.name.R]]),
               \thinnbsp, \closeparen])),
    chunk_content(Cs, DB).
@

Code chunk headers:
<<>>=
chunk_defn(_M, L, DB) -->
    html(span([class("defn"),id(L)],
              [span(class("chunknr"), "C~d:"-DB.code.nr.L), \nbsp,
               \openparen, \thinnbsp,
               a(href("#~a"-L), \[DB.code.name.L]),
               \thinnbsp, \closeparen, \thinnbsp, \defeq])).
chunk_uses(M, DB) -->
    {   code{uses:Us} :< M
    }, !,
    html(span(class("chunkuses"), \chunk_uses_(Us, DB))).
chunk_uses(_, _) --> [].
@

Uses in the header:
<<>>=
chunk_uses_([U|Us], DB) -->
    html([br([]),
         "Appears in ",
         a(href("#~a"-U),["C~d"-DB.code.nr.U])]),
    chunk_uses_rest(Us, DB).
chunk_uses_(notused, _DB) --> html([br([]), "root chunk"]).
chunk_uses_rest([], _DB) --> [].
chunk_uses_rest([U|Us], DB) -->
    html([", ", a(href("#~a"-U), ["C~d"-DB.code.nr.U])]),
    chunk_uses_rest(Us, DB).
@

Previous and next chunks in the header:
<<>>=
chunk_next(M, DB) -->
    {   code{next:L} :< M
    }, !,
    html(span(class("chunknext"),
         a(href("#~a"-L),
           [\nextsym, \thinnbsp, "C~d"-DB.code.nr.L]))).
chunk_next(_, _DB) -->
    html(\defend).

chunk_prev(M, DB) -->
    {   code{prev:L} :< M
    },
    html([br([]),
          span(class("chunkprev"),
               a(href("#~a"-L),
                 [\prevsym, \thinnbsp, "C~d"-DB.code.nr.L]))]).
chunk_prev(_, _DB) --> [].

chunk_defs(M, DB) -->
    {   code{defs:Ds} :< M
    }, !,
    html(span(class("chunkdefs"), \chunk_defs_(Ds, DB))).
chunk_defs(_, _) --> [].

chunk_defs_([D|Ds], DB) -->
    html([br([]),
          "definition continued in ",
          a(href("#~a"-D),
            [\contsym, \thinnbsp, "C~d"-DB.code.nr.D])]),
    chunk_defs_rest(Ds, DB).
chunk_defs_rest([], _) --> [].
chunk_defs_rest([D|Ds], DB) -->
    html([" + ",
          a(href("#~a"-D),
            [\contmoresym, \thinnbsp, "C~d"-DB.code.nr.D])]),
    chunk_defs_rest(Ds, DB).
@

# Layout
The system tries to separate content and layout as much as possible.
It also aims to provide a sensible default layout for the human-readable documentation.

## Code chunk language
The program in [[<<langs.cpp>>]] can be used as a filter to `noweb`.
It deduces the programming language of code chunks based on the names of the chunks.
This is a table that maps known extensions to programming language names.
<<Ext-Lang>>=
{"pl", "prolog"},
{"sh", "bash"},
{"bash", "bash"},
{"cpp", "cpp"},
{"R", "R"},
{"awk", "awk"},
{"sed", "sed"},
{"css", "css"}
@

The program implements a state machine with the states necessary to extract code chunk names and the uses within a code chunk.
The start state is `out`;
when a code chunk starts, it transitions to `code`, where it expects a code chunk name, and transitions to `content`;
while in `content`, it collects uses, and goes back to `out` at the end of the code chunk.
Throughout, each line from input is saved to a list of all lines.
<<langs.cpp>>=
<<Includes (`langs`)>>
<<Function definitions (`langs`)>>

int main()
{
    <<Variable definitions (`langs`)>>

out:
{   
    <<`Out` transitions>>
    goto out;
}

code:
{
    <<`Code` transitions>>
    goto code;
}

content:
{
    <<`Content` transitions>>
    <<`Content`: collect uses>>
    goto content;
}

end:
{
    <<Propagate language to uses>>
    <<Output all lines, with `@language` after each `@defn`>>
    return 0;
}

error:
    return 1;
}
@

When in `out`, end of input signals transition to the `end` state.
A `@begin code` token signals a transition to `code`.
<<`Out` transitions>>=
if (!std::getline(std::cin, line)) goto end;
lines.push_back(line);

if (string_prefix(line, {"@begin code"})) goto code;
@

When in `code`, end of input is an error.
A `@defn` token contains the code chunk name.
The name is processed and the state machine transitions to `content`.
<<`Code` transitions>>=
if (!std::getline(std::cin, line)) goto error;
lines.push_back(line);

if (string_prefix_rest(line, {"@defn "}, name)) {
    <<Process code chunk name>>
    goto content;
}
@

A code chunk name is inserted to the DAG of code chunks.
Initially the set of neighbours is empty.
Note that `std::map::insert` will only insert if the key does not yet exist, so it is safe to do this.
<<Process code chunk name>>=
uses.insert({name, {}}); 
<<Try to set code chunk language>>
@

If the language of the chunk can be guessed from the name, the chunk name and its language are recorded.
The code chunk name is also added to the queue later used for the breadth-first traversal of the DAG of code chunks.
<<Try to set code chunk language>>=
std::string cl;
if (name_dict_lang(name, langs, cl)) {
    chunk_lang.insert({name, cl});
    pending.push(name);
}
@

When in `content`, end of input is an error.
An `@end code` token signals the transition back to `out`.
<<`Content` transitions>>=
if (!std::getline(std::cin, line)) goto error;
lines.push_back(line);

if (string_prefix(line, {"@end code"})) goto out;
@

When in `content`, each `@uses` token is used to populate the set of neighbours of the current code chunk in the DAG.
<<`Content`: collect uses>>=
std::string un;
if (string_prefix_rest(line, {"@use "}, un)) uses[name].insert(un);
@

The language of a code chunk can be deduced in two ways.
First, from the code chunk name; this is done as soon as the code chunk is encountered for the first time (see [[<<Try to set code chunk language>>]]).
Second, if a code chunk without a known language is used by a code chunk with a language, it inherits it.
To achieve this, a breadth-first traversal of the DAG of code chunks is used.
Initially, the queue holds the names of all chunks with known languages (also done while reading).
Any child code chunk _that does not yet have a language_ has its language set to that of the parent, and is pushed to the back of the queue.
<<Propagate language to uses>>=
while (!pending.empty()) {
    std::string next = pending.front();
    pending.pop();

    for (auto c : uses[next]) {
        auto x = chunk_lang.find(c);
        if (x == chunk_lang.cend()) {
            chunk_lang.insert({c, chunk_lang[next]});
            pending.push(c);
        }
    }
}
@

At the end, all tokens are emitted.
If the token is `@defn`, a `@language` token is added right after it.
For code chunks that don't have a language, the `txt` language is used.
<<Output all lines, with `@language` after each `@defn`>>=
for (auto l : lines) {
    std::cout << l << '\n';

    if (string_prefix_rest(l, {"@defn "}, name)) {
        auto x = chunk_lang.find(name);
        if (x != chunk_lang.cend())
            std::cout << "@language " << x->second << '\n';
        else
            std::cout << "@language txt\n";
    }
}
@

These variables are "global" to the `main` function, or in other words, are available to all states of the state machine.
<<Variable definitions (`langs`)>>=
<<A list of all lines as they are read>>
<<The DAG of the code chunks as adjacency-list>>
<<The language of each code chunk>>
<<A queue for the traversal of the chunks DAG>>
<<Mapping of extensions to languages>>
<<A list of all lines as they are read>>=
std::list<std::string> lines{};
<<The DAG of the code chunks as adjacency-list>>=
std::map<std::string, std::set<std::string>> uses{};
<<The language of each code chunk>>=
std::map<std::string, std::string> chunk_lang{};
<<A queue for the traversal of the chunks DAG>>=
std::queue<std::string> pending{};
<<Mapping of extensions to languages>>=
const std::map<std::string, std::string> langs{<<Ext-Lang>>};
@

Two strings, one for the last line that was read and one for the name of the last code chunk defined with `@defn`:
<<Variable definitions (`langs`)>>=
std::string line;
std::string name;
@

The necessary standard libraries:
<<Includes (`langs`)>>=
#include <iostream>
#include <string>
#include <list>
#include <map>
#include <set>
#include <queue>
@

<<Function definitions (`langs`)>>=
<<Does a string have a prefix?>>
<<Can you guess the language from the name?>>
@

<<Does a string have a prefix?>>=
bool
string_prefix(const std::string& s, const std::string& t)
{
    if (0 == s.compare(0, t.length(), t)) return true;
    return false;
}
@

A three-argument version to get the rest, too:
<<>>=
bool
string_prefix_rest(const std::string& s, const std::string& t,
                   std::string& rest)
{
    if (string_prefix(s, t)) {
        rest = s.substr(t.length());
        return true;
    }
    return false;
}
@

<<Can you guess the language from the name?>>=
bool
name_dict_lang(const std::string& n,
               const std::map<std::string, std::string>& d,
               std::string& l)
{
    size_t x = n.find_last_of('.');
    if (x == std::string::npos) return false;

    ++x;
    auto ext_lang = d.find(n.substr(x));
    if (ext_lang == d.cend()) return false;

    l = ext_lang->second;
    return true;
}
@

## HTML
For HTML documentation, [[<<lir.css>>]] is used.
<<lir.css>>=
<<Text width and margin>>
<<Headers in sans-serif>>
<<Quoted code in monospace>>
<<Use names typesetting>>
<<Highlight chunk names on hover>>
<<List of chunks formatting>>
<<Display item formatting>>
@

The text width is limited and a left margin is inserted to improve readability.
<<Text width and margin>>=
p {
    max-width: 14cm;
}
blockquote {
    max-width: 11cm;
    font-size: small;
}
ul, ol, dl,
span.chunkdefs {
    max-width: 12cm;
}
body {
    padding-left: 1cm;
}
@

<<Headers in sans-serif>>=
h1, h2, h3 {
    font-family: sans-serif;
    max-width: 12cm;
}
@

<<Use names typesetting>>=
span.sourcefile a,
span.resultfile a,
span.listingfile a,
span.figurefile a,
span.chunkdefs a,
span.chunkuses a,
span.chunkprev a,
span.chunknext a,
span.defn a,
span.quoteduse a,
span.embeddeduse a {
    text-decoration-line: none;
}
span.chunkdefs,
span.chunkuses,
span.chunkprev,
span.chunknext {
    font-size: small;
}
span.quoteduse {
    font-family: serif;
}
span.embeddeduse {
    font-family: serif;
}
span.chunknr {
    font-weight: bold;
}
@

<<Highlight chunk names on hover>>=
span.quoteduse a {
    color: darkGreen;
}
span.quoteduse a:hover {
    color: limeGreen;
}
span.embeddeduse a {
    color: darkblue;
    font-style: italic;
}
span.embeddedusesym {
    font-style: normal;
}
span.embeddeduse a:hover {
    color: dodgerBlue;
}
span.sourcefile a,
span.resultfile a,
span.listingfile a,
span.figurefile a,
span.chunkdefs a,
span.chunkprev a,
span.chunknext a,
span.defn a,
span.chunkuses a {
    color: darkred;
}
span.sourcefile a:hover,
span.resultfile a:hover,
span.listingfile a:hover,
span.figurefile a:hover,
span.chunkdefs a:hover,
span.chunkprev a:hover,
span.chunknext a:hover,
span.defn a:hover,
span.chunkuses a:hover {
    color: darkGoldenRod;
}
@

<<Quoted code in monospace>>=
span.quote {
    font-family: monospace;
}
@

<<List of chunks formatting>>=
ul.chunkslist {
    list-style-type: square;
}
@

<<Display item formatting>>=
div.lirtable th {
    border-style: none none solid none;
    border-width: 0 0 1px 0;
}
div.lirtable th,
div.lirtable td {
    padding-left: 3.3mm;
    padding-right: 3.3mm;
    padding-top: 0.9mm;
    padding-bottom: 0.9mm;
}
div.lirtable table {
    font-family: sans-serif;
    font-size: small;
    margin-top: 3mm;
    margin-bottom: 3mm;
    border-collapse: collapse;
    border-style: solid none solid none;
    border-width: 2px 0 1px 0;
}
div.lirlisting {
    max-width: 14cm;
}
span.sourcename,
span.resultname,
span.tablename,
span.figurename,
span.listingname {
    font-weight: bold;
}
div.figurecontents img {
    display: block;
    max-width: 14cm;
    width: auto;
    height: auto;
}
div.listingcontents {
    font-size: small;
    margin-top: 3mm;
    margin-bottom: 3mm;
    border-style: solid none solid none;
    border-width: 0.3mm;
}
span.sourcefile,
span.resultfile,
span.tablefile,
span.figurefile,
span.listingfile {
    font-style: italic;
}
div.tablemeta,
div.figuremeta,
div.listingmeta {
    max-width: 14cm;
    font-size: small;
}
span.tabletitle,
span.figuretitle,
span.listingtitle {
    font-weight: bold;
}
div.codechunk,
div.lirlisting,
div.lirtable,
div.lirfigure {
    margin-bottom: 3mm;
}
@
