---
title: 'Reproducible computing with `lir`'
author: Boris Vassilev
version: 0.3
rights: (c) 2015 Boris Vassilev, University of Helsinki
...

# Introduction
This is a collection of tools that support programming language-agnostic reproducible computing.

This document is self-hosting: the implementation presented here can be used to produce the final human-readable version of itself.
This makes informal validation trivial ("does the final document look right?"), but it also means that the only guarantee is that it implements enough to be able to compile itself.

# Overview
The tools provided by `lir` aim to support two goals:

1. Generating results by running the code in a `lir` source file.
2. Presenting all code and possibly results in a human-readable form.

The overall workflow for `lir` is as follows:

  * Extract all source code from the `lir` source file and bring in external dependencies, done by [[<<lir-tangle>>]].

    This might be scripts that are immediately executable, or code that needs to be compiled.

    For externally maintained files to be managed by `lir`, they need to be explicitly declared in the `lir` source file.
    Example use cases would be input data files, or a bibliography file maintained by a reference manager.

  * Generate all results, implemented by [[<<lir-make>>]].

    This is a step that usually will include resolving dependencies.
    In the case of intermediate analysis steps, a final result will depend on an intermediate result.
    For example, a figure might be generated from the results of an analysis of the input data.

    In the case of programs that need to be compiled, a compilation step will have to occur before actually running the program.

    All dependencies have to be explicitly declared in the `lir` source file.

  * Compile the final documentation, implemented by [[<<lir-weave>>]].

    Once all results and figures have been generated, the original `lir` source file is processed to obtain a valid Pandoc markdown source file.
    At that point, Pandoc is used to compile this to a final human readable file with any results and figures generated in the previous step.

To implement this, `lir` provides the three tools mentioned above, and a wrapper script, [[<<lir>>]], which simply runs them in that order.

# Tangling
"Tangling" is the process of extracting the source code embedded in the `lir` literate source file so that it can be compiled and executed to obtain results.

First, we have [[<<lir-tangle>>]], which accepts three arguments, in this order: (1) the name of a `lir` source file; (2) the name of a chunk; and (3) the name of the file that the chunk will be extracted to.
It uses `noweb`'s `notangle` to extract from (1) the chunk (2) to a temporary file, compares it to (3) and only writes (2) to (3) if (3) does not exist, or is different from the temporary file.
<<lir-tangle>>=
<<Bash header>>
<<lir-tangle.bash>>
<<lir-tangle.bash>>=
tmpfile=$(mktemp)
notangle -t8 -R"$2" "$1" > "$tmpfile" \
    && "$LIR_LIBPATH"/lir-cmpcp "$tmpfile" "$3" \
    && rm "$tmpfile"
@


The tool [[<<lir-tangle>>]] uses another tool, [[<<lir-cmpcp>>]].
It overwrites a file with a newer version of a file after comparing them.
It takes two arguments: (1) the new file, and (2) the old file.
The standard command line tool `cmp` used for the comparison will exit with status 0 (success) if the two files have identical contents; with status 1 if they are different, and with status 2 if there is some trouble: for example, if one of the files does not exist.
<<lir-cmpcp>>=
<<Bash header>>
<<lir-cmpcp.bash>>
<<lir-cmpcp.bash>>=
if ! cmp --silent "$1" "$2"
then
    cp --verbose "$1" "$2"
fi
@

The tool [[<<lir-tangle>>]] allows us to implement another tool, [[<<lir-mtangle>>]] ("multiple" tangle).
It accepts as argument (1) the `lir` source file, (2) the target directory, and the rest of the arguments are the code chunk names that are to be extracted from (1) to the directory (2), under their own names.
(The last point might be important: any string is a valid chunk name, so one could end up with strange file names with blanks or other special characters in them.)
<<lir-mtangle>>=
<<Bash header>>
<<lir-mtangle.bash>>
<<lir-mtangle.bash>>=
for chunk in "${@:3}"
do
    "$LIR_LIBPATH"/lir-tangle "$1" "$chunk" "$2/$chunk"
done
@

# Externally maintained files
Not all data in an analysis naturally fits into the `lir` source file.
Examples already mentioned above are the input data, or an externally maintained file like a bibliography.

Taking in use such a file means simply making a symbolic link to the file in the working direcory.
_However_, keep in mind that the externally maintained file still needs to be in the project directory, and its name should on a path relative to the project directory.
The `make` utility will check the timestamp on the _target of the link_, so when a "used" file changes, this will be taken into consideration.

The arguments to the script are (1) the directory in which the symbolic links will be made and from (2) on, the file names that should be taken in use:
<<lir-use>>=
<<Bash header>>
<<lir-use.bash>>
<<lir-use.bash>>=
for used in "${@:2}"
do
    useddir=$(dirname "$used")
    mkdir  --verbose --parents "$1/$useddir"
    original=$(readlink --canonicalize "$used")
    if [ -f "$original" ]
    then
        ln --symbolic --force "$original" "$1/$used"
    else
        >&2 echo "LIR ERROR: File not found! ($original)"
        exit 1
    fi
done
@

# Extracting names
With the help of [[<<lir-mtangle>>]] and the tools it uses, we can tangle all relevant chunks to files.
The question remains: which are the code chunks that need to be tangled?

`lir` assumes that all root chunks (code chunks that are not used in other chunks) are meant to be tangled to files.
The `noroots` tool provided by `noweb` will print to standard output all root chunk names, enclosed in double angle brackets.
It is used by [[<<lir-chunknames>>]], which takes as its single argument the `lir` source file:
<<lir-chunknames>>=
<<Bash header>>
<<lir-chunknames.bash>>
@

One implicitly ignored chunk is the "anonymous", unnamed code chunk as in "`@<<>>=`"; this is interpreted as a continuation of the previous chunk by `lir`, but "`@<<>>=`" will appear in the list of root chunks as listed by `noroots`.
A `lir` source file can also contain root chunks that have names beginning with `lir::`.
These are used by the user to control `lir`'s functionality, and are not actual code.
<<lir-chunknames.bash>>=
rootchunks=$(mktemp)
noroots "$1" \
    | <<Drop double angle brackets>> \
    | <<Remove empty lines>> \
    | <<Remove names starting with `lir::`>> \
    | sort --unique \
    > "$rootchunks"
@

If there is a root chunk that does not need to be tangled (for whatever reason), it can be declared in a chunk named [[<<lir::ignore>>]], one chunk name per line.
<<lir-chunknames.bash>>=
ignoredchunks=$(mktemp)
if noroots "$1" | grep --silent --line-regexp '@<<lir::ignore@>>'
then
    notangle -t8 -R"lir::ignore" "$1" \
        | <<Remove empty lines>> \
        | <<Drop double angle brackets>> \
        | sort --unique \
        > "$ignoredchunks"
fi
@

The chunk names declared in [[<<lir::ignore>>]] are removed from the list of chunk names emitted by [[<<lir-chunknames>>]] using the standard command line tool `comm`.
<<lir-chunknames.bash>>=
comm -2 -3 "$rootchunks" "$ignoredchunks"
rm "$rootchunks" "$ignoredchunks"
@

Filters used in the pipelines above:
<<Drop double angle brackets>>=
sed -n 's/^@<<\(.*\)@>>$/\1/p'
@
<<Remove empty lines>>=
sed '/^\s*$/d'
@
<<Remove names starting with `lir::`>>=
sed -n '/^lir::.*/!p'
@

# The wrapper script
A script called `lir` can be invoked with the source file as its first argument.
<<lir>>=
<<Bash header>>
<<lir.bash>>
<<lir.bash>>=
<<Setup `lir` script>>
<<Tangle relevant chunks>>
<<Use external dependencies>>
<<Make everything>>
<<Weave the documentation>>
@

The script will exit immediately on any command that fails.
Any arguments after the first will be given as options to Pandoc.
If the `.lir` working directory does not exist, it is created.
The paths to the library directories for `noweb` and `lir` are made available for all scripts used by the wrapper script.
<<Setup `lir` script>>=
set -e
LIR_SOURCE="$1"
LIR_DIR='.lir'
export PANDOC_OPTS="${@:2}"
MAKE_JOBS='--jobs'
mkdir --verbose --parents "$LIR_DIR"
export NOWEB_LIBPATH=@@NOWEB_LIBPATH@@
export LIR_LIBPATH=@@LIR_LIBPATH@@
@

Then, determine which chunks need to be tangled using [[<<lir-chunknames>>]], and tangle those using [[<<lir-mtangle>>]].
<<Tangle relevant chunks>>=
"$LIR_LIBPATH"/lir-chunknames "$LIR_SOURCE" \
    | xargs --delimiter='\n' \
        "$LIR_LIBPATH"/lir-mtangle "$LIR_SOURCE" "$LIR_DIR"
@

Then, bring in external dependencies (uses).
Not everything fits in a single source file.
A bibliography file, for example, is best maintained by a proper reference manager.
Such files can be listed explicitly in a code chunk named `lir::use` like so:
<<lir::use>>=
lir.bib
@

Note that the `lir` source file itself is a special case of an external dependency.
<<Use external dependencies>>=
if <<List `lir::*` chunks>> | grep use
then
    notangle -t8 -R"lir::use" "$LIR_SOURCE" \
        | sort --unique \
        | <<Remove empty lines>> \
        | xargs --delimiter='\n' \
            "$LIR_LIBPATH"/lir-use "$LIR_DIR" "$LIR_SOURCE"
fi
@
<<List `lir::*` chunks>>=
"$NOWEB_LIBPATH"/markup "$LIR_SOURCE" \
    | sed -n 's/^@defn lir::\(.*\)/\1/p'
@

Then, from inside the working directory:
<<Make everything>>=
notangle -t8 -R"lir::make" "$LIR_SOURCE" \
    > "$LIR_DIR"/lirmakefile
make --debug=b "$MAKE_JOBS" --directory="$LIR_DIR" --makefile=lirmakefile all
@

This is what an empty Makefile with only `all` declared would look like:
<<lir::make>>=
.PHONY : all
all: ;

@

<<Weave the documentation>>=
make --debug=b --directory="$LIR_DIR" --makefile=lirmakefile html
BASENAME=$(basename "$1" .lir)
cp --verbose --update ".lir/$BASENAME.html" .
@

# Weaving
Weaving means generating the final human readable documentation from the `lir` source file.
The original source is transformed by several filters before passed to Pandoc for generating an HTML document that can be viewed in a browser.
Everything is wrapped as a bash script that will be installed, along with [[<<lir>>]], in `LIR_BINPATH`.
It reads the `lir` source from standard input and writes the HTML to standard output.
<<lir-weave>>=
<<Bash header>>
<<Weave `lir` source to HTML>>
@

Using it, we can add a rule for building the HTML documentation.
For this project, it would be:
<<lir::make>>=
.PHONY: html
html: lir.html

lir.html: lir.lir
	lir-weave $(PANDOC_OPTS) < $< > $@

@

<<Weave `lir` source to HTML>>=
<<`lir`-specific transformations>> \
    | <<Source to `noweb` pipeline representation>> \
    | <<`Noweb` pipeline representation to Pandoc source>> \
    | <<Pandoc source to HTML>>
@

Of the two transformations done on the `lir` source before giving it to the `noweb` tools, "including" files is the more general one and has to be done first.
<<`lir`-specific transformations>>=
awk -f "$LIR_LIBPATH"/lir-listing.awk
@

Each line inside a code chunk named `lir::include` will be treated as a file name that has to be **pasted as it is** to the `lir` source file **before** the source file is passed to Pandoc.
_Use with care!_

Each line inside a code chunk named `lir::listing` will be treated as a file name that has to be included verbatim in the weaved final document.
The full contents of the file will be inserted unchanged in a code chunk named [[<<lir::listing::`filename`>>]].
<<lir-listing.awk>>=
<<Setup `awk` to treat lines as single-field records>>
/^@<<lir::listing@>>=$/ {
    while(getline > 0) {
        if (<<End of code chunk>>) {
            print $0
            next
        }
        if (NF > 0) {
            name=$0
            print "@<<lir::listing::`" name "`@>>="
            <<`Awk`: echo file to output>>
        }
    }
}
{ print $0 }
@

To make `awk` treat lines of input as records with one field each, set both the record and the field separator to the newline character.
With these settings, an empty line (nothing but a new line) will have 0 fields (`NF == 0`) and a length of 0 (`length($0) == 0`).
<<Setup `awk` to treat lines as single-field records>>=
BEGIN {
    RS="\n"
    FS="\n"
}
@

The end of a code chunk is either a line with a `@` in the first column, or the beginning of another code chunk.
<<End of code chunk>>=
$0 ~ /^@|^@<<.*@>>=$/
<<`Awk`: echo file to output>>=
while ((getline < name) > 0) {
    print $0
}
close(name)
@

**Warning**: both `lir::listing` and `lir::include` work on lines.
That is, any **non-empty** line is assumed to be a file name.
In other words, if you put a line of one or more white-space characters (spaces or tabs), and since those are valid filenames (in Linux, at least), `lir` will look for that file name and try to list or include that file.

This also means that the filenames included in this fashion cannot contain newline characters.
If you really want this, you should use more advanced trickery.

A custom `noweb` pipeline is used, as we definitely want to be able to use "anonymous" chunks as continuations of the previous chunk.
Since tabs can be significant, for example in Makefiles, those are preserved (the `-t` option to `markup`).
The `-delay` option to `noidx` makes sure that the index of code chunks is emitted before the last document chunk.
This is necessary to be able to generate a bibliography with Pandoc.
<<Source to `noweb` pipeline representation>>=
"$NOWEB_LIBPATH"/markup -t \
    | "$NOWEB_LIBPATH"/emptydefn \
    | "$NOWEB_LIBPATH"/noidx -delay
@

To make it marginally easier to deal with the pipeline representation, we completely remove any empty text tokens from it before tranforming it to Pandoc source.
We then use [[<<nwpipe-pandoc.pl>>]] to transform the `noweb` pipeline representation of the `lir` source file to a valid Pandoc source file.
<<`Noweb` pipeline representation to Pandoc source>>=
sed -n '/^@text $/!p' \
    | "$LIR_LIBPATH"/nwpipe-pandoc
@

<<Pandoc source to HTML>>=
pandoc "$@" \
    --table-of-contents \
    --standalone \
    --self-contained \
    --smart \
    --to=html5 \
    --css="$LIR_LIBPATH"/lir.css \
    --output=-
@

# `Noweb` to Pandoc
The program [[<<nwpipe-pandoc.pl>>]] is written in Prolog, using the SWI-Prolog implementation [@Wielemaker2012].
It is written as a stand-alone command line program: once compiled, can be run from the command line as a part of a pipe.
It uses the module [[<<driver.pl>>]] to do the actual work.
<<nwpipe-pandoc.pl>>=
<<No banner or informational messages>>
<<Expand meta-predicates at compile time>>

:- use_module(driver, [nwpipeline_pandoc/1]).
main :-
    current_prolog_flag(argv, Argv), % command line arguments
    nwpipeline_pandoc(Argv),
    <<Warn about non-determinism>>,
    halt. % exit with success
main :-
    format(user_error, "NWPIPE-PANDOC : FAILED~n", []),
    halt(1). % otherwise, exit with failure (1)
@

Prolog programs are usually used from the "top level" (the Prolog REPL), not as command-line tools.
We need to explicitly turn off the REPL banner and informational messages written to standard output.
<<No banner or informational messages>>=
:- set_prolog_flag(verbose, silent).
@

Meta-predicates (predicates that take other predicates as arguments) are fundamentally slow: they need to evaluate the passed predicate dynamically.
This can be avoided for the more commonly used meta-predicates (`maplist/1+N`, `forall/2` etc) by treating them as macros and expanding them at compile time.
<<Expand meta-predicates at compile time>>=
:- use_module(library(apply_macros)).
@

During development, it is good to know if the program has left behind any unintentional choice points -- they are certainly errors, as the program is designed to succeed deterministically.
They would be "hidden" by the `halt/0` used at the end of the main program.
<<Warn about non-determinism>>=
deterministic(D),
(   D == false
->  format(user_error, "NWPIPE-PANDOC : DANGLING CHOICE POINT(S)!~n", [])
;   true
)
@

The driver uses [[<<nwpipe.pl>>]] to parse its input to native Prolog terms and [[<<lirhtml.pl>>]] to emit a valid Pandoc source document with embedded HTML fragments.
<<driver.pl>>=
:- module(driver, [nwpipeline_pandoc/1]).

:- use_module(nwpipe, [nwpipe_term/2]).
:- use_module(lirhtml, [emit_html/2]).

nwpipeline_pandoc(_) :-
    I = user_input,
    read_string(I, _, S),
    split_string(S, "\r\n", "\r\n", Str_Ls),
    maplist(string_codes, Str_Ls, Ls),
    nwpipe_term(Ls, T),
    emit_html(user_output, T).
@

## Parsing the `noweb` pipeline tokens
This module is used by the [[<<driver.pl>>]] to convert the list of lines representing the line-oriented `noweb` pipeline to a Prolog term.
This is done in two steps.

#. Each line in the list is converted to a term;
#. The flat list of terms is parsed to obtain a term that represents the structure of the document.
<<nwpipe.pl>>=
:- module(nwpipe, [nwpipe_term/2]).

nwpipe_term(Pipe, Doc) :-
    maplist(nwtoken_term, Pipe, Terms),
    phrase(lir_doc(Doc), Terms).

<<`Noweb` token $\rightarrow$ Prolog term>>
<<Flat list of terms $\rightarrow$ structured term>>
@

A `noweb` token in the `noweb` pipeline representation [@Ramsey1992] is parsed to become a Prolog term.
Here, we use an approach to parsing them described in "The Craft of Prolog" [@OKeefe1990].
We use the keyword at the beginning of each token to look up (deterministically) in a table the list of items that this token contains.
Then, each item is converted to a Prolog term using a mini-interpreter for the "language" of `noweb` tokens.
<<`Noweb` token $\rightarrow$ Prolog term>>=
:- use_module(library(dcg/basics), [nonblanks//1, integer//1]).

nwtoken_term([0'@|Token], Term) :-
    phrase(token(Back, Term), Token, Back).

token(Back, Term) -->
    nonblanks(NB),
    {   atom_codes(Key, NB),
        keyword(Key, Items, Back, Term)
    },
    items(Items).
<<Keyword table>>
<<Mini-interpreter for items>>
@

<<Keyword table>>=
<<Structural keywords>>
<<Tagging keywords>>

<<Structural keywords>>=
keyword(begin, [space, chunk_kind(K)], _, K).
keyword(end, [], _, end).
keyword(text, [space, string_rest(S, Rest)], Rest, text(S)).
keyword(nl, [], [], nl).
keyword(defn, [space, string_rest(S, Rest)], Rest, defn(S)).
keyword(use, [space, string_rest(S, Rest)], Rest, use(S)).
keyword(quote, [], [], quote).
keyword(endquote, [], [], endquote).

<<Tagging keywords>>=
keyword(file, [space, atom_rest(File, Rest)], Rest, file(File)).
keyword(xref, [space, xref(XRef, Rest)], Rest, XRef).
keyword(index, [space, index(Index, Rest)], Rest, Index).

<<Cross-referencing keyword table>>
<<Index table>>

<<Cross-referencing keyword table>>=
<<Basic cross-referencing>>
<<Linking previous and next definitions of a code chunk>>
<<Continued definitions of the current chunk>>
<<The list of all code chunks>>
<<Chunks where the code is used>>

<<Index table>>=
index(beginindex, [], [], index_beginindex).
index(endindex, [], [], index_endindex).

<<Basic cross-referencing>>=
xref(label, [space, atom_rest(L, Rest)], Rest, xref_label(L)).
xref(ref, [space, atom_rest(L, Rest)], Rest, xref_ref(L)).

<<Linking previous and next definitions of a code chunk>>=
xref(prevdef, [space, atom_rest(L, Rest)], Rest, xref_prevdef(L)).
xref(nextdef, [space, atom_rest(L, Rest)], Rest, xref_nextdef(L)).

<<Continued definitions of the current chunk>>=
xref(begindefs, [], [], xref_begindefs).
xref(defitem, [space, atom_rest(L, Rest)], Rest, xref_defitem(L)).
xref(enddefs, [], [], xref_enddefs).

<<Chunks where the code is used>>=
xref(beginuses, [], [], xref_beginuses).
xref(useitem, [space, atom_rest(L, Rest)], Rest, xref_useitem(L)).
xref(enduses, [], [], xref_enduses).
xref(notused, [space, string_rest(Name, Rest)], Rest, xref_notused(Name)).

<<The list of all code chunks>>=
xref(beginchunks, [], [], xref_beginchunks).
xref(chunkbegin,
    [space, atom(L), space, string_rest(Name, Rest)],
    Rest,
    xref_chunkbegin(L, Name)).
xref(chunkuse, [space, atom_rest(L, Rest)], Rest, xref_chunkuse(L)).
xref(chunkdefn, [space, atom_rest(L, Rest)], Rest, xref_chunkdefn(L)).
xref(chunkend, [], [], xref_chunkend).
xref(endchunks, [], [], xref_endchunks).
@

Using the tables above, we have looked up the exact items that we expect in a `noweb` token.
This is a small interpreter that takes the list of items and converts each item to the corresponding Prolog term.
<<Mini-interpreter for items>>=
items([]) --> [].
items([I|Is]) -->
    item(I),
    items(Is).

<<Individual items>>
<<Individual items>>=
<<Atoms and text>>
<<"Space">>
<<Chunk number>>
<<Chunk kind>>
<<Cross-reference>>
@

Identifiers are converted to atoms, while "text" (text and names) are converted to strings:
<<Atoms and text>>=
item(atom(A)) -->
    nonblanks(Codes),
    {   atom_codes(A, Codes)
    }.

% Using when/2 here is probably an ugly hack.
% Could not figure out a better way to deal with
% "rest of line" situations.
item(atom_rest(A, Rest)) -->
    {   when(ground(Rest), atom_codes(A, Rest))
    }.

item(string_rest(Str, Rest)) -->
    {   when(ground(Rest), string_codes(Str, Rest))
    }.
@

We assume that [[<<"Space">>]] is represented by a single "space" character; the "Hacker's Guide" is not explicit about this, but so far it has always worked.
<<"Space">>=
item(space) --> [0'\s]. % could it be another "white"?...
@

Integers are used to enumerate the code and documentation:
<<Chunk number>>=
item(chunk_number(N)) --> integer(N).
@

Code and documentation chunks are delimited by the same keywords; the [[<<Chunk kind>>]] is encoded in a secondary keyword:
<<Chunk kind>>=
item(chunk_kind(CK)) -->
    nonblanks(Codes),
    {   atom_codes(CK, Codes)
    }.
@

Finally, [[<<Cross-reference>>]].
Note that it employs quite a few secondary keywords collected in their own look up tables, [[<<Cross-referencing keyword table>>]] and [[<<Index table>>]].
<<Cross-reference>>=
item(xref(XRef, Rest)) -->
    nonblanks(Codes),
    {   atom_codes(X, Codes),
        xref(X, Items, Rest, XRef)
    },
    items(Items).
item(index(Index, Rest)) -->
    nonblanks(Codes),
    {   atom_codes(X, Codes),
        index(X, Items, Rest, Index)
    },
    items(Items).
@

## Converting the pipeline to a structured term
The structure of the document is implicitly present in the `noweb` pipeline representation: documentation and code chunks are delimited by a start and an end token, new lines and other formatting are contained in text tokens, and so on.
The pipeline representation also contains cross-referencing information, using labels and references to them.
It is however a bit inconvenient to use this representation for generating markup.
This is because the order of tokens in the pipeline representation exactly mirrors the layout of the final human-readable documentation, as generated by `noweb`, and `lir` uses a slightly different layout.

At that point, we have a list of Prolog terms.
Those are easier to deal with in the context of Prolog, as it is much easier to make the rules deterministic.
The general approach is to consume the next input and use it as the first argument to a rule, thus taking advantage of Prolog's first-argument indexing.
Then, the rules defined below describe a state machine where each rule is a state and each rule clause is a transition that depends on the last input.

On the highest level, the `noweb` pipeline is made of a sequence of files.
Note, however, that since in `lir` the input is standard input (and not a list of files), there will be only one, unnamed file in the pipeline.
<<Flat list of terms $\rightarrow$ structured term>>=
lir_doc(L) --> [file('')],
    lir_rest(L).

lir_rest(L) --> [X], !,
    lir_file(X, L).
lir_rest([]) --> [].

<<Parse a file>>
@

A file is a sequence of documentation and code chunks.
It will contain a list of chunks and an index.
<<Parse a file>>=
lir_file(docs, L) --> [X],
    docs(X, L).
lir_file(code, [code_meta(C, M)|L]) -->
    code(C, M),
    lir_rest(L).
lir_file(nl, [nl|L]) -->
    lir_rest(L).
lir_file(xref_beginchunks, [chunks_list(Cs)|L]) --> [X],
    xref_chunks(X, Cs),
    lir_rest(L).
lir_file(index_beginindex, [index(I)|L]) --> [X],
    index_list(X, I),
    lir_rest(L).

<<Parse a documentation chunk>>
<<Parse a code chunk>>
<<Parse the list of chunks>>
<<Parse the index>>
@

A documentation chunk can contain text, new lines, and quoted code.
<<Parse a documentation chunk>>=
docs(end, L) -->
    lir_rest(L).
docs(text(T), [text(Text)|L]) -->
    docs_text(Ts),
    {   atomics_to_string([T|Ts], Text)
    },
    [X],
    docs(X, L).
docs(nl, [nl|L]) --> [X],
    docs(X, L).
docs(quote, [quote(Q)|L]) --> [X],
    quote(X, Q),
    [Y],
    docs(Y, L).

<<Parse quoted code>>
@

Consequitive text tokens that are not interrupted by any other structural contents are collected and concatenated.
<<>>=
docs_text([T|Ts]) --> [text(T)], !,
    docs_text(Ts).
docs_text(["\n"|Ts]) --> [nl], !,
    eol(Ts).
docs_text([]) --> [].

eol([T|Ts]) --> [text(T)], !,
    docs_text(Ts).
eol([]) --> [].
@

<<Parse quoted code>>=
quote(text(T), [text(T)|Q]) --> [X],
    quote(X, Q).
quote(xref_ref(L), [quote_use(N, L)|Q]) --> [use(N), X],
    quote(X, Q).
quote(endquote, []) --> [].
@

<<Parse a code chunk>>=
code(Cs, M) -->
    [X],
    defn(X, M_pairs),
    {   dict_create(M, code, M_pairs)
    },
    code_content(Cs).

<<Parse the code chunk header>>
<<Parse the code chunk contents>>
@

<<Parse the code chunk header>>=
defn(nl, []) --> [].
defn(xref_label(L), [label(L)|M]) --> [X],
    defn(X, M).
defn(xref_ref(L), [ref(L)|M]) --> [X],
    defn(X, M).
defn(defn(N), [name(N)|M]) --> [X],
    defn(X, M).
defn(xref_notused(_N), [uses(notused)|M]) --> [X],
    defn(X, M).
defn(xref_beginuses, [uses(Us)|M]) --> [X],
    uses(X, Us, Us),
    [Y],
    defn(Y, M).
defn(xref_prevdef(L), [prev(L)|M]) --> [X],
    defn(X, M).
defn(xref_nextdef(L), [next(L)|M]) --> [X],
    defn(X, M).
defn(language(L), [language(L)|M]) --> [X],
    defn(X, M).
defn(xref_begindefs, [defs(Ds)|M]) --> [X],
    defs(X, Ds, Ds),
    [Y],
    defn(Y, M).

<<Parse defs and uses>>
@

<<Parse the code chunk contents>>=
code_content([text(Text)|Cs]) -->
    text_token(T), !,
    code_text(Ts),
    {   atomics_to_string([T|Ts], Text)
    },
    code_content(Cs).
code_content([code_use(L, R, N)|Cs]) -->
    [xref_label(L), xref_ref(R), use(N)], !,
    code_content(Cs).
code_content([]) --> [end].

text_token("\n") --> [nl].
text_token(T) --> [text(T)].
    
code_text([T|Ts]) -->
    text_token(T), !,
    code_text(Ts).
code_text([]) --> [].
@

<<Parse defs and uses>>=
uses(xref_enduses, _, []) --> [].
uses(xref_useitem(L), Us, [L|Us0]) --> [X],
    uses(X, Us, Us0).

defs(xref_enddefs, _, []) --> [].
defs(xref_defitem(L), Ds, [L|Ds0]) --> [X],
    defs(X, Ds, Ds0).
@

<<Parse the list of chunks>>=
xref_chunks(xref_endchunks, []) --> [].
xref_chunks(xref_chunkbegin(L, N), [chunk(L, N, Us, Ds)|Cs]) --> [X],
    xref_chunk(X, Us, Ds),
    [Y],
    xref_chunks(Y, Cs).

xref_chunk(xref_chunkend, [], []) --> [].
xref_chunk(xref_chunkuse(U), [chunkuse(U)|Us], Ds) --> [X],
    xref_chunk(X, Us, Ds).
xref_chunk(xref_chunkdefn(D), Us, [chunkdefn(D)|Ds]) --> [X],
    xref_chunk(X, Us, Ds).
@

We don't have an index for now.
<<Parse the index>>=
index_list(index_endindex, []) --> [].
@

## Emit Pandoc source
Here:
<<lirhtml.pl>>=
:- module(lirhtml, [emit_html/2]).

:- use_module(library(http/html_write)).

emit_html(Out, L) :-
    formatting(L, F),
    phrase(lir_pandoc(P), F),
    phrase(html(P), H),
    print_html(Out, H).
@

Add to the database a table with all code chunks.
<<>>=
formatting(L, R) :-
    include(is_code_meta, L, L0),
    maplist(code_meta_meta, L0, Ms),
    retractall(label_nr_name_fname(_,_,_,_)),
    forall(nth0(N, Ms, M),
            (   through_pandoc(html5, M.name, FN),
                assertz(label_nr_name_fname(M.label, N, M.name, FN))
            )),
    L = R,
    true.

is_code_meta(code_meta(_, _)).
code_meta_meta(code_meta(_, M), M).
@

Use Pandoc to pre-format chunk names.
<<>>=
:- use_module(library(sgml), [load_structure/3]).
:- use_module(library(sgml_write), [xml_write/3]).

through_pandoc(To, Str, FStr) :-
	atomics_to_string(L, "'", Str), % split
	atomics_to_string(L, "'\\''", EscStr), % put together
	atomics_to_string(["echo '", EscStr, "' | pandoc -t ", To], Pandoc),
	process_create(path(bash), ['-c', Pandoc], [stdout(pipe(Out))]),
	through_pandoc_1(To, Out, FStr),
	close(Out).

through_pandoc_1(html5, Out, FStr) :-
	load_structure(Out, [element(p, [], StrDOM)], [dialect(xml)]),
	with_output_to(string(FStr),
		xml_write(current_output, StrDOM,
			[header(false), layout(false)])).

lir_include("lir::include", C, [div(class(include), Contents)|P], P) :- !,
    maplist(name_filecontents, C, Contents).
lir_include(_, _, P, P).

name_filecontents(text(T), \[FC]) :- !,
    split_string(T, "\n", "\n", [F]),
    setup_call_cleanup(open(F, read, In),
                       read_string(In, _, FC),
                       close(In)).
name_filecontents(Term, Text) :-
    format(string(Text), " * ~w~n", [Term]).

lir_pandoc(P) -->
    [X], !,
    lir_pandoc(X, P).
lir_pandoc([]) --> [].
lir_pandoc(nl, ["\n"|P]) --> lir_pandoc(P).
lir_pandoc(text(T), [\[T]|P]) --> lir_pandoc(P).
lir_pandoc(quote(Q), [span(class(quote), QC)|P]) -->
    {   phrase(lir_pandoc(QC), Q)
    },
    lir_pandoc(P).
lir_pandoc(quote_use(Name, Label),
            [span(class(quoteduse),
                  [&(lang), \thinnbsp,
                   a(href("#~a"-Label), Name),
                   \thinnbsp, &(rang)])|P]) -->
    lir_pandoc(P).
lir_pandoc(code_meta(C, M),
            [div(class(codechunk),
             [\chunk_defn(M),
              \chunk_uses(M),
              \chunk_defs(M),
              \chunk_prev(M),
              pre(\chunk_content(C)),
              \chunk_next(M)])|P]) -->
    lir_pandoc(P).
lir_pandoc(chunks_list(_Cs), P) -->
/*
            ["# List of Chunks\n\n",
             div(class(listofchunks), \chunks_list(Cs))|P]) -->
*/
    lir_pandoc(P).
lir_pandoc(index(_), P) --> [], lir_pandoc(P).

thinnbsp --> html(span(style("white-space:nowrap"), &(thinsp))).

chunk_content([]) --> [].
chunk_content([C|Cs]) -->
    chunk_content_(C, Cs).

chunk_content_(nl, Cs) -->
    html("\n"),
    chunk_content(Cs).
chunk_content_(text(T), Cs) -->
    html(T),
    chunk_content(Cs).
chunk_content_(code_use(L, R, N), Cs) -->
    {   label_nr_name_fname(R, _, N, FN)
    },
    html(span(class(embeddeduse),
              [&(lang), \thinnbsp,
               a([id(L), href("#~a"-R)], \[FN]),
               \thinnbsp, &(rang)])),
    chunk_content(Cs).

chunk_defn(M) -->
    {   label_nr_name_fname(M.label, Nr, _, FName)
    },
    html(span([class(defn),id(M.label)],
              [span(class(chunknr), "C~d:"-Nr), &(nbsp),
               &(lang), \thinnbsp,
               a(href("#~a"-M.label), \[FName]),
               \thinnbsp, &(rang), &(equiv)])).
chunk_uses(M) -->
    {   code{uses:Us} :< M
    }, !,
    html(span(class(chunkuses), \chunk_uses_(Us))).
chunk_uses(_) --> [].

chunk_uses_([U|Us]) -->
    {   label_nr_name_fname(U, Nr, _, _)
    }, !,
    html([br([]), "used by ", a(href("#~a"-U),"C~d"-Nr)]),
    chunk_uses_rest(Us).
chunk_uses_(notused) --> html([br([]), "root chunk"]).
chunk_uses_rest([]) --> [].
chunk_uses_rest([U|Us]) -->
    {   label_nr_name_fname(U, Nr, _, _)
    },
    html([", ", a(href("#~a"-U), "C~d"-Nr)]),
    chunk_uses_rest(Us).

chunk_next(M) -->
    {   code{next:L} :< M,
        label_nr_name_fname(L, Nr, _, _)
    }, !,
    html(span(class(chunknext),
         a(href("#~a"-L), [&(8681), \thinnbsp, "C~d"-Nr]))).
chunk_next(_) -->
    html(&(9633)). % white square

chunk_prev(M) -->
    {   code{prev:L} :< M,
        label_nr_name_fname(L, Nr, _, _)
    },
    html([br([]),
          span(class(chunkprev),
               a(href("#~a"-L), [&(8679), \thinnbsp, "C~d"-Nr]))]).
chunk_prev(_) --> [].

chunk_defs(M) -->
    {   code{defs:Ds} :< M
    }, !,
    html(span(class(chunkdefs), \chunk_defs_(Ds))).
chunk_defs(_) --> [].

chunk_defs_([D|Ds]) -->
    {   label_nr_name_fname(D, Nr, _, _)
    },
    html([br([]), "definition continued in ", a(href("#~a"-D), "C~d"-Nr)]),
    chunk_defs_rest(Ds).
chunk_defs_rest([]) --> [].
chunk_defs_rest([D|Ds]) -->
    {   label_nr_name_fname(D, Nr, _, _)
    },
    html(["+", a(href("#~a"-D), "C~d"-Nr)]),
    chunk_defs_rest(Ds).
@

# Layout
The system tries to separate content and layout as much as possible.
It also aims to provide a sensible default layout for the human-readable documentation.

## HTML
For HTML documentation, [[<<lir.css>>]] is used.
<<lir.css>>=
<<Text width and margin>>
<<Headers in sans-serif>>
<<Quoted code in monospace>>
<<Use names formatting>>
<<List of chunks formatting>>
@

The text width is limited and a left margin is inserted to improve readability.
<<Text width and margin>>=
p {
    max-width: 14cm;
}
ul,
ol,
dl {
    max-width: 12cm;
}
body {
    padding-left: 2cm;
}
@

<<Headers in sans-serif>>=
h1,
h2,
h3 {
    font-family: sans-serif;
}
@

<<Use names formatting>>=
span.chunkdefs a,
span.chunkuses a,
span.chunkprev a,
span.chunknext a,
span.defn a,
span.quoteduse a,
span.embeddeduse a {
    text-decoration-line: None;
}
span.chunkdefs,
span.chunkuses,
span.chunkprev,
span.chunknext {
    font-size: small;
}
span.quoteduse {
    font-family: Serif;
}
span.embeddeduse {
    font-family: Serif;
}
span.chunknr {
    font-weight: bold;
}
@

Use names in quoted code and code chunks are typeset in different colors and are high-lighted when the mouse is over them.
<<>>=
span.quoteduse a {
    color: DarkGreen;
}
span.quoteduse a:hover {
    color: LimeGreen;
}
span.embeddeduse a {
    color: DarkBlue;
    font-style: Italic;
}
span.embeddeduse a:hover {
    color: DodgerBlue;
}
span.chunkdefs a,
span.chunkprev a,
span.chunknext a,
span.defn a,
span.chunkuses a {
    color: DarkRed;
}
span.chunkdefs a:hover,
span.chunkprev a:hover,
span.chunknext a:hover,
span.defn a:hover,
span.chunkuses a:hover {
    color: DarkGoldenRod;
}
@

<<Quoted code in monospace>>=
span.quote {
    font-family: Monospace;
}
@

<<List of chunks formatting>>=
ul.chunkslist {
    list-style-type: square;
}
@

# Common definitions
The portable [[<<Bash header>>]] is, according to [Stack Overflow](http://stackoverflow.com/questions/10376206/preferred-bash-shebang):
<<Bash header>>=
#! /usr/bin/env bash
@
Furthermore, we set the scripts to immediately exit with error on failure, consider failure in a pipe a failure, and to consider "empty" (unset) variables an error:
<<Bash header>>=
set -o errexit
set -o pipefail
set -o nounset
@

# References

