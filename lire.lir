---
title: 'Reproducible computing with `lire`'
author: Boris Vassilev
version: 0.3
rights: (c) 2015 Boris Vassilev, University of Helsinki
...

# Introduction
This is a collection of tools that support programming language-agnostic reproducible computing.

This document is self-hosting: the implementation presented here can be used to produce the final human-readable documentation.
This makes informal validation trivial ("does the final document look right?"), but it also means that the only guarantee is that it implements enough to be able to compile itself.

# Bootstrapping
The following small programs are the components used to provide the functionality of [[<<lire>>]].

The first one, [[<<lire-tangle.sh>>]], is used to tangle a code chunk and, if necessary, deploy it to the working directory, `.lire`:
<<lire-tangle.sh>>=
<<Bash shebang>>
f=$(mktemp)
notangle -t8 -filter emptydefn -R"$1" lire.lir > "$f"
cmp --quiet "$f" .lire/"$1"
if [ $? -eq 0 ]; then
	rm "$f"
else
	mv --verbose "$f" .lire/"$1"
fi
@

This next program, [[<<lire-use.sh>>]], is meant for files that do not need to be tangled; for example, the `lire` source file itself, or a bibliography file maintained separately.
<<lire-use.sh>>=
cmp --quiet "$1" .lire/"$1"
if [ $? -ne 0 ]; then
	cp --verbose "$1" .lire/"$1"
fi
@

Finally, a script [[<<lire-make.sh>>]] that uses these building blocks to put the necessary files in the working directory (all scripts and the Makefile) and make everything:
<<lire-make.sh>>=
for tangled in lire-nwpipe.sh nwpipe-pandoc.pl fsm.pl nwpipe.pl tohtml.pl pandoc-html.sh lire.css Makefile; do
	bash lire-tangle.sh "$tangled"
done
for used in lire.lir lire.bib; do
	bash lire-use.sh "$used"
done
make -C .lire
@

Of course, the building blocks themselves need to be "tangled" first.
Here is the [[<<bootstrap>>]] script:
<<bootstrap>>=
<<Bash shebang>>
for file in lire-tangle.sh lire-use.sh lire-make.sh; do
	notangle -t8 -R"$file" lire.lir > "$file"
done
bash lire-make.sh
@

To tangle and run it, do:

<<lire>>=
@
~~~{.bash}
$ notangle -R"bootstrap" lire.lir > lire && chmod u+x lire && ./lire
~~~

Once `lire` is tangled, unless the [[<<bootstrap>>]] code chunk is changed, re-making everything (while taking care of any changes, and avoiding unnecessary work) should not take more than just running `lire` again.

# The Back End
In its first incarnation, it consist only of a [Pandoc](http://johnmacfarlane.net/pandoc/) back end for [Noweb](https://www.cs.tufts.edu/~nr/noweb/) [@Noweb2008].
The [[<<Back end>>]]  generates `noweb` pipeline representation from a `.lir` source file ([[lire-nwpipe.sh]]), converts it to a document in Pandoc's markdown with embedded markup ([[nwpipe-pandoc]]), and uses Pandoc to compile the final HTML file:
<<Back end>>=
bash lire-nwpipe.sh lire.lir \
	| ./nwpipe-pandoc \
	| bash pandoc-html.sh \
		--bibliography=lire.bib \
		--css=lire.css \
		--output=lire.html
@
It assumes that all documentation chunks in the source document are written in Pandoc's markdown.

In the first stage [[<<lire-nwpipe.sh>>]] takes `.lir` source file(s) and writes to standard output `noweb`'s pipeline representation.
At the moment, this uses `noweb`'s own back end, `markup`, followed by the `emptydefn` and `noidx` filters.
(All empty `@text` lines are removed from the output of `markup`; this should not change the structure of the pipeline or the final result, and it will make it unnecessary to look out for empty `@text` tokens in other parts of the program: `sed -n '/^@text $/!p'`.)
<<lire-nwpipe.sh>>=
<<`Noweb` location>>/markup "$@" \
	| <<`Noweb` location>>/emptydefn \
	| <<`Noweb` location>>/noidx
@

The program [[<<nwpipe-pandoc.pl>>]] is written in Prolog, using the SWI-Prolog implementation [@Wielemaker2012].
It is written as a stand-alone command line program: once compiled, can be run from the command line as a part of a pipe.
<<nwpipe-pandoc.pl>>=
% no messages of type `informational` or `banner`
:- set_prolog_flag(verbose, silent).

main :-
	current_prolog_flag(argv, Argv), % command line arguments
	prompt(_Old, ''), %  turn off prompt
	nwpipeline_to_pandoc(Argv),
	halt. % exit with success
main :-
	halt(1). % otherwise, exit with failure (1)

:- use_module(fsm, [nwpipeline_fsm/0]).

nwpipeline_to_pandoc(_) :- % command line arguments ignored TODO
	nwpipeline_fsm.
@

The final documentation is produced by Pandoc.
The script [[<<pandoc-html.sh>>]] calls Pandoc for you to generate an HTML.
It expects to read Pandoc's markdown, with optional raw HTML embedded in it, from standard input.
It adds any additional arguments you provide it with:
<<pandoc-html.sh>>=
pandoc \
	--table-of-contents \
	--standalone --smart --self-contained \
	--to=html5 \
	"$@"
@

We use the GNU `make` utility with [[<<Makefile>>]] to generate the human readable documentation and generate any files that are required.
<<Makefile>>=
lire.html: lire.lir lire.css lire.bib lire-nwpipe.sh nwpipe-pandoc pandoc-html.sh
	<<Back end>>
	cp lire.html ..

nwpipe-pandoc: nwpipe-pandoc.pl fsm.pl nwpipe.pl tohtml.pl
	swipl --goal=main -o nwpipe-pandoc -c nwpipe-pandoc.pl
@

## Generating Pandoc
Logically, converting `noweb`'s pipeline to valid Pandoc markdown is done by three components, each taking care of one of the following:

- Reading each _token_ in `noweb`'s pipeline representation from standard input and parsing it, to obtain an equivalent Prolog term: [[<<Parse `noweb` tokens>>]];
- A finite state machine that consumes the Prolog terms and emits Pandoc markdown: [[<<Finite State Machine>>]] (it is rather a finite state transducer, as it generates output, but let's not get too formal);
- A writer that transforms each term to the actual emitted markdown: [[<<Emit Pandoc markdown>>]].

### Program structure
The program is separated in three modules implementing the three components.
The driver is implemented in [[<<fsm.pl>>]] and provides the main program loop through [[nwpipeline_fsm/0]], which initializes and starts the [[<<Finite State Machine>>]].
It uses [[next_input/1]] from [[module(nwpipe)]] to read the `noweb` pipeline representation and [[emit/1]] from [[module(tohtml)]] to write Pandoc markdown with embedded HTML markup.
<<fsm.pl>>=
:- module(fsm, [nwpipeline_fsm/0]).

:- use_module(nwpipe, [next_input/1]).
:- use_module(tohtml, [emit/1]).

<<Finite State Machine>>
@

Transforming the `noweb` tokens to Prolog terms is implemented in [[<<nwpipe.pl>>]].
<<nwpipe.pl>>=
:- module(nwpipe, [next_input/1]).

<<Parse `noweb` tokens>>
@

Emitting Pandoc is implemented in [[<<tohtml.pl>>]].
<<tohtml.pl>>=
:- module(tohtml, [emit/1]).

<<Emit Pandoc markdown>>
@

<<Emit Pandoc markdown>>=
<<The ugliness called `through_pandoc/3`>>
<<Emit HTML for Pandoc>>
@

### Parsing `noweb` tokens
To [[<<Parse `noweb` tokens>>]] the standard input is first read linewise with the help of the standard library.
Each line is then converted to a Prolog token.
`Noweb` keywords that are not yet implemented are skipped and reported to the standard error output stream.
<<Parse `noweb` tokens>>=
:- use_module(library(readutil), [read_line_to_codes/2]).
next_input(T) :-
	read_line_to_codes(user_input, Codes),
	(	nwtoken_to_plterm(Codes, T)
	->	true
	;	format(user_error, "BAD TOKEN: ~s~n", [Codes]),
		next_input(T)
	).

<<`Noweb` tokens $\rightarrow$ Prolog terms>>
@

To convert [[<<`Noweb` tokens $\rightarrow$ Prolog terms>>]] we use an approach directly borrowed from the chapter on definite clause grammers in "The Craft of Prolog" [@OKeefe1990, pp. 299-300].

With this approach each of the [[<<Structural keywords>>]] and [[<<Tagging keywords>>]] is associated with a description of how to parse the rest of the token and a Prolog term representing the result. The description is used to parse the [[<<Individual items>>]] to instantiate the result.

The parser for the miniature programs in the second argument of the keyword tables is defined as a Prolog [**d**efinite **c**lause **g**rammar](http://en.wikipedia.org/wiki/Definite_clause_grammar).
A couple of the DCG primitives offered by the SWI-Prolog library [DCG Basics](http://www.swi-prolog.org/pldoc/doc/swi/library/dcg/basics.pl) are used.
<<`Noweb` tokens $\rightarrow$ Prolog terms>>=
nwtoken_to_plterm(end_of_file, end_of_file).
nwtoken_to_plterm([0'@|Token], T) :-
	phrase(token(T), Token).

:- use_module(library(dcg/basics), [nonblanks//1, integer//1]).
token(Term) -->
	nonblanks(NB),
	{	atom_codes(Key, NB),
		keyword(Key, Items, Term)
	},
	items(Items).
<<Structural keywords>>
<<Tagging keywords>>

<<Individual items>>
@

In the table of keywords, the first column is `noweb`'s keyword, the second column represents the items in this token, and the third argument is the resulting Prolog term.
The structure of the Prolog terms mostly mirrors the structure of the tokens as described in "The `noweb` Hacker's Guide" [@Ramsey1992].
The [[<<Structural keywords>>]] represent the documentation and code chunks in the `noweb` source:
<<Structural keywords>>=
keyword(begin,
		[space, chunk_kind(K), space, chunk_number(N)],
		begin_kind_number(K, N)).
keyword(end,
		[space, chunk_kind(K), space, chunk_number(N)],
		end_kind_number(K, N)).
keyword(text, [space, string_rest(S)], text(S)).
keyword(nl, [], nl).
keyword(defn, [space, string_rest(S)], defn(S)).
keyword(use, [space, string_rest(S)], use(S)).
keyword(quote, [], quote).
keyword(endquote, [], endquote).
@

For the [[<<Tagging keywords>>]], most are represented in `noweb`'s pipeline by a single keyword, `xref`.
There are several categories of cross-referencing concepts.
<<Tagging keywords>>=
keyword(file, [space, atom_rest(File)], file(File)).
keyword(xref, [space, xref(XRef)], xref(XRef)).

<<Basic cross-referencing>>
<<Linking previous and next definitions of a code chunk>>
/* <<Continued definitions of the current chunk>> */
/* <<The list of all code chunks>> */
<<Chunks where the code is used>>
@

<<Basic cross-referencing>>=
xref(label, [space, atom_rest(Label)], label(Label)).
xref(ref, [space, atom_rest(Label)], ref(Label)).
@

<<Linking previous and next definitions of a code chunk>>=
xref(prevdef, [space, atom_rest(Label)], prevdef(Label)).
xref(nextdef, [space, atom_rest(Label)], nextdef(Label)).
@

<<Continued definitions of the current chunk>>=
xref(begindefs, [], begindefs).
xref(defitem, [space, atom_rest(Label)], defitem(Label)).
xref(enddefs, [], enddefs).
@

<<Chunks where the code is used>>=
xref(beginuses, [], beginuses).
xref(useitem, [space, atom_rest(Label)], useitem(Label)).
xref(enduses, [], enduses).
xref(notused, [space, string_rest(Name)], notused(Name)).
@

<<The list of all code chunks>>=
xref(beginchunks, [], beginchunks).
xref(chunkbegin,
	[space, atom(Label), space, string_rest(Name)],
	chunkbegin(Label, Name)).
xref(chunkuse, [space, atom_rest(Label)], chunkuse(Label)).
xref(chunkdefn, [space, atom_rest(Label)], chunkdefn(Label)).
xref(chunkend, [], chunkend).
xref(endchunks, [], endchunks).
@

These are the rules used to transform [[<<Individual items>>]] that appear in the `noweb` tokens to Prolog terms.
<<Individual items>>=
items([]) --> [].
items([I|Is]) -->
	item(I),
	items(Is).

<<Atoms and text>>
<<"Space">>
<<Chunk number>>
<<Chunk kind>>
<<Cross-reference>>
<<Helper DCGs>>
@

Identifiers are converted to atoms, while "text" (text and names) are converted to strings:
<<Atoms and text>>=
item(atom_rest(A)) -->
	rest(Codes),
	{	atom_codes(A, Codes)
	}.
item(string_rest(Str)) -->
	rest(Codes),
	{	string_codes(Str, Codes)
	}.
<<Helper DCGs>>=
rest([]) --> [].
rest([C|Cs]) -->
	[C],
	rest(Cs).
@

We assume that [[<<"Space">>]] is represented by a single "space" character; the "Hacker's Guide" is not explicit about this, but so far it has always worked.
<<"Space">>=
item(space) -->
	[0'\s]. % could it be another "white"?...
@

Integers are used to enumerate the code and documentation [[<<Chunk number>>]]:
<<Chunk number>>=
item(chunk_number(N)) -->
	integer(N).
@

Code and documentation chunks are delimited by the same keywords; the [[<<Chunk kind>>]] is encoded in a secondary keyword:
<<Chunk kind>>=
item(chunk_kind(CK)) -->
	nonblanks(Codes),
	{	atom_codes(CK, Codes),
		chunk_kind(CK) % validation; is it necessary?
	}.
<<Helper DCGs>>=
chunk_kind(code). chunk_kind(docs).
@

Finally, [[<<Cross-reference>>]].
Note that it employs quite a few secondary keywords collected in their own look up table.
<<Cross-reference>>=
item(xref(XRef)) -->
	nonblanks(Codes),
	{	atom_codes(X, Codes),
		xref(X, Items, XRef)
	},
	items(Items).
@

### Finite State Machine
To represent the [[<<Finite State Machine>>]] in Prolog we use predicates for states and predicate clauses for arcs (transitions), another idea borrowed from "The Craft of Prolog" [@OKeefe1990, pp. 353].
To avoid having too many arguments in the predicates of the [[<<Finite State Machine>>]], we define [[<<Data structures>>]] that are used to represent the attributes and contents of code chunks.
The input to the state machine are the terms representing the `noweb` tokens, obtained by consecutive calls to [[next_input/1]].
<<Finite State Machine>>=
<<Data structures>>

nwpipeline_fsm :-
	next_input(T),
	start(T).

<<Start>>
<<File>>
<<Begin chunk>>
<<Docs chunk>>
<<Quoted code>>
<<Code chunk>>
@

The attributes of a chunk, or uses inside a chunk, are maintained as Prolog _records_.
We "hide" the difference list pair of a rudimentary _queue_.
<<Data structures>>=
:- use_module(library(record)).
:- record
	codechunk(
		number:integer,
		name:string,
		label:atom,
		ref:atom,
		nextdef:atom,
		prevdef:atom,
		uses:list(atom)),
	use(
		name:string,
		label:atom,
		ref:atom).

queue_init(q(Q,Q)).
queue_back(q(Q,[New|Back]), New, q(Q,Back)).
queue_list(q(Q,[]), Q).
@

The first structural keyword is always `file`:
<<Start>>=
start(file(_File)) :-
	next_input(T),
	file(T).
@

<<File>>=
file(begin_kind_number(K, N)) :-
	begin_kind_number(K, N).
file(file(_File)) :-
	next_input(T),
	file(T).
file(end_of_file).
@

[[<<Begin chunk>>]] is not a real state, since the transition happens without any input; it is here to make the transition deterministic:
<<Begin chunk>>=
begin_kind_number(docs, N) :-
	next_input(T),
	docs(T, N).
begin_kind_number(code, N) :-
	make_codechunk([number(N)], C),
	queue_init(Q),
	next_input(T),
	code(T, C, Q, []).
@

Since the [[<<Docs chunk>>]]s are expected to be valid Pandoc markdown, the contents are simply emitted unchanged to the output:
<<Docs chunk>>=
docs(end_kind_number(docs, N), N) :-
	next_input(T),
	file(T).
docs(nl, N) :-
	format("~n"),
	next_input(T),
	docs(T, N).
docs(text(Text), N) :-
	format("~s", [Text]),
	next_input(T),
	docs(T, N).
docs(quote, N) :-
	next_input(T),
	queue_init(Q),
	quote(T, N, Q, []).
@

For both (inline) [[<<Quoted code>>]] and [[<<Code chunk>>]]s, the contents are first collected before emitted at the end of the quote or code chunk.
Since there are enough similarities between quoted code and proper code chunks, we use the same [[<<Data structures>>]] for both.
It allows us to keep the state machine a bit cleaner.
<<Quoted code>>=
quote(endquote, N, Q, []) :-
	queue_list(Q, L),
	emit(quotedcode(L)),
	next_input(T),
	docs(T, N).
quote(text(Text), N, Q0, XRef) :-
	queue_back(Q0, text(Text), Q),
	next_input(T),
	quote(T, N, Q, XRef).
quote(nl, N, Q0, XRef) :-
	queue_back(Q0, nl, Q),
	next_input(T),
	quote(T, N, Q, XRef).
quote(use(Name), N, Q0, XRef) :-
	make_use([name(Name)|XRef], Use),
	queue_back(Q0, use(Use), Q),
	next_input(T),
	quote(T, N, Q, []).
quote(xref(XRef), N, Q, XRef_rest) :-
	next_input(T),
	quote(T, N, Q, [XRef|XRef_rest]).
@

[[<<Code chunk>>]]s are numbered, have a name, can contain uses, can be labelled and referenced, and so on:
<<Code chunk>>=
code(end_kind_number(code, N), C, Q, []) :-
	codechunk_number(C, N),
	queue_list(Q, [nl|Content]),
	emit(codechunk(C, Content)),
	next_input(T),
	file(T).
code(xref(XRef), C, Q, XRefs) :-
	next_input(T),
	code_xref(XRef, T, C, Q, XRefs).
code(defn(Name), C0, Q, XRef) :-
	set_codechunk_fields([name(Name)|XRef], C0, C),
	next_input(T),
	code(T, C, Q, []).
code(use(Name), C, Q0, XRef) :-
	make_use([name(Name)|XRef], Use),
	queue_back(Q0, use(Use), Q),
	next_input(T),
	code(T, C, Q, []).
code(nl, C, Q0, XRef) :-
	next_input(T),
	queue_back(Q0, nl, Q),
	code(T, C, Q, XRef).
code(text(Text), C, Q0, XRef) :-
	next_input(T),
	queue_back(Q0, text(Text), Q),
	code(T, C, Q, XRef).

code_xref(label(Label), T, C, Q, XRefs) :-
	code(T, C, Q, [label(Label)|XRefs]).
code_xref(ref(Label), T, C, Q, XRefs) :-
	code(T, C, Q, [ref(Label)|XRefs]).
code_xref(prevdef(Label), T, C, Q, XRefs) :-
	codechunk_prevdef(C, Label),
	code(T, C, Q, XRefs).
code_xref(nextdef(Label), T, C, Q, XRefs) :-
	codechunk_nextdef(C, Label),
	code(T, C, Q, XRefs).
code_xref(notused(_Name), T, C, Q, XRefs) :-
	codechunk_uses(C, []),
	code(T, C, Q, XRefs).
code_xref(beginuses, xref(T), C, Q, XRefs) :-
	code_xref_uses(T, C, Q, XRefs, []).

code_xref_uses(useitem(I), C, Q, XRefs, Use_items) :-
	next_input(xref(T)),
	code_xref_uses(T, C, Q, XRefs, [I|Use_items]).
code_xref_uses(enduses, C, Q, XRefs, Use_items) :-
	codechunk_uses(C, Use_items),
	next_input(T),
	code(T, C, Q, XRefs).
@


### Writing Pandoc's Markdown
Unfortunately, Pandoc's markdown is not expressive enough to allow code chunks in the literate program to be expressed in pure markdown.
Instead, native markup for each supported front end is used.
To [[<<Emit Pandoc's markdown>>]], the single predicate [[emit/1]] is used, with one clause for each possible output, as produced by the finite state machine.
There is [[<<The ugliness called `through_pandoc/3`>>]], which uses Pandoc to generate correct markup for the layout of elements that are unusual for normal documents, but necessary for a literate program.
At the moment, only a Pandoc document that can be compiled to an HTML page is generated.

A Prolog library is used to [[<<Emit HTML for Pandoc>>]].
<<Emit HTML for Pandoc>>=
:- use_module(library(http/html_write),
		[	html//1,
			html_begin//1,
			html_end//1,
			print_html/1
		]).

<<Data structures>>
<<Emit HTML for a code chunk>>
<<Emit HTML for quoted code>>
name_ref(Name, Ref, Name_ref) :-
	(	dif(Ref, 'nw@notdef')
	->	Name_ref = a(href="#~a"-Ref, Name)
	;	Name_ref = Name
	).
@

Code chunks are put in a `<div>` tag with a class `codechunk`, with `noweb`'s `defn` at the top, inside its own `<span>` tag with a class `nwdefn`, and all content in a `<pre>` tag.
<<Emit HTML for a code chunk>>=
codechunk(Content) -->
	html_begin(pre),
	codechunk_items(Content),
	html_end(pre).
codechunk_items([]) --> [].
codechunk_items([H|T]) -->
	codechunk_item(H),
	codechunk_items(T).

codechunk_item(nl) --> [nl(1)].
codechunk_item(text(Text)) --> [Text].
codechunk_item(use(Use)) -->
	{	use_name(Use, Name),
		use_label(Use, Label),
		use_ref(Use, Ref),
		through_pandoc(html5, Name, F)
	},
	html(span([class=nwuse, id=Label], [
				&(lang), &(nbsp),
				a(href="#~a"-Ref, \[F]), % add directly to the output
				&(nbsp), &(rang)])).

emit(codechunk(Attr, Content)) :-
	codechunk_name(Attr, Name),
	codechunk_label(Attr, Label),
	codechunk_uses(Attr, Uses),
	codechunk_prevdef(Attr, Prevdef),
	codechunk_nextdef(Attr, Nextdef),
	Codechunk = div(class=codechunk,
		[	span([class=nwdefn, id=Label],
				[	&(lang), &(nbsp),
					Name,
					&(nbsp), &(rang), &(equiv)
				]),
			br([]),
			\uses(Uses),
			br([]),
			\prevdef(Prevdef),
			\codechunk(Content),
			\nextdef(Nextdef)
		]),
	phrase(html(Codechunk), HTML),
	print_html(HTML).
@

Quoted code is put in a `<span>` with a class `quotedcode`.
<<Emit HTML for quoted code>>=

emit(quotedcode(Q)) :-
	maplist(quotedcode_html, Q, Content),
	phrase(html(span(class=quotedcode, Content)),
		HTML),
	print_html(HTML).


quotedcode_html(nl, '\n').
quotedcode_html(text(Text), Text).
quotedcode_html(use(Use), span(class=nwquoteduse,
		[	&(lang), &(nbsp),
			Name_ref,
			&(nbsp), &(rang)
		])) :-
	use_ref(Use, Ref),
	use_name(Use, Name),
	name_ref(Name, Ref, Name_ref).

uses(Uses) -->
	{	nonvar(Uses)
	}, !,
	html_begin(span(class(uses))),
	uses_links(Uses),
	html_end(span).

uses(_) --> uses_links([]).

uses_links([H|T]) -->
	html(["Used ", a(href="#~a"-H, "here")]),
	uses_links_rest(T).
uses_links([]) --> html(["Root chunk"]).

uses_links_rest([]) --> [].
uses_links_rest([H|T]) --> uses_links_rest_1(T, H).
uses_links_rest_1([], Last) --> html([", and ", a(href="#~a"-Last, "here")]).
uses_links_rest_1([H|T], Prev) --> html([", ", a(href="#~a"-Prev, "here")]),
	uses_links_rest_1(T, H).

prevdef(Prevdef) --> { nonvar(Prevdef) }, !,
	html(span(class=prevdef, [". . . ", a(href="#~a"-Prevdef, "cont.")])).
prevdef(_) --> [].

nextdef(Nextdef) --> { nonvar(Nextdef) }, !,
	html(span(class=nextdef, [a(href="#~a"-Nextdef, "more"), " . . ."])).
nextdef(_) --> [].

@

Transforming [[<<`Noweb` $\rightarrow$ HTML>>]] could be quite straight-forward: just map the `noweb` token to an HTML tag with the appropriate attributes.
For quoted code, this is good enough.
Apparently, Pandoc does recognize markdown inside `<span>` contents, and adds the necessary HTML markup.

For example, we can put a "use" inside quoted code, complete with its own Pandoc markdown, as in:
<<foo>>=
[[quoted @<<foo $\rightarrow \pi =$ `bar`@>>]]
@ Which will turn into: [[quoted <<foo $\rightarrow \pi =$ `bar`>>]]

Unfortunately, it does not do that for the contents of `<span>` which is inside a `<pre>` (I guess rightfully so?).

[[<<The ugliness called `through_pandoc/3`>>]] works as follows:

To make the contents of code chunks look like code, they are put inside a `<pre>` tag.
But in a `lire` source file, a code chunk can contain a [[<<Use>>]], which will have to be typeset properly.
Pandoc will not do this automatically for a `<span>` inside raw HTML `<pre>`.
The hacky solution here is to pass the Name in a [[<<Use>>]] (formatted in markdown) to Pandoc, and embed the resulting markup in the output.

#. Turn single quotes -- "`'`" -- into "`'\''`" sequences;
#. Construct the pipe;
#. Call it as a `bash` process;
#. Parse the output, removing the enclosing `<p>`;
#. Convert back to HTML
<<The ugliness called `through_pandoc/3`>>=
:- use_module(library(sgml), [load_structure/3]).
:- use_module(library(sgml_write), [xml_write/3]).

through_pandoc(To, Str, FStr) :-
	atomics_to_string(L, "'", Str), % split
	atomics_to_string(L, "'\\''", EscStr), % put together
	atomics_to_string(["echo '", EscStr, "' | pandoc -t ", To], Pandoc),
	process_create(path(bash), ['-c', Pandoc], [stdout(pipe(Out))]),
	through_pandoc_1(To, Out, FStr),
	close(Out).

through_pandoc_1(html5, Out, FStr) :-
	load_structure(Out, [element(p, [], StrDOM)], [dialect(xml)]),
	with_output_to(string(FStr),
		xml_write(current_output, StrDOM,
			[header(false), layout(false)])).
@

## Layout
The system tries to separate content and layout as much as possible.
It also aims to provide a sensible default layout for the human-readable documentation.

### HTML
For HTML documentation, [[<<lire.css>>]] is used. Here it is:
<<lire.css>>=
span.nwuse a, span.uses a, span.nwquoteduse a, span.prevdef a, span.nextdef a {
	color: forestgreen;
	text-decoration-line: none;
}
h1, h2, h3 {
	font-family: sans-serif;
}
p {
	max-width: 14cm;
}
ul {
	max-width: 12cm;
}
body {
	padding-left: 2cm;
}
span.quotedcode {
	font-family: monospace;
}
span.nwquoteduse {
	font-family: serif;
}
span.nwuse {
	font-family: serif;
	font-style: italic;
}
span.nwdefn {
	font-family: serif;
	font-weight: bold;
}
span.uses, span.prevdef, span.nextdef {
	font-size: small;
}
@

# Common definitions
All building block for `noweb` (front ends, filters, back ends) are usually installed in the same [[<<`Noweb` location>>]]:
<<`Noweb` location>>=
/home/boris/lib/noweb
@

The portable [[<<Bash shebang>>]] is, according to [Stack Overflow](http://stackoverflow.com/questions/10376206/preferred-bash-shebang):
<<Bash shebang>>=
#! /usr/bin/env bash
@

# References

