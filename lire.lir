---
title: 'Reproducible computing with `lire`'
author: Boris Vassilev
version: 0.3
rights: (c) 2015 Boris Vassilev, University of Helsinki
...

# Introduction
This is a collection of tools that support programming language-agnostic reproducible computing.

This document is self-hosting: the implementation presented here can be used to produce the final human-readable documentation.
This makes informal validation trivial ("does the final document look right?"), but it also means that the only guarantee is that it implements enough to be able to compile itself.

It might or might not have [[quoted code with <<uses>>, or maybe without any]].
I haven't decided on it yet.

# Bootstrapping
The following small programs are the components used to provide the functionality of [[lire]].

The first one, [[<<lire-tangle.sh>>]], is used to tangle a code chunk and, if necessary, deploy it to the working directory, `.lire`:
<<lire-tangle.sh>>=
<<Bash shebang>>
f=$(mktemp)
notangle -t8 -filter emptydefn -R"$1" lire.lir > "$f"
cmp --quiet "$f" .lire/"$1"
if [ $? -eq 0 ]; then
    rm "$f"
else
    echo "Updating $1 (changed)"
    mv "$f" .lire/"$1"
fi
@

This next program, [[<<lire-use.sh>>]], is meant for files that do not need to be tangled; for example, the `lire` source file itself, or a bibliography file maintained separately.
<<lire-use.sh>>=
cmp --quiet "$1" .lire/"$1"
if [ $? -ne 0 ]; then
    echo "Updating $1 (changed)"
    cp "$1" .lire/"$1"
fi
@

Finally, a script [[<<lire-make.sh>>]] that uses these building blocks to put the necessary files in the working directory (all scripts and the Makefile) and make everything:
<<lire-make.sh>>=
for tangled in lire-nwpipe.sh nwpipe-pandoc.pl driver.pl nwpipe.pl lirehtml.pl pandoc-html.sh lire.css Makefile; do
    bash lire-tangle.sh "$tangled"
done
for used in lire.lir lire.bib; do
    bash lire-use.sh "$used"
done
make -C .lire
@

Of course, the building blocks themselves need to be "tangled" first.
Here is the [[<<bootstrap>>]] script:
<<bootstrap>>=
<<Bash shebang>>
for file in lire-tangle.sh lire-use.sh lire-make.sh; do
    notangle -t8 -R"$file" lire.lir > "$file"
done
bash lire-make.sh
@

To tangle and run it, do:

<<lire>>=
$ notangle -R"bootstrap" lire.lir > lire && chmod u+x lire && ./lire emithtml.pl
@

Once `lire` is tangled, unless the [[<<bootstrap>>]] code chunk is changed, re-making everything (while taking care of any changes, and avoiding unnecessary work) should not take more than just running `lire` again.

# The Back End
In its first incarnation, it consist only of a [Pandoc](http://johnmacfarlane.net/pandoc/) back end for [Noweb](https://www.cs.tufts.edu/~nr/noweb/) [@Noweb2008].
The [[<<Back end>>]]  generates `noweb` pipeline representation from a `.lir` source file ([[lire-nwpipe.sh]]), converts it to a document in Pandoc's markdown with embedded markup ([[nwpipe-pandoc]]), and uses Pandoc to compile the final HTML file:
<<Back end>>=
bash lire-nwpipe.sh lire.lir \
    | ./nwpipe-pandoc \
    | bash pandoc-html.sh \
        --bibliography=lire.bib \
        --css=lire.css \
        --output=lire.html
@
It assumes that all documentation chunks in the source document are written in Pandoc's markdown.

In the first stage [[<<lire-nwpipe.sh>>]] takes `.lir` source file(s) and writes to standard output `noweb`'s pipeline representation.
At the moment, this uses `noweb`'s own back end, `markup`, followed by the `emptydefn` and `noidx` filters.
All empty `@text` lines are removed from the output of the `noweb` components pipe; this should not change the structure of the pipeline or the final result, and it will make it unnecessary to look out for empty `@text` tokens in other parts of the program:
<<lire-nwpipe.sh>>=
<<`Noweb` location>>/markup "$@" \
    | <<`Noweb` location>>/emptydefn \
    | <<`Noweb` location>>/noidx -delay \
    | sed -n '/^@text $/!p'
@

The program [[<<nwpipe-pandoc.pl>>]] is written in Prolog, using the SWI-Prolog implementation [@Wielemaker2012].
It is written as a stand-alone command line program: once compiled, can be run from the command line as a part of a pipe.
<<nwpipe-pandoc.pl>>=
% no messages of type `informational` or `banner`
:- set_prolog_flag(verbose, silent).

:- use_module(driver, [nwpipeline_pandoc/1]).
main :-
    current_prolog_flag(argv, Argv), % command line arguments
    prompt(_Old, ''), %  turn off prompt
    nwpipeline_pandoc(Argv),
    <<Warn about non-determinism>>,
    halt. % exit with success
main :-
    format(user_error, "FAILURE~n", []),
    halt(1). % otherwise, exit with failure (1)
@

During development, it is good to know if the program has left behind any unintentional choice points -- they are almost certainly errors.
They would be "hidden" by the `halt/0` used at the end of the main program.
<<Warn about non-determinism>>=
deterministic(D),
(   D == false
->  format(user_error, "DANGLING CHOICE POINT(S)!~n", [])
;   true
)
@

The final documentation is produced by Pandoc.
The script [[<<pandoc-html.sh>>]] calls Pandoc for you to generate an HTML.
It expects to read Pandoc's markdown, with optional raw HTML embedded in it, from standard input.
It adds any additional arguments you provide it with:
<<pandoc-html.sh>>=
pandoc \
    --table-of-contents \
    --standalone --smart --self-contained \
    --to=html5 \
    "$@"
@

We use the GNU `make` utility with [[<<Makefile>>]] to generate the human readable documentation and generate any files that are required.
<<Makefile>>=
lire.html: lire.lir lire.css lire.bib lire-nwpipe.sh nwpipe-pandoc pandoc-html.sh
	<<Back end>>
	cp lire.html ..

nwpipe-pandoc: nwpipe-pandoc.pl driver.pl nwpipe.pl lirehtml.pl
	swipl --goal=main -o nwpipe-pandoc -c nwpipe-pandoc.pl
@

## From `lire` source to documentation

The driver:
<<driver.pl>>=
:- module(driver, [nwpipeline_pandoc/1]).

:- use_module(nwpipe, [nwtokens_terms/2, lire//1]).
:- use_module(lirehtml, [emit_html/2]).

nwpipeline_pandoc(_) :-
    I = user_input,
    input_to_terms(I, Ts),
    open('terms.txt', write, Terms_file),
    forall(member(T, Ts), format(Terms_file, "~q~n", [T])),
    close(Terms_file),
    phrase(lire(L), Ts),
    setup_call_cleanup(open('doc.txt', write, Doc_file),
            (   maplist(format_to_string("~q~n"), L, LS),
                atomics_to_string(LS, S),
                format(Doc_file, "~s~n", [S])
            ),
            %write_lire(L, Doc_file),
            close(Doc_file)),
    setup_call_cleanup(open('weaved.pandoc', write, Pandoc_file),
            emit_html(Pandoc_file, L),
            close(Pandoc_file)),
    emit_html(user_output, L).

format_to_string(F, T, S) :-
    format(string(S), F, [T]).

write_lire([], _).
write_lire([H|T], Out) :-
    format(Out, "~q~n", [H]),
    write_lire(T, Out).

input_to_terms(I, Ts) :-
    read_string(I, _, S),
    split_string(S, "\r\n", "\r\n", Str_Ls),
    maplist(string_codes, Str_Ls, Ls),
    nwtokens_terms(Ls, Ts).
@

<<lirehtml.pl>>=
:- module(lirehtml, [emit_html/2]).

:- use_module(library(http/html_write)).
:- multifile html_write:expand//1.

emit_html(Out, L) :-
    formatting(L, F),
    phrase(lire_pandoc(P), F),
    phrase(html(P), H),
    print_html(Out, H).
@

Add to the database a table with all code chunks.
<<>>=
formatting(L, L) :-
    code_metas(L, Ms),
    retractall(label_nr_name_fname(_,_,_,_)),
    forall(nth1(N, Ms, M),
            (   through_pandoc(html5, M.name, FN),
                assertz(label_nr_name_fname(M.label, N, M.name, FN))
            )).

code_metas([], []).
code_metas([H|T], Ms) :-
    (   H = code_meta(_, M)
    ->  Ms = [M|Ms0]
    ;   Ms = Ms0
    ),
    code_metas(T, Ms0).
@

Use Pandoc to pre-format chunk names.
<<>>=
:- use_module(library(sgml), [load_structure/3]).
:- use_module(library(sgml_write), [xml_write/3]).

through_pandoc(To, Str, FStr) :-
	atomics_to_string(L, "'", Str), % split
	atomics_to_string(L, "'\\''", EscStr), % put together
	atomics_to_string(["echo '", EscStr, "' | pandoc -t ", To], Pandoc),
	process_create(path(bash), ['-c', Pandoc], [stdout(pipe(Out))]),
	through_pandoc_1(To, Out, FStr),
	close(Out).

through_pandoc_1(html5, Out, FStr) :-
	load_structure(Out, [element(p, [], StrDOM)], [dialect(xml)]),
	with_output_to(string(FStr),
		xml_write(current_output, StrDOM,
			[header(false), layout(false)])).

lire_pandoc(P) -->
    [X], !,
    lire_pandoc(X, P).
lire_pandoc([]) --> [].
lire_pandoc(nl, ["\n"|P]) --> lire_pandoc(P).
lire_pandoc(text(T), [T|P]) --> lire_pandoc(P).
lire_pandoc(quote(Q), [span(class(quote), QC)|P]) -->
    {   phrase(lire_pandoc(QC), Q)
    },
    lire_pandoc(P).
lire_pandoc(quote_use(Name, Label),
            [span(class(quoteduse),
                  [&(lang), \thinnbsp,
                   a(href("#~a"-Label), Name),
                   \thinnbsp, &(rang)])|P]) -->
    lire_pandoc(P).
lire_pandoc(code_meta(C, M),
            [div(class(codechunk),
             [\chunk_defn(M),
              \chunk_uses(M),
              \chunk_defs(M),
              \chunk_prev(M),
              pre(CC),
              \chunk_next(M)])|P]) -->
    {   phrase(lire_pandoc(CC), C)
    },
    lire_pandoc(P).
lire_pandoc(code_use(L, R, N),
            [span(class(embeddeduse),
                  [&(lang), \thinnbsp,
                   a([id(L), href("#~a"-R)], \[FN]),
                   \thinnbsp, &(rang)])|P]) -->
    {   label_nr_name_fname(R, _, N, FN)
    },
    lire_pandoc(P).
lire_pandoc(chunks_list(_Cs), P) -->
/*
            ["# List of Chunks\n\n",
             div(class(listofchunks), \chunks_list(Cs))|P]) -->
*/
    lire_pandoc(P).
lire_pandoc(index(_), P) --> [], lire_pandoc(P).

thinnbsp --> html(span(style("white-space:nowrap"), &(thinsp))).

chunk_defn(M) -->
    {   label_nr_name_fname(M.label, Nr, _, FName)
    },
    html(span([class(defn), id(M.label)],
              [span(class(chunknr), "C~d:"-Nr), &(nbsp),
               &(lang), \thinnbsp,
               a(href("#~a"-M.label), \[FName]),
               \thinnbsp, &(rang), &(equiv)])).
chunk_uses(M) -->
    {   code{uses:Us} :< M
    }, !,
    html(span(class(chunkuses), \chunk_uses_(Us))).
chunk_uses(_) --> [].
chunk_uses_([U|Us]) -->
    {   label_nr_name_fname(U, Nr, _, _)
    }, !,
    html([br([]), "used by ", a(href("#~a"-U),"C~d"-Nr)]),
    chunk_uses_rest(Us).
chunk_uses_(notused) --> html([br([]), "root chunk"]).
chunk_uses_rest([]) --> [].
chunk_uses_rest([U|Us]) -->
    {   label_nr_name_fname(U, Nr, _, _)
    },
    html([", ", a(href("#~a"-U), "C~d"-Nr)]),
    chunk_uses_rest(Us).

chunk_next(M) -->
    {   code{next:L} :< M,
        label_nr_name_fname(L, Nr, _, _)
    }, !,
    html(span(class(chunknext),
         a(href("#~a"-L), [&(8681), \thinnbsp, "C~d"-Nr]))).
chunk_next(_) -->
    html(&(9646)).

chunk_prev(M) -->
    {   code{prev:L} :< M,
        label_nr_name_fname(L, Nr, _, _)
    },
    html([br([]),
          span(class(chunkprev),
               a(href("#~a"-L), [&(8679), \thinnbsp, "C~d"-Nr]))]).
chunk_prev(_) --> [].

chunk_defs(M) -->
    {   code{defs:Ds} :< M
    }, !,
    html(span(class(chunkdefs), \chunk_defs_(Ds))).
chunk_defs(_) --> [].

chunk_defs_([D|Ds]) -->
    {   label_nr_name_fname(D, Nr, _, _)
    },
    html([br([]), "definition continued in ", a(href("#~a"-D), "C~d"-Nr)]),
    chunk_defs_rest(Ds).
chunk_defs_rest([]) --> [].
chunk_defs_rest([D|Ds]) -->
    {   label_nr_name_fname(D, Nr, _, _)
    },
    html(["+", a(href("#~a"-D), "C~d"-Nr)]),
    chunk_defs_rest(Ds).
    
html_write:expand(ref_name(R, N)) -->
    html(a(href("#~a"-R), N)).
html_write:expand(chunks_list(Cs)) -->
    html([text("\n# List of chunks\n"), ul(class(chunkslist), Cs)]).
html_write:expand(chunk(L, N, Us, Ds)) -->
    html(li([ref_name(L, N), xrefchunkuses(Us), xrefchunkdefns(Ds)])).
html_write:expand(xrefchunkuses(Us)) -->
    html(["Used:"|Us]).
html_write:expand(chunkuse(U)) -->
    html(a(href("#~a"-U), " here")).
html_write:expand(xrefchunkdefns(Ds)) -->
    html(["Defined:"|Ds]).
html_write:expand(chunkdefn(D)) -->
    html(a(href("#~a"-D), " here")).
html_write:expand(index(_)) --> [].
@

Parsing the pipeline:
<<nwpipe.pl>>=
:- module(nwpipe, [nwtokens_terms/2, lire//1]).

nwtokens_terms([], []).
nwtokens_terms([[0'@|Token]|Tokens], [Term|Terms]) :-
    phrase(token(Back, Term), Token, Back),
    %format(user_error, "~q~n", [Term]),
    nwtokens_terms(Tokens, Terms).

<<`Noweb` tokens $\rightarrow$ Prolog terms>>
<<Build the document structure>>
@

<<`Noweb` tokens $\rightarrow$ Prolog terms>>=
:- use_module(library(dcg/basics), [nonblanks//1, integer//1]).
token(Back, Term) -->
    nonblanks(NB),
    {   atom_codes(Key, NB),
        keyword(Key, Items, Back, Term)
    },
    items(Items).
<<Structural keywords>>
<<Tagging keywords>>

<<Individual items>>
@

<<Structural keywords>>=
keyword(begin, [space, chunk_kind(K)]/*, space, chunk_number(N)]*/, _, K).
keyword(end, []/*[space, chunk_kind(_K), space, chunk_number(N)]*/, _, end).
keyword(text, [space, string_rest(S, Rest)], Rest, text(S)).
keyword(nl, [], [], nl).
keyword(defn, [space, string_rest(S, Rest)], Rest, defn(S)).
keyword(use, [space, string_rest(S, Rest)], Rest, use(S)).
keyword(quote, [], [], quote).
keyword(endquote, [], [], endquote).
@

For the [[<<Tagging keywords>>]], most are represented in `noweb`'s pipeline by a single keyword, `xref`.
There are several categories of cross-referencing concepts.
<<Tagging keywords>>=
keyword(file, [space, atom_rest(File, Rest)], Rest, file(File)).
keyword(xref, [space, xref(XRef, Rest)], Rest, XRef).
keyword(index, [space, index(Index, Rest)], Rest, Index).

<<Basic cross-referencing>>
<<Linking previous and next definitions of a code chunk>>
<<Continued definitions of the current chunk>>
<<The list of all code chunks>>
<<Chunks where the code is used>>

<<The index of identifiers>>
@

<<Basic cross-referencing>>=
xref(label, [space, atom_rest(L, Rest)], Rest, xref_label(L)).
xref(ref, [space, atom_rest(L, Rest)], Rest, xref_ref(L)).
@

<<Linking previous and next definitions of a code chunk>>=
xref(prevdef, [space, atom_rest(L, Rest)], Rest, xref_prevdef(L)).
xref(nextdef, [space, atom_rest(L, Rest)], Rest, xref_nextdef(L)).
@

<<Continued definitions of the current chunk>>=
xref(begindefs, [], [], xref_begindefs).
xref(defitem, [space, atom_rest(L, Rest)], Rest, xref_defitem(L)).
xref(enddefs, [], [], xref_enddefs).
@

<<Chunks where the code is used>>=
xref(beginuses, [], [], xref_beginuses).
xref(useitem, [space, atom_rest(L, Rest)], Rest, xref_useitem(L)).
xref(enduses, [], [], xref_enduses).
xref(notused, [space, string_rest(Name, Rest)], Rest, xref_notused(Name)).
@

<<The list of all code chunks>>=
xref(beginchunks, [], [], xref_beginchunks).
xref(chunkbegin,
    [space, atom(L), space, string_rest(Name, Rest)],
    Rest,
    xref_chunkbegin(L, Name)).
xref(chunkuse, [space, atom_rest(L, Rest)], Rest, xref_chunkuse(L)).
xref(chunkdefn, [space, atom_rest(L, Rest)], Rest, xref_chunkdefn(L)).
xref(chunkend, [], [], xref_chunkend).
xref(endchunks, [], [], xref_endchunks).
@

<<The index of identifiers>>=
index(beginindex, [], [], index_beginindex).
index(endindex, [], [], index_endindex).
@

These are the rules used to transform [[<<Individual items>>]] that appear in the `noweb` tokens to Prolog terms.
<<Individual items>>=
items([]) --> [].
items([I|Is]) -->
    item(I),
    items(Is).

<<Atoms and text>>
<<"Space">>
<<Chunk number>>
<<Chunk kind>>
<<Cross-reference>>
@

Identifiers are converted to atoms, while "text" (text and names) are converted to strings:
<<Atoms and text>>=
item(atom(A)) -->
    nonblanks(Codes),
    {   atom_codes(A, Codes)
    }.

% Using when here is probably an ugly hack.
% Could not figure out a better way to deal with
% "rest of line" situations.
item(atom_rest(A, Rest)) -->
    {   when(ground(Rest), atom_codes(A, Rest))
    }.

item(string_rest(Str, Rest)) -->
    {   when(ground(Rest), string_codes(Str, Rest))
    }.
@

We assume that [[<<"Space">>]] is represented by a single "space" character; the "Hacker's Guide" is not explicit about this, but so far it has always worked.
<<"Space">>=
item(space) --> [0'\s]. % could it be another "white"?...
@

Integers are used to enumerate the code and documentation [[<<Chunk number>>]]:
<<Chunk number>>=
item(chunk_number(N)) --> integer(N).
@

Code and documentation chunks are delimited by the same keywords; the [[<<Chunk kind>>]] is encoded in a secondary keyword:
<<Chunk kind>>=
item(chunk_kind(CK)) -->
    nonblanks(Codes),
    {   atom_codes(CK, Codes)
    }.
@

Finally, [[<<Cross-reference>>]].
Note that it employs quite a few secondary keywords collected in their own look up table.
<<Cross-reference>>=
item(xref(XRef, Rest)) -->
    nonblanks(Codes),
    {   atom_codes(X, Codes),
        xref(X, Items, Rest, XRef)
    },
    items(Items).
item(index(Index, Rest)) -->
    nonblanks(Codes),
    {   atom_codes(X, Codes),
        index(X, Items, Rest, Index)
    },
    items(Items).
@

<<Build the document structure>>=
lire(L) -->
    [file(_)],
    lire_rest(L).

lire_rest(L) -->
    [X], !,
    lire_file(X, L).
lire_rest([]) --> [].

lire_file(docs, L) -->
    [X],
    docs(X, L).
lire_file(code, [code_meta(C, M)|L]) -->
    code(C, M),
    lire_rest(L).
lire_file(nl, [nl|L]) -->
    lire_rest(L).
lire_file(xref_beginchunks, [chunks_list(Cs)|L]) -->
    [X],
    xref_chunks(X, Cs),
    lire_rest(L).
lire_file(index_beginindex, [index(I)|L]) -->
    [X],
    index_list(X, I),
    lire_rest(L).
@

<<Build the document structure>>=
docs(end, L) -->
    lire_rest(L).
docs(text(T), [text(Text)|L]) -->
    docs_text(Ts),
    {   atomics_to_string([T|Ts], Text)
    },
    [X],
    docs(X, L).
docs(nl, [nl|L]) -->
    [X],
    docs(X, L).
docs(quote, [quote(Q)|L]) -->
    [X],
    quote(X, Q),
    [Y],
    docs(Y, L).

quote(text(T), [text(T)|Q]) -->
    [X],
    quote(X, Q).
quote(xref_ref(L), [quote_use(N, L)|Q]) -->
    [use(N), X],
    quote(X, Q).
quote(endquote, []) --> [].
@

<<Build the document structure>>=
code(Cs, M) -->
    [X],
    defn(X, M_pairs),
    {   dict_create(M, code, M_pairs)
    },
    code_content(Cs).

defn(nl, []) --> [].
defn(xref_label(L), [label(L)|M]) -->
    [X],
    defn(X, M).
defn(xref_ref(L), [ref(L)|M]) -->
    [X],
    defn(X, M).
defn(defn(N), [name(N)|M]) -->
    [X],
    defn(X, M).
defn(xref_notused(_N), [uses(notused)|M]) -->
    [X],
    defn(X, M).
defn(xref_beginuses, [uses(Us)|M]) -->
    [X],
    uses(X, Us, Us),
    [Y],
    defn(Y, M).
defn(xref_prevdef(L), [prev(L)|M]) -->
    [X],
    defn(X, M).
defn(xref_nextdef(L), [next(L)|M]) -->
    [X],
    defn(X, M).
defn(language(L), [language(L)|M]) -->
    [X],
    defn(X, M).
defn(xref_begindefs, [defs(Ds)|M]) -->
    [X],
    defs(X, Ds, Ds),
    [Y],
    defn(Y, M).
@

<<Build the document structure>>=
uses(xref_enduses, _, []) --> [].
uses(xref_useitem(L), Us, [L|Us0]) -->
    [X],
    uses(X, Us, Us0).

defs(xref_enddefs, _, []) --> [].
defs(xref_defitem(L), Ds, [L|Ds0]) -->
    [X],
    defs(X, Ds, Ds0).

code_content([text(Text)|Cs]) -->
    text_token(T), !,
    code_text(Ts),
    {   atomics_to_string([T|Ts], Text)
    },
    code_content(Cs).
code_content([code_use(L, R, N)|Cs]) -->
    [xref_label(L), xref_ref(R), use(N)], !,
    code_content(Cs).
code_content([]) --> [end].
@

<<Build the document structure>>=
text_token("\n") --> [nl].
text_token(T) --> [text(T)].
    
code_text([T|Ts]) -->
    text_token(T), !,
    code_text(Ts).
code_text([]) --> [].

docs_text([T|Ts]) -->
    [text(T)], !,
    docs_text(Ts).
docs_text(["\n"|Ts]) -->
    [nl], !,
    eol(Ts).
docs_text([]) --> [].

eol([T|Ts]) -->
    [text(T)], !,
    docs_text(Ts).
eol([]) --> [].
@

<<Build the document structure>>=
xref_chunks(xref_endchunks, []) --> [].
xref_chunks(xref_chunkbegin(L, N), [chunk(L, N, Us, Ds)|Cs]) -->
    [X],
    xref_chunk(X, Us, Ds),
    [Y],
    xref_chunks(Y, Cs).

xref_chunk(xref_chunkend, [], []) --> [].
xref_chunk(xref_chunkuse(U), [chunkuse(U)|Us], Ds) -->
    [X],
    xref_chunk(X, Us, Ds).
xref_chunk(xref_chunkdefn(D), Us, [chunkdefn(D)|Ds]) -->
    [X],
    xref_chunk(X, Us, Ds).

index_list(index_endindex, []) --> [].
% we don't have an index for now
@

## Layout
The system tries to separate content and layout as much as possible.
It also aims to provide a sensible default layout for the human-readable documentation.

### HTML
For HTML documentation, [[<<lire.css>>]] is used.
<<lire.css>>=
<<Text width and margin>>
<<Headers in sans-serif>>
<<Quoted code in monospace>>
<<Code chunks>>
<<Use names formatting>>
<<List of chunks formatting>>
@

The text width is limited and a left margin is inserted to improve readability.
<<Text width and margin>>=
p {
    max-width: 14cm;
}
ul {
    max-width: 12cm;
}
body {
    padding-left: 2cm;
}
@

<<Headers in sans-serif>>=
h1,
h2,
h3 {
    font-family: sans-serif;
}
@

<<Use names formatting>>=
span.chunkdefs a,
span.chunkuses a,
span.chunkprev a,
span.chunknext a,
span.defn a,
span.quoteduse a,
span.embeddeduse a {
    text-decoration-line: None;
}
span.quoteduse {
    font-family: Serif;
}
span.embeddeduse {
    font-family: Serif;
}
span.chunknr {
    font-weight: bold;
}
@

Use names in quoted code and code chunks are typeset in different colors and are high-lighted when the mouse is over them.
<<>>=
span.quoteduse a {
    color: DarkGreen;
}
span.quoteduse a:hover {
    color: LimeGreen;
}
span.embeddeduse a {
    color: DarkBlue;
    font-style: Italic;
}
span.embeddeduse a:hover {
    color: DodgerBlue;
}
span.chunkdefs a,
span.chunkprev a,
span.chunknext a,
span.defn a,
span.chunkuses a {
    color: DarkRed;
}
span.chunkdefs a:hover,
span.chunkprev a:hover,
span.chunknext a:hover,
span.defn a:hover,
span.chunkuses a:hover {
    color: DarkGoldenRod;
}
@

<<Quoted code in monospace>>=
span.quote {
    font-family: Monospace;
}
@

<<Code chunks>>=
div.codechunk {
    /*
    max-width: 16cm;
    background-color: HoneyDew;
    */
}
@

<<List of chunks formatting>>=
ul.chunkslist {
    list-style-type: square;
}
@

# Common definitions
All building block for `noweb` (front ends, filters, back ends) are usually installed in the same [[<<`Noweb` location>>]]:
<<`Noweb` location>>=
/usr/lib/noweb
@

The portable [[<<Bash shebang>>]] is, according to [Stack Overflow](http://stackoverflow.com/questions/10376206/preferred-bash-shebang):
<<Bash shebang>>=
#! /usr/bin/env bash
@

# References

