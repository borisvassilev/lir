---
title: 'Reproducible computing with `lire`'
author: Boris Vassilev
version: 0.3
rights: (c) 2015 Boris Vassilev, University of Helsinki
...

# Introduction
This is a collection of tools that support programming language-agnostic reproducible computing.

This document is self-hosting: the implementation presented here can be used to produce the final human-readable documentation.
This makes informal validation trivial ("does the final document look right?"), but it also means that the only guarantee is that it implements enough to be able to compile itself.

It might or might not have [[quoted code with <<uses>>, or maybe without any]].
I haven't decided on it yet.

# Bootstrapping
The following small programs are the components used to provide the functionality of [[<<lire>>]].

The first one, [[<<lire-tangle.sh>>]], is used to tangle a code chunk and, if necessary, deploy it to the working directory, `.lire`:
<<lire-tangle.sh>>=
<<Bash shebang>>
f=$(mktemp)
notangle -t8 -filter emptydefn -R"$1" lire.lir > "$f"
cmp --quiet "$f" .lire/"$1"
if [ $? -eq 0 ]; then
    rm "$f"
else
    mv --verbose "$f" .lire/"$1"
fi
@

This next program, [[<<lire-use.sh>>]], is meant for files that do not need to be tangled; for example, the `lire` source file itself, or a bibliography file maintained separately.
<<lire-use.sh>>=
cmp --quiet "$1" .lire/"$1"
if [ $? -ne 0 ]; then
    cp --verbose "$1" .lire/"$1"
fi
@

Finally, a script [[<<lire-make.sh>>]] that uses these building blocks to put the necessary files in the working directory (all scripts and the Makefile) and make everything:
<<lire-make.sh>>=
for tangled in lire-nwpipe.sh nwpipe-pandoc.pl driver.pl nwpipe.pl pandoc-html.sh lire.css Makefile; do
    bash lire-tangle.sh "$tangled"
done
for used in lire.lir lire.bib; do
    bash lire-use.sh "$used"
done
make -C .lire
@

Of course, the building blocks themselves need to be "tangled" first.
Here is the [[<<bootstrap>>]] script:
<<bootstrap>>=
<<Bash shebang>>
for file in lire-tangle.sh lire-use.sh lire-make.sh; do
    notangle -t8 -R"$file" lire.lir > "$file"
done
bash lire-make.sh
@

To tangle and run it, do:

<<lire>>=
@
~~~{.bash}
$ notangle -R"bootstrap" lire.lir > lire && chmod u+x lire && ./lire
~~~

Once `lire` is tangled, unless the [[<<bootstrap>>]] code chunk is changed, re-making everything (while taking care of any changes, and avoiding unnecessary work) should not take more than just running `lire` again.

# The Back End
In its first incarnation, it consist only of a [Pandoc](http://johnmacfarlane.net/pandoc/) back end for [Noweb](https://www.cs.tufts.edu/~nr/noweb/) [@Noweb2008].
The [[<<Back end>>]]  generates `noweb` pipeline representation from a `.lir` source file ([[lire-nwpipe.sh]]), converts it to a document in Pandoc's markdown with embedded markup ([[nwpipe-pandoc]]), and uses Pandoc to compile the final HTML file:
<<Back end>>=
bash lire-nwpipe.sh lire.lir \
    | ./nwpipe-pandoc \
    | bash pandoc-html.sh \
        --bibliography=lire.bib \
        --css=lire.css \
        --output=lire.html
@
It assumes that all documentation chunks in the source document are written in Pandoc's markdown.

In the first stage [[<<lire-nwpipe.sh>>]] takes `.lir` source file(s) and writes to standard output `noweb`'s pipeline representation.
At the moment, this uses `noweb`'s own back end, `markup`, followed by the `emptydefn` and `noidx` filters.
All empty `@text` lines are removed from the output of the `noweb` components pipe; this should not change the structure of the pipeline or the final result, and it will make it unnecessary to look out for empty `@text` tokens in other parts of the program:
<<lire-nwpipe.sh>>=
<<`Noweb` location>>/markup "$@" \
    | <<`Noweb` location>>/emptydefn \
    | <<`Noweb` location>>/noidx \
    | sed -n '/^@text $/!p'
@

The program [[<<nwpipe-pandoc.pl>>]] is written in Prolog, using the SWI-Prolog implementation [@Wielemaker2012].
It is written as a stand-alone command line program: once compiled, can be run from the command line as a part of a pipe.
<<nwpipe-pandoc.pl>>=
% no messages of type `informational` or `banner`
:- set_prolog_flag(verbose, silent).

:- use_module(driver, [nwpipeline_pandoc/1]).
main :-
    current_prolog_flag(argv, Argv), % command line arguments
    prompt(_Old, ''), %  turn off prompt
    nwpipeline_pandoc(Argv),
    deterministic(D),
    (   D == false
    ->  format(user_error, "DANGLING CHOICE POINT(S)!~n", [])
    ;   true
    ),
    halt. % exit with success
main :-
    format(user_error, "FAILURE~n", []),
    halt(1). % otherwise, exit with failure (1)
@

The final documentation is produced by Pandoc.
The script [[<<pandoc-html.sh>>]] calls Pandoc for you to generate an HTML.
It expects to read Pandoc's markdown, with optional raw HTML embedded in it, from standard input.
It adds any additional arguments you provide it with:
<<pandoc-html.sh>>=
pandoc \
    --table-of-contents \
    --standalone --smart --self-contained \
    --to=html5 \
    "$@"
@

We use the GNU `make` utility with [[<<Makefile>>]] to generate the human readable documentation and generate any files that are required.
<<Makefile>>=
lire.html: lire.lir lire.css lire.bib lire-nwpipe.sh nwpipe-pandoc pandoc-html.sh
	<<Back end>>
	cp lire.html ..

nwpipe-pandoc: nwpipe-pandoc.pl driver.pl nwpipe.pl
	swipl --goal=main -o nwpipe-pandoc -c nwpipe-pandoc.pl
@

## From `lire` source to documentation
Add to the database a table with all code chunks.

The driver:
<<driver.pl>>=
:- module(driver, [nwpipeline_pandoc/1]).

:- use_module(nwpipe, [nwtokens_terms/2]).

nwpipeline_pandoc(_) :-
    user_input = I,
    input_to_terms(I, Ts),
    open('terms.txt', write, Terms_file),
    forall(member(T, Ts), format(Terms_file, "~q~n", [T])),
    close(Terms_file).

input_to_terms(I, Ts) :-
    read_string(I, _, S),
    split_string(S, "\r\n", "\r\n", Str_Ls),
    maplist(string_codes, Str_Ls, Ls),
    nwtokens_terms(Ls, Ts0),
    phrase(lire(Ts), Ts0).

lire(L) -->
    [file(_)],
    lire_rest(L).

lire_rest(L) --> [X], !, % cut
    lire_file(X, L).
lire_rest([]) --> [].

lire_file(docs, L) --> [X],
    docs(X, L).
lire_file(code, [code(C)|L]) --> [X],
    code(X, C),
    lire_rest(L).
lire_file(nl, [nl(N)|L]) -->
    nls(N),
    lire_rest(L).
lire_file(xref_beginchunks, [chunks_list(Cs)|L]) --> [X],
    xref_chunks(X, Cs),
    lire_rest(L).
lire_file(index_beginindex, [index(I)|L]) --> [X],
    index(X, I),
    lire_rest(L).

docs(end, L) -->
    lire_rest(L).
docs(text(T), [text(Text)|L]) -->
    more_text(Ts),
    {   atomics_to_string([T|Ts], Text)
    },
    [X],
    docs(X, L).
docs(nl, [nl(N)|L]) -->
    nls(N),
    [X],
    docs(X, L).
docs(quote, [quote(Q)|L]) -->
    [X],
    quote(X, Q),
    [Y],
    docs(Y, L).

quote(text(T), [text(T)|Q]) --> [X],
    quote(X, Q).
quote(xref_ref(L), [Ref|Q]) -->
    [use(N)],
    {   ref(L, N, Ref)
    },
    [X],
    quote(X, Q).
quote(endquote, []) --> [].

ref(L, N, Ref) :-
    (   L \== 'nw@notdef'
    ->  Ref = use_ref(N, L)
    ;   Ref = use(N)
    ).

code(end, []) --> !, [].
code(X, [X|Cs]) --> [Y], !,
    code(Y, Cs).

more_text([T|Ts]) -->
    [text(T)], !, % cut
    more_text(Ts).
more_text(["\n"|Ts]) -->
    [nl], !, % cut
    eol(Ts).
more_text([]) --> [].

eol([T|Ts]) -->
    [text(T)], !, % cut
    more_text(Ts).
eol([]) --> [].

nls(N) -->
    [nl], !, % cut
    nls(N0),
    {   succ(N0, N)
    }.
nls(1) --> [].

xref_chunks(xref_endchunks, []) --> [].
xref_chunks(xref_chunkbegin(L, N), [chunk(L, N, Us, Ds)|Cs]) --> [X],
    xref_chunk(X, Us, Ds),
    [Y],
    xref_chunks(Y, Cs).

xref_chunk(xref_chunkend, [], []) --> [].
xref_chunk(xref_chunkuse(U), [U|Us], Ds) --> [X],
    xref_chunk(X, Us, Ds).
xref_chunk(xref_chunkdefn(D), Us, [D|Ds]) --> [X],
    xref_chunk(X, Us, Ds).

index(index_endindex, []) --> [].
% we don't have an index for now
/*
index(X, [X|Is]) --> [Y],
    xref_chunks(Y, Is).
*/
@

Parsing the pipeline:
<<nwpipe.pl>>=
:- module(nwpipe, [nwtokens_terms/2]).

nwtokens_terms([], []).
nwtokens_terms([[0'@|Token]|Tokens], [Term|Terms]) :-
    phrase(token(Back, Term), Token, Back),
    format(user_error, "~q~n", [Term]),
    nwtokens_terms(Tokens, Terms).
<<`Noweb` tokens $\rightarrow$ Prolog terms>>
@

<<`Noweb` tokens $\rightarrow$ Prolog terms>>=
:- use_module(library(dcg/basics), [nonblanks//1, integer//1]).
token(Back, Term) -->
    nonblanks(NB),
    {   atom_codes(Key, NB),
        keyword(Key, Items, Back, Term)
    },
    items(Items).
<<Structural keywords>>
<<Tagging keywords>>

<<Individual items>>
@

<<Structural keywords>>=
keyword(begin, [space, chunk_kind(K)]/*, space, chunk_number(N)]*/, _, K).
keyword(end, []/*[space, chunk_kind(_K), space, chunk_number(N)]*/, _, end).
keyword(text, [space, string_rest(S, Rest)], Rest, text(S)).
keyword(nl, [], [], nl).
keyword(defn, [space, string_rest(S, Rest)], Rest, defn(S)).
keyword(use, [space, string_rest(S, Rest)], Rest, use(S)).
keyword(quote, [], [], quote).
keyword(endquote, [], [], endquote).
@

For the [[<<Tagging keywords>>]], most are represented in `noweb`'s pipeline by a single keyword, `xref`.
There are several categories of cross-referencing concepts.
<<Tagging keywords>>=
keyword(file, [space, atom_rest(File, Rest)], Rest, file(File)).
keyword(xref, [space, xref(XRef, Rest)], Rest, XRef).
keyword(index, [space, index(Index, Rest)], Rest, Index).

<<Basic cross-referencing>>
<<Linking previous and next definitions of a code chunk>>
<<Continued definitions of the current chunk>>
<<The list of all code chunks>>
<<Chunks where the code is used>>

<<The index of identifiers>>
@

<<Basic cross-referencing>>=
xref(label, [space, atom_rest(L, Rest)], Rest, xref_label(L)).
xref(ref, [space, atom_rest(L, Rest)], Rest, xref_ref(L)).
@

<<Linking previous and next definitions of a code chunk>>=
xref(prevdef, [space, atom_rest(L, Rest)], Rest, xref_prevdef(L)).
xref(nextdef, [space, atom_rest(L, Rest)], Rest, xref_nextdef(L)).
@

<<Continued definitions of the current chunk>>=
xref(begindefs, [], [], xref_begindefs).
xref(defitem, [space, atom_rest(L, Rest)], Rest, xref_defitem(L)).
xref(enddefs, [], [], xref_enddefs).
@

<<Chunks where the code is used>>=
xref(beginuses, [], [], xref_beginuses).
xref(useitem, [space, atom_rest(L, Rest)], Rest, xref_useitem(L)).
xref(enduses, [], [], xref_enduses).
xref(notused, [space, string_rest(Name, Rest)], Rest, xref_notused(Name)).
@

<<The list of all code chunks>>=
xref(beginchunks, [], [], xref_beginchunks).
xref(chunkbegin,
    [space, atom(L), space, string_rest(Name, Rest)],
    Rest,
    xref_chunkbegin(L, Name)).
xref(chunkuse, [space, atom_rest(L, Rest)], Rest, xref_chunkuse(L)).
xref(chunkdefn, [space, atom_rest(L, Rest)], Rest, xref_chunkdefn(L)).
xref(chunkend, [], [], xref_chunkend).
xref(endchunks, [], [], xref_endchunks).
@

<<The index of identifiers>>=
index(beginindex, [], [], index_beginindex).
index(endindex, [], [], index_endindex).
@

These are the rules used to transform [[<<Individual items>>]] that appear in the `noweb` tokens to Prolog terms.
<<Individual items>>=
items([]) --> [].
items([I|Is]) -->
    item(I),
    items(Is).

<<Atoms and text>>
<<"Space">>
<<Chunk number>>
<<Chunk kind>>
<<Cross-reference>>
<<Helper DCGs>>
@

Identifiers are converted to atoms, while "text" (text and names) are converted to strings:
<<Atoms and text>>=
item(atom(A)) -->
    nonblanks(Codes),
    {   atom_codes(A, Codes)
    }.

item(atom_rest(A, Rest)) -->
    {   when(ground(Rest), atom_codes(A, Rest))
    }.

item(string_rest(Str, Rest)) -->
    {   when(ground(Rest), string_codes(Str, Rest))
    }.
@

We assume that [[<<"Space">>]] is represented by a single "space" character; the "Hacker's Guide" is not explicit about this, but so far it has always worked.
<<"Space">>=
item(space) --> [0'\s]. % could it be another "white"?...
@

Integers are used to enumerate the code and documentation [[<<Chunk number>>]]:
<<Chunk number>>=
item(chunk_number(N)) --> integer(N).
@

Code and documentation chunks are delimited by the same keywords; the [[<<Chunk kind>>]] is encoded in a secondary keyword:
<<Chunk kind>>=
item(chunk_kind(CK)) -->
    nonblanks(Codes),
    {   atom_codes(CK, Codes)
    }.
@

Finally, [[<<Cross-reference>>]].
Note that it employs quite a few secondary keywords collected in their own look up table.
<<Cross-reference>>=
item(xref(XRef, Rest)) -->
    nonblanks(Codes),
    {   atom_codes(X, Codes),
        xref(X, Items, Rest, XRef)
    },
    items(Items).
item(index(Index, Rest)) -->
    nonblanks(Codes),
    {   atom_codes(X, Codes),
        index(X, Items, Rest, Index)
    },
    items(Items).
@

## Layout
The system tries to separate content and layout as much as possible.
It also aims to provide a sensible default layout for the human-readable documentation.

### HTML
For HTML documentation, [[<<lire.css>>]] is used. Here it is:
<<lire.css>>=
span.nwuse a, span.uses a, span.nwquoteduse a, span.prevdef a, span.nextdef a {
    color: forestgreen;
    text-decoration-line: none;
}
h1, h2, h3 {
    font-family: sans-serif;
}
p {
    max-width: 14cm;
}
ul {
    max-width: 12cm;
}
body {
    padding-left: 2cm;
}
span.quotedcode {
    font-family: monospace;
}
span.nwquoteduse {
    font-family: serif;
}
span.nwuse {
    font-family: serif;
    font-style: italic;
}
span.nwdefn {
    font-family: serif;
    font-weight: bold;
}
span.uses, span.prevdef, span.nextdef {
    font-size: small;
}
@

# Common definitions
All building block for `noweb` (front ends, filters, back ends) are usually installed in the same [[<<`Noweb` location>>]]:
<<`Noweb` location>>=
/usr/lib/noweb
@

The portable [[<<Bash shebang>>]] is, according to [Stack Overflow](http://stackoverflow.com/questions/10376206/preferred-bash-shebang):
<<Bash shebang>>=
#! /usr/bin/env bash
@

# References

