---
title: 'Reproducible computing with `lire`'
author: Boris Vassilev
version: 0.2
rights: (c) 2015 Boris Vassilev, University of Helsinki
...

# Introduction
This is a collection of tools that support programming language-agnostic reproducible computing.

This document is self-hosting: the implementation presented here can be used to produce the final human-readable documentation.
This means that at any point it only implements enough to be able to compile itself.

The following bootstrapping script can be extracted and run as follows:

~~~
$ notangle -R"bootstrap" lire.lir > bootstrap.sh
$ chmod u+x bootrstrap.sh
$ ./bootstrap.sh
~~~

Running this will extract and install the `lire` executable and tools.
From there on, `lire` can be used to generate the documentation.

<<bootstrap>>=
<<Bash shebang>>

notangle -R"lir-pandoc" -filter emptydefn lire.lir > lir-pandoc
chmod u+x lir-pandoc
echo "tangled lir-pandoc"

notangle -R"lire.css" lire.lir > lire.css
echo "tangled lire.css"

notangle -R"nwpipe-pandoc.pl" -filter emptydefn lire.lir > nwpipe-pandoc.pl
echo "tangled nwpipe-pandoc.pl"
swipl --goal=main -o nwpipe-pandoc -c nwpipe-pandoc.pl
echo "compiled nwpipe-pandoc.pl"

./lir-pandoc lire.lir > lire.pandoc
echo "converted to pandoc"

pandoc \
	--table-of-contents --standalone --smart \
	--bibliography=lire.bib \
	--to=html5 \
	--css=lire.css \
	--output=lire.html \
	lire.pandoc
echo "used pandoc to compile to html"
@

# The Back End
In its first version, it consist only of a [Pandoc](http://johnmacfarlane.net/pandoc/) back end for [Noweb](https://www.cs.tufts.edu/~nr/noweb/) [@Noweb2008].
The back end reads from standard input the pipeline representation emitted by Noweb's `markup` and writes to standard output a document in Pandoc's markdown.
It assumes that all documentation chunks in the source document are written in Pandoc's markdown.
The code chunks in the source document are represented as fenced code blocks preceded by a level-three Atx-style header with the name of the code chunk and followed by a single horizontal rule.

This script uses `markup` provided by `noweb`, and passes the output to the back end that transform `noweb`'s pipeline to Pandoc's markdown.
<<lir-pandoc>>=
<<Bash shebang>>
<<`Noweb` location>>/markup "$@" \
	| <<`Noweb` location>>/emptydefn \
	| ./nwpipe-pandoc
@

Logically, the back end can be broken down to components, each taking care of one of the following:

- Reading each _token_ in `noweb`'s pipeline representation from standard input and parsing it, to obtain an equivalent Prolog term -- [[<<Parse `noweb` tokens>>]];
- A finite state machine that consumes the Prolog terms and emits Pandoc markdown -- [[<<Pipeline FSM>>]] (it is rather a finite state transducer, as it generates output, but let's not get too formal);
- A writer that transforms each term to the actual emitted markdown -- [[<<Emit Pandoc's markdown>>]].

The program defines a [[main/0]] so that it can be compiled and run as a command line tool.
<<nwpipe-pandoc.pl>>=
% no messages of type `informational` or `banner`
:- set_prolog_flag(verbose, silent).

main :-
	current_prolog_flag(argv, Argv), % command line arguments
	prompt(_Old, ''), %  turn off prompt
	nwpipeline_to_pandoc(Argv),
	halt. % exit with success
main :-
	halt(1). % otherwise, exit with failure (1)

nwpipeline_to_pandoc(_) :- % command line arguments ignored TODO
	nwpipeline_fsm.

<<Parse `noweb` tokens>>
<<Pipeline FSM>>
<<Emit Pandoc's markdown>>
@

## Parsing `noweb` tokens
The first component reads the line-oriented pipeline representation of `noweb` from standard input and turns each line (token) into a Prolog term.
<<Parse `noweb` tokens>>=
<<Read lines from standard input>>
<<`Noweb` tokens $\rightarrow$ Prolog terms>>
@

To read lines from standard input we define [[next_input/1]] that unifies its argument with the Prolog term representing the next `noweb` token, or `end_of_file` at the end of input.
<<Read lines from standard input>>=
:- use_module(library(readutil), [read_line_to_codes/2]).
next_input(T) :-
	read_line_to_codes(user_input, Codes),
	nwtoken_to_plterm(Codes, T).

nwtoken_to_plterm(end_of_file, end_of_file).
nwtoken_to_plterm([0'@|Token], T) :-
	phrase(token_term(T), Token).
@

The structure of the Prolog terms mostly mirrors the structure of the tokens as described in "The `noweb` Hacker's Guide" [@Ramsey1992].
In the table [[nwtoken/3]], the first argument is `noweb`'s keyword, the second argument represents the items in this token, and the third argument is the resulting Prolog term.
<<`Noweb` tokens $\rightarrow$ Prolog terms>>=
% Tagging keywords
nwtoken(file, [space, atom_rest(File)], file(File)).
% Structural keywords
nwtoken(begin, [space, chunk_kind(Kind), space, integer(N)], begin(Kind, N)).
nwtoken(end, [space, chunk_kind(Kind), space, integer(N)], end(Kind, N)).
nwtoken(text, [space, string_rest(Text)], text(Text)).
nwtoken(nl, [], nl).
nwtoken(defn, [space, string_rest(Name)], defn(Name)).
nwtoken(use, [space, string_rest(Name)], use(Name)).
nwtoken(quote, [], quote).
nwtoken(endquote, [], endquote).

<<Interpreter>>
@

This approach of using a table containing a miniature program in the second argument and implementing an interpreter for it is directly borrowed from the chapter on definite clause grammer in "The Craft of Prolog" [@OKeefe1990, pp. 299-300].
Here is the interpreter:
<<Interpreter>>=
token_term(T) -->
	nonblanks(NB),
	{	atom_codes(Keyword, NB),
		nwtoken(Keyword, Items, T)
	},
	nwtokenitems(Items).

nwtokenitems([]) --> [].
nwtokenitems([I|Is]) -->
	nwtokenitem(I),
	nwtokenitems(Is).

<<Transform individual items>>
@

Here we define the transformation for each item appearing in [[nwtoken/3]]:
<<Transform individual items>>=
:- use_module(library(dcg/basics), [nonblanks//1, integer//1]).
nwtokenitem(atom_rest(A)) -->
	rest(Codes),
	{	atom_codes(A, Codes)
	}.
nwtokenitem(space) -->
	[0'\s]. % could it be another "white"?...
nwtokenitem(integer(N)) -->
	integer(N).
nwtokenitem(string_rest(Str)) -->
	rest(Codes),
	{	string_codes(Str, Codes)
	}.
nwtokenitem(chunk_kind(CK)) -->
	nonblanks(Codes),
	{	atom_codes(CK, Codes),
		nwchunk_kind(CK)
	}.

rest([]) --> [].
rest([C|Cs]) -->
	[C],
	rest(Cs).

nwchunk_kind(code).
nwchunk_kind(docs).
@

## Pipeline Finite State Machine
To represent a finite state machine in Prolog we use predicates for states and predicate clauses for arcs (transitions), another idea borrowed from "The Craft of Prolog" [@OKeefe1990, pp. 353].
The finite state machine is initialized by [[nwpipeline_fsm/0]].
The input to the state machine are the terms representing the `noweb` tokens, obtained by consecutive calls to [[next_input/1]].
<<Pipeline FSM>>=
nwpipeline_fsm :-
	next_input(T),
	start(T).
% States
<<Start>>
<<File>>
<<Begin chunk>>
<<Docs chunk>>
<<Quoted code>>
<<Code chunk>>
% etc
@

The pipeline representation consists of one or more files:
<<Start>>=
start(file(_File)) :-
	next_input(T),
	file(T).
@

<<File>>=
file(begin(Kind, N)) :-
	begin_chunk(Kind, N).
file(file(_File)) :-
	next_input(T),
	file(T).
file(end_of_file).
@

[[<<Begin chunk>>]] is not a real state, since the transition happens without any input:
<<Begin chunk>>=
begin_chunk(docs, N) :-
	next_input(T),
	docs(T, N).
begin_chunk(code, N) :-
	code_begin(N, Code),
	next_input(T),
	code(T, Code).
@

Since the documentation chunks are expected to be valid Pandoc markdown, the contents are simply emitted unchanged to the output:
<<Docs chunk>>=
docs(end(docs, N), N) :-
	next_input(T),
	file(T).
docs(nl, N) :-
	nl,
	next_input(T),
	docs(T, N).
docs(text(Text), N) :-
	format("~s", [Text]),
	next_input(T),
	docs(T, N).
docs(quote, N) :-
	next_input(T),
	quote(T, N, Content, Content).
@

For both quoted (inline) code and code chunks, the contents are first collected before emitted at the end of the quote or code chunk.
Note the two additional arguments for these two states: they represent the difference-list pair that is used to collect the contents to a list.
<<Quoted code>>=
quote(endquote, N, Content, []) :-
	emit(quote(Content)),
	next_input(T),
	docs(T, N).
quote(text(Text), N, Content, [text(Text)|Rest]) :-
	next_input(T),
	quote(T, N, Content, Rest).
quote(nl, N, Content, [nl|Rest]) :-
	next_input(T),
	quote(T, N, Content, Rest).
quote(use(Name), N, Content, [use(Name)|Rest]) :-
	next_input(T),
	quote(T, N, Content, Rest).
@

The state for code chunks is largely the same, with the addition of a transition for the `defn` token.
<<Code chunk>>=
code(end(code, N), Code) :-
	code_end(N, Code),
	emit(Code),
	next_input(T),
	file(T).
code(defn(Name), Code0) :-
	code_defn(Name, Code0, Code),
	next_input(nl), % must be here! (says the "Hacker's Guide")
	next_input(T),
	code(T, Code).
code(use(Name), Code0) :-
	code_content(use(Name), Code0, Code),
	next_input(T),
	code(T, Code).
code(nl, Code0) :-
	code_content(nl, Code0, Code),
	next_input(T),
	code(T, Code).
code(text(Text), Code0) :-
	code_content(text(Text), Code0, Code),
	next_input(T),
	code(T, Code).

code_begin(N,
	codechunk(N, _, Content, Content)).
code_defn(Name,
	codechunk(N, _, Content, Content),
	codechunk(N, Name, Content, Content)).
code_content(New,
	codechunk(N, Name, Content, [New|Rest]),
	codechunk(N, Name, Content, Rest)).
code_end(N,
	codechunk(N, _, _, [])).
@

## Writing Pandoc's Markdown
Unfortunately, Pandoc's markdown is not expressive enough to allow the code of the literate program to be expressed in pure markdown.
Instead, native markup for each supported front end is used.
The writer is implemented as a single predicate, [[emit/1]], with one clause for each possible output, as produced by the finite state machine.
There is an ugly help predicate, [[through_pandoc/3]], which uses Pandoc to generate correct markup for the layout of elements that are unusual for normal documents, but necessary for a literate program.
At the moment, only a Pandoc document that can be compiled to an HTML page is generated.
<<Emit Pandoc's markdown>>=
<<The ugliness called `through_pandoc/3`>>
<<Emit HTML for Pandoc>>
@

### Emitting HTML
A Prolog library is used for emitting HTML.
Code chunks are put in a `<div>` tag with a class `codechunk`, with `noweb`'s `defn` at the top, inside its own `<span>` tag with a class `nwdefn`, and all content in a `<pre>` tag.
Quoted code is put in a `<span>` with a class `quotedcode` and typeset in monospace.
<<Emit HTML for Pandoc>>=
:- use_module(library(http/html_write)).

emit(codechunk(_, Name, C, _)) :-
	Defn = span(class=nwdefn, [
			&(lang), &(nbsp),
			Name,
			&(nbsp), &(rang), &(equiv)]),
	maplist(codechunk_html, C, Content),
	phrase(html(div(class=codechunk, [Defn, pre(Content)])), HTML),
	print_html(HTML).
emit(quote(Q)) :-
	maplist(quotedcode_html, Q, Content),
	phrase(html(span(class=quotedcode, Content)),
		HTML),
	print_html(HTML).

<<`Noweb` $\rightarrow$ HTML>>
@

Transforming the Prolog terms representing code to HTML could be quite straight-forward: just map the `noweb` token to an HTML tag with the appropriate attributes.
For quoted code, this is good enough:
<<`Noweb` $\rightarrow$ HTML>>=
quotedcode_html(nl, '\n').
quotedcode_html(text(Text), Text).
quotedcode_html(use(Name),
		span(class=nwuse, [
			&(lang), &(nbsp),
			Name,
			&(nbsp), &(rang)])).
@
Apparently, Pandoc does recognize markdown inside `<span>` contents, and adds the necessary HTML markup.

For example, we can put a "use" inside quoted code, complete with its own Pandoc markdown, as in:
<<foo>>=
[[quoted @<<foo $\rightarrow \pi =$ `bar`@>>]]
@ Which will turn into: [[quoted <<foo $\rightarrow \pi =$ `bar`>>]]

Unfortunately, it does not do that for the contents of `<span>` which is inside a `<pre>` (I guess rightfully so?).
<<`Noweb` $\rightarrow$ HTML>>=
codechunk_html(nl, '\n').
codechunk_html(text(Text), Text).
codechunk_html(use(Name),
		span(class=nwuse, [
			&(lang), &(nbsp),
			\[F], % add directly to the output
			&(nbsp), &(rang)])) :-
	through_pandoc(html5, Name, F).
@

### Ad-hoc markup with Pandoc
To make the contents of code chunks look like code, they are put inside a `<pre>` tag.
But in a `lire` source file, a code chunk can contain a [[<<Use>>]], which will have to be typeset properly.
Pandoc will not do this automatically for a `<span>` inside raw HTML `<pre>`.
The hacky solution here is to pass the Name in a [[<<Use>>]] (formatted in markdown) to Pandoc, and embed the resulting markup in the output.
Here is how it is done:

#. Turn single quotes -- "`'`" -- into "`'\''`" sequences;
#. Construct the pipe;
#. Call it as a `bash` process;
#. Parse the output, removing the enclosing `<p>`;
#. Convert back to HTML
<<The ugliness called `through_pandoc/3`>>=
:- use_module(library(sgml), [load_structure/3]).
:- use_module(library(sgml_write), [xml_write/3]).

through_pandoc(To, Str, FStr) :-
	atomics_to_string(L, "'", Str), % split
	atomics_to_string(L, "'\\''", EscStr), % put together
	atomics_to_string(["echo '", EscStr, "' | pandoc -t ", To], Pandoc),
	process_create(path(bash), ['-c', Pandoc], [stdout(pipe(Out))]),
	through_pandoc_1(To, Out, FStr),
	close(Out).

through_pandoc_1(html5, Out, FStr) :-
	load_structure(Out, [element(p, [], StrDOM)], [space(preserve)]),
	with_output_to(string(FStr),
		xml_write(current_output, StrDOM, [header(false), layout(false)])).
@

# Common definitions
All building block for `noweb` (front ends, filters, back ends) are usually installed in the same place:
<<`Noweb` location>>=
/usr/lib/noweb
@

The portable Bash shebang is, according to [Stack Overflow](http://stackoverflow.com/questions/10376206/preferred-bash-shebang):
<<Bash shebang>>=
#! /usr/bin/env bash
set -e
@
We also set the script to stop immediately at errors.

# Layout
The system tries to separate content and layout as much as possible.
It also aims to provide a sensible default layout for the human-readable documentation.

## HTML
For HTML documentation, a CSS file is used. Here it is:
<<lire.css>>=
h1, h2, h3 {
	font-family: sans-serif;
}
p {
	max-width: 17cm;
}
ul {
	max-width: 15cm;
}
body {
	padding-left: 3cm;
}
span.quotedcode {
	font-family: monospace;
}
span.nwuse {
	font-family: serif;
	font-style: italic;
}
span.nwdefn {
	font-family: serif;
	font-weight: bold;
}
@

# References

