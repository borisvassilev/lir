---
title: 'Reproducible computing with `lire`'
author: Boris Vassilev
version: 0.3
rights: (c) 2015 Boris Vassilev, University of Helsinki
...

# Introduction
This is a collection of tools that support programming language-agnostic reproducible computing.

This document is self-hosting: the implementation presented here can be used to produce the final human-readable documentation.
This makes informal validation trivial ("does the final document look right?"), but it also means that the only guarantee is that it implements enough to be able to compile itself.

# Bootstrapping
The following small programs are the components used to provide the functionality of `lire`.

The first one is used to tangle a code chunk and, if necessary, deploy it to the working directory, `.lire`:
<<lire-tangle.sh>>=
f=$(mktemp)
notangle -t8 -filter emptydefn -R"$1" *.lir > "$f"
cmp --quiet "$f" .lire/"$1"
if [ $? -eq 0 ]; then
	rm "$f"
else
	mv --verbose "$f" .lire/"$1"
fi
@

This next program is meant for files that do not need to be tangled; for example, the `lire` source file itself, or a bibliography file maintained separately.
<<lire-use.sh>>=
cmp --quiet "$1" .lire/"$1"
if [ $? -ne 0 ]; then
	cp --verbose "$1" .lire/"$1"
fi
@

Finally, a script that uses these building blocks to put the necessary files in the working directory (all scripts and the Makefile) and make everything:
<<lire-make.sh>>=
for tangled in lire-nwpipe.sh nwpipe-pandoc.pl pandoc-html.sh lire.css Makefile; do
	bash lire-tangle.sh "$tangled"
done
for used in lire.lir lire.bib; do
	bash lire-use.sh "$used"
done
make -C .lire
@

Of course, the building blocks themselves need to be "tangled" first.
Here is the bootstrapping script:
<<bootstrap>>=
<<Bash shebang>>
for file in lire-tangle.sh lire-use.sh lire-make.sh; do
	notangle -t8 -R"$file" *.lir > "$file"
done
bash lire-make.sh
@

To tangle and run it, do:

~~~{.bash}
$ notangle -R"bootstrap" lire.lir > lire && chmod u+x lire && ./lire
~~~

Once `lire` is tangled, unless the [[<<bootstrap>>]] code chunk is changed, re-making everything (while taking care of any changes, and avoiding unnecessary work) should not take more than just running `lire` again.

# The Back End
In its first incarnation, it consist only of a [Pandoc](http://johnmacfarlane.net/pandoc/) back end for [Noweb](https://www.cs.tufts.edu/~nr/noweb/) [@Noweb2008].
The back end reads from standard input the pipeline representation emitted by Noweb's `markup` and writes a document in Pandoc's markdown.
It assumes that all documentation chunks in the source document are written in Pandoc's markdown.

The back end first takes `lire` file(s) and writes to standard output `noweb`'s pipeline representation.
At the moment, this uses `noweb`'s own back end, `markup`, followed by the `emptydefn` and `noidx` filters:
<<lire-nwpipe.sh>>=
<<`Noweb` location>>/markup "$@" \
	| <<`Noweb` location>>/emptydefn \
	| <<`Noweb` location>>/noidx
@

The `noweb` pipeline representation of a `lire` source file is then converted to Pandoc markdown, with embedded raw markup for the intended final documentation source.
At the moment, we only create a human readable file in HTML.

The program is written in Prolog, using the SWI-Prolog implementation [@Wielemaker2012].
It is written as a stand-alone command line program: once compiled, can be run from the command line as a part of a pipe.
<<nwpipe-pandoc.pl>>=
% no messages of type `informational` or `banner`
:- set_prolog_flag(verbose, silent).

main :-
	current_prolog_flag(argv, Argv), % command line arguments
	prompt(_Old, ''), %  turn off prompt
	nwpipeline_to_pandoc(Argv),
	halt. % exit with success
main :-
	halt(1). % otherwise, exit with failure (1)

nwpipeline_to_pandoc(_) :- % command line arguments ignored TODO
	nwpipeline_fsm.

<<Parse `noweb` tokens>>
<<Pipeline FSM>>
<<Emit Pandoc's markdown>>
@

The final documentation is produced by Pandoc.
This script calls Pandoc for you to generate an HTML.
It expects to read Pandoc's markdown, with optional raw HTML embedded in it, from standard input.
It adds any additional arguments you provide it with:
<<pandoc-html.sh>>=
pandoc \
	--table-of-contents \
	--standalone --smart --self-contained \
	--to=html5 \
	"$@"
@

We use the GNU `make` utility to generate the human readable documentation and generate any files that are required.
<<Makefile>>=
lire.html: lire.lir lire.css lire.bib lire-nwpipe.sh nwpipe-pandoc pandoc-html.sh
	bash lire-nwpipe.sh lire.lir \
		| ./nwpipe-pandoc \
		| bash pandoc-html.sh \
			--bibliography=lire.bib \
			--css=lire.css \
			--output=lire.html
	cp lire.html ..

nwpipe-pandoc: nwpipe-pandoc.pl
	swipl --goal=main -o nwpipe-pandoc -c nwpipe-pandoc.pl
@

Logically, converting `noweb`'s pipeline to valid Pandoc markdown be broken down to components, each taking care of one of the following:

- Reading each _token_ in `noweb`'s pipeline representation from standard input and parsing it, to obtain an equivalent Prolog term---[[<<Parse `noweb` tokens>>]];
- A finite state machine that consumes the Prolog terms and emits Pandoc markdown---[[<<Pipeline FSM>>]] (it is rather a finite state transducer, as it generates output, but let's not get too formal);
- A writer that transforms each term to the actual emitted markdown---[[<<Emit Pandoc's markdown>>]].

## Parsing `noweb` tokens
The first component reads the line-oriented pipeline representation of `noweb` from standard input and turns each line (token) into a Prolog term.
<<Parse `noweb` tokens>>=
<<Read lines from standard input>>
<<`Noweb` tokens $\rightarrow$ Prolog terms>>
@

To read lines from standard input we define [[next_input/1]] that unifies its argument with the Prolog term representing the next `noweb` token, or `end_of_file` at the end of input.
<<Read lines from standard input>>=
:- use_module(library(readutil), [read_line_to_codes/2]).

next_input(T) :-
	read_line_to_codes(user_input, Codes),
	nwtoken_to_plterm(Codes, T0),
	(	T0 = xref(T1), \+ basic_xref(T1)
	->	writeln(user_error, T0),
		next_input(T)
	;	T0 = T
	).
basic_xref(label(_)). basic_xref(ref(_)).

nwtoken_to_plterm(end_of_file, end_of_file).
nwtoken_to_plterm([0'@|Token], T) :-
	phrase(token(T), Token).
@

To convert the `noweb` token to a Prolog term we use an approach directly borrowed from the chapter on definite clause grammers in "The Craft of Prolog" [@OKeefe1990, pp. 299-300].

With this approach, in the three-column [[<<Table of keywords>>]], each `noweb` token keyword is associated with (a) a description of how to parse the rest of the token and (b) a Prolog term representing the result. The description in (a) is in effect a miniature program passed to the [[<<Interpreter>>]].
<<`Noweb` tokens $\rightarrow$ Prolog terms>>=
<<Table of keywords>>
<<Interpreter>>
@

In the table [[keyword/3]], the first column is `noweb`'s keyword, the second column represents the items in this token, and the third argument is the resulting Prolog term.
The structure of the Prolog terms mostly mirrors the structure of the tokens as described in "The `noweb` Hacker's Guide" [@Ramsey1992].
The only tagging keyword recognized at the moment is `file`:
<<Table of keywords>>=
keyword(file, [space, atom_rest(File)], file(File)).
@

The structural keywords represent the documentation and code chunks in the `noweb` source:
<<>>=
keyword(begin, [space, chunk_kind(Kind), space, chunk_number(N)], begin(Kind, N)).
keyword(end, [space, chunk_kind(Kind), space, chunk_number(N)], end(Kind, N)).
keyword(text, [space, string_rest(Text)], text(Text)).
keyword(nl, [], nl).
keyword(defn, [space, string_rest(Name)], defn(Name)).
keyword(use, [space, string_rest(Name)], use(Name)).
keyword(quote, [], quote).
keyword(endquote, [], endquote).
@

All cross-referencing is represented in `noweb`'s pipeline by a single keyword, `xref`.
It is then followed by a second keyword which is described in its own table.
<<>>=
keyword(xref, [space, xref(XRef)], xref(XRef)).
<<Table of cross-referencing keywords>>
@

There are several categories of cross-referencing concepts.
The basic cross-reference for labels and references to labels:
<<Table of cross-referencing keywords>>=
xref(label, [space, atom_rest(Label)], label(Label)).
xref(ref, [space, atom_rest(Label)], ref(Label)).
@

Linking previous and next definitions of a code chunk:
<<>>=
xref(prevdef, [space, atom_rest(Label)], prevdef(Label)).
xref(nextdef, [space, atom_rest(Label)], nextdef(Label)).
@

Continued definitions of the current chunk:
<<>>=
xref(begindefs, [], begindefs).
xref(defitem, [space, atom_rest(Label)], defitem(Label)).
xref(enddefs, [], enddefs).
@

Chunks where the code is used:
<<>>=
xref(beginuses, [], beginuses).
xref(useitem, [space, atom_rest(Label)], useitem(Label)).
xref(enduses, [], enduses).
xref(notused, [space, string_rest(Name)], notused(Name)).
@

And the list of all code chunks:
<<>>=
xref(beginchunks, [], beginchunks).
xref(chunkbegin,
	[space, atom(Label), space, string_rest(Name)],
	chunkbegin(Label, Name)).
xref(chunkuse, [space, atom_rest(Label)], chunkuse(Label)).
xref(chunkdefn, [space, atom_rest(Label)], chunkdefn(Label)).
xref(chunkend, [], chunkend).
xref(endchunks, [], endchunks).
@

The interpreter for the miniature programs in the second argument of the table of keywords is defined as a Prolog [**d**efinite **c**lause **g**rammar](http://en.wikipedia.org/wiki/Definite_clause_grammar).
A couple of the DCG primitives offered by the SWI-Prolog library [DCG Basics](http://www.swi-prolog.org/pldoc/doc/swi/library/dcg/basics.pl) are used.

The `noweb` tokens that are being parsed are simple enough to make it unnecessary to have separate tokenization and parse steps: both are done in one pass.
Each token is represented as a single line that starts with a keyword: a string of non-blank characters.
The keyword is converted to an atom; this atom is the "key" in the [[keyword/3]] table that associated the list of items with the final Prolog term.
<<Interpreter>>=
:- use_module(library(dcg/basics), [nonblanks//1, integer//1]).

token(Term) -->
	nonblanks(NB),
	{	atom_codes(Key, NB),
		keyword(Key, Items, Term)
	},
	items(Items).

items([]) --> [].
items([I|Is]) -->
	item(I),
	items(Is).

<<Transform individual items>>
<<Helper DCGs>>
@

These are the rules for parsing the individual items that appear in the `noweb` tokens to Prolog terms.
Identifiers are converted to atoms, while "text" (text and names) are converted to strings:
<<Transform individual items>>=
item(atom_rest(A)) -->
	rest(Codes),
	{	atom_codes(A, Codes)
	}.
item(string_rest(Str)) -->
	rest(Codes),
	{	string_codes(Str, Codes)
	}.
@

We assume that space is represented by a single "space" character; the "Hacker's Guide" is not explicit about this, but so far it has always worked.
<<>>=
item(space) -->
	[0'\s]. % could it be another "white"?...
@

Integers are used to enumerate the code and documentation chunks:
<<>>=
item(chunk_number(N)) -->
	integer(N).
@

Code and documentation chunks are delimited by the same keywords; the type of chunk is encoded in a secondary keyword:
<<>>=
item(chunk_kind(CK)) -->
	nonblanks(Codes),
	{	atom_codes(CK, Codes),
		chunk_kind(CK) % validation; is it necessary?
	}.
@

Finally, cross-reference: it employs quite a few secondary keywords.
These are collected in their own look up table.
<<>>=
item(xref(XRef)) -->
	nonblanks(Codes),
	{	atom_codes(X, Codes),
		xref(X, Items, XRef)
	},
	items(Items).
@

Two helper DCGS are used so far.
The first one just takes the rest of the input; the other one is used for validating the chunk kind.
<<Helper DCGs>>=
rest([]) --> [].
rest([C|Cs]) -->
	[C],
	rest(Cs).

chunk_kind(code). chunk_kind(docs).
@

## Pipeline Finite State Machine
To represent a finite state machine in Prolog we use predicates for states and predicate clauses for arcs (transitions), another idea borrowed from "The Craft of Prolog" [@OKeefe1990, pp. 353].
The finite state machine is initialized by [[nwpipeline_fsm/0]].
The input to the state machine are the terms representing the `noweb` tokens, obtained by consecutive calls to [[next_input/1]].
<<Pipeline FSM>>=
nwpipeline_fsm :-
	next_input(T),
	start(T).
% States
<<Start>>
<<File>>
<<Begin chunk>>
<<Docs chunk>>
<<Quoted code>>
<<Code chunk>>
% etc
@

The pipeline representation consists of one or more files:
<<Start>>=
start(file(_File)) :-
	next_input(T),
	file(T).
@

<<File>>=
file(begin(Kind, N)) :-
	begin_chunk(Kind, N).
file(file(_File)) :-
	next_input(T),
	file(T).
file(end_of_file).
@

[[<<Begin chunk>>]] is not a real state, since the transition happens without any input; it is here to make the transition deterministic:
<<Begin chunk>>=
begin_chunk(docs, N) :-
	next_input(T),
	docs(T, N).
begin_chunk(code, N) :-
	code_begin(N, Code),
	next_input(T),
	code(T, Code, []).
@

Since the documentation chunks are expected to be valid Pandoc markdown, the contents are simply emitted unchanged to the output:
<<Docs chunk>>=
docs(end(docs, N), N) :-
	next_input(T),
	file(T).
docs(nl, N) :-
	nl,
	next_input(T),
	docs(T, N).
docs(text(Text), N) :-
	format("~s", [Text]),
	next_input(T),
	docs(T, N).
docs(quote, N) :-
	code_begin(N, Quoted),
	next_input(T),
	quote(T, Quoted, []).
@

For both quoted (inline) code and code chunks, the contents are first collected before emitted at the end of the quote or code chunk.
Since there are enough similarities between quoted code and proper code chunks, we use the same [[<<Data structure for code>>]].
It allows us to keep the state machine a bit cleaner.
<<Quoted code>>=
<<Data structure for code>>
quote(endquote, Quoted, []) :-
	code_end(N, Quoted),
	code_content(Quoted, Content),
	emit(quote(Content)),
	next_input(T),
	docs(T, N).
quote(text(Text), Quoted0, XRef) :-
	code_add(text(Text), Quoted0, Quoted),
	next_input(T),
	quote(T, Quoted, XRef).
quote(nl, Quoted0, XRef) :-
	code_add(nl, Quoted0, Quoted),
	next_input(T),
	quote(T, Quoted, XRef).
quote(use(Name), Quoted0, XRef) :-
	code_use([name(Name)|XRef], Quoted0, Quoted),
	next_input(T),
	quote(T, Quoted, []).
quote(xref(XRef), Quoted, XRef_rest) :-
	next_input(T),
	quote(T, Quoted, [XRef|XRef_rest]).
@

Code chunks are numbered, have a name, can contain uses, can be labelled and referenced, and so on:
<<Code chunk>>=
code(end(code, N), Code, []) :-
	code_end(N, Code),
	emit(Code),
	next_input(T),
	file(T).
code(xref(XRef), Code, XRef_rest) :-
	next_input(T),
	code(T, Code, [XRef|XRef_rest]).
code(defn(Name), Code0, XRef) :-
	code_defn([name(Name)|XRef], Code0, Code),
	next_input(nl), % must be here! (says the "Hacker's Guide")
	next_input(T),
	code(T, Code, []).
code(use(Name), Code0, XRef) :-
	code_use([name(Name)|XRef], Code0, Code),
	next_input(T),
	code(T, Code, []).
code(nl, Code0, XRef) :-
	code_add(nl, Code0, Code),
	next_input(T),
	code(T, Code, XRef).
code(text(Text), Code0, XRef) :-
	code_add(text(Text), Code0, Code),
	next_input(T),
	code(T, Code, XRef).
@

To avoid having too many arguments in the predicates of the [[<<Pipeline FSM>>]], we define a few help predicates that serve as an interface to the data structure (Prolog term) that is used to represent a code chunk.
The attributes of a chunk, or uses inside a chunk, are maintained as Prolog _records_.
To allow for efficient appending to the end of the list of contents, it is kept as a _difference list_ pair:
<<Data structure for code>>=
:- use_module(library(record)).

:- record
	ccattr(
		number:integer,
		name:string,
		label:atom,
		ref:atom),
	use(
		name:string,
		label:atom,
		ref:atom).

code_begin(N, codechunk(Attr, Content, Content)) :-
	make_ccattr([number(N)], Attr).
code_defn(NameXRef,
		codechunk(Attr0, Content, Content),
		codechunk(Attr, Content, Content)) :-
	set_ccattr_fields(NameXRef, Attr0, Attr).
code_use(UseAttr,
		codechunk(Attr, Content, [use(Use)|Rest]),
		codechunk(Attr, Content, Rest)) :-
	make_use(UseAttr, Use).
code_add(New,
		codechunk(Attr, Content, [New|Rest]),
		codechunk(Attr, Content, Rest)).
code_content(codechunk(_, Content, []), Content).
code_end(N, codechunk(Attr, _, [])) :-
	ccattr_number(Attr, N).
@

## Writing Pandoc's Markdown
Unfortunately, Pandoc's markdown is not expressive enough to allow code chunks in the literate program to be expressed in pure markdown.
Instead, native markup for each supported front end is used.
The writer is implemented as a single predicate, [[emit/1]], with one clause for each possible output, as produced by the finite state machine.
There is an ugly help predicate, [[through_pandoc/3]], which uses Pandoc to generate correct markup for the layout of elements that are unusual for normal documents, but necessary for a literate program.
At the moment, only a Pandoc document that can be compiled to an HTML page is generated.
<<Emit Pandoc's markdown>>=
<<The ugliness called `through_pandoc/3`>>
<<Emit HTML for Pandoc>>
@

### Emitting HTML
A Prolog library is used for emitting HTML.
Code chunks are put in a `<div>` tag with a class `codechunk`, with `noweb`'s `defn` at the top, inside its own `<span>` tag with a class `nwdefn`, and all content in a `<pre>` tag.
Quoted code is put in a `<span>` with a class `quotedcode`.
<<Emit HTML for Pandoc>>=
:- use_module(library(http/html_write)).

emit(codechunk(Attr, C, _)) :-
	ccattr_name(Attr, Name),
	ccattr_label(Attr, Label),
	ccattr_ref(Attr, Ref),
	name_ref(Name, Ref, Name_ref),
	Defn = span([class=nwdefn, id=Label],
		[	&(lang), &(nbsp),
			Name_ref,
			&(nbsp), &(rang), &(equiv)
		]),
	maplist(codechunk_html, C, Content),
	phrase(html(div(class=codechunk, [Defn, pre(Content)])), HTML),
	print_html(HTML).
emit(quote(Q)) :-
	maplist(quotedcode_html, Q, Content),
	phrase(html(span(class=quotedcode, Content)),
		HTML),
	print_html(HTML).

name_ref(Name, Ref, Name_ref) :-
	(	dif(Ref, 'nw@notdef')
	->	Name_ref = a(href="#~a"-Ref, Name)
	;	Name_ref = Name
	).

<<`Noweb` $\rightarrow$ HTML>>
@

Transforming the Prolog terms representing code to HTML could be quite straight-forward: just map the `noweb` token to an HTML tag with the appropriate attributes.
For quoted code, this is good enough:
<<`Noweb` $\rightarrow$ HTML>>=
quotedcode_html(nl, '\n').
quotedcode_html(text(Text), Text).
quotedcode_html(use(Use), span(class=nwquoteduse,
		[	&(lang), &(nbsp),
			Name_ref,
			&(nbsp), &(rang)
		])) :-
	use_ref(Use, Ref),
	use_name(Use, Name),
	name_ref(Name, Ref, Name_ref).
@
Apparently, Pandoc does recognize markdown inside `<span>` contents, and adds the necessary HTML markup.

For example, we can put a "use" inside quoted code, complete with its own Pandoc markdown, as in:
<<foo>>=
[[quoted @<<foo $\rightarrow \pi =$ `bar`@>>]]
@ Which will turn into: [[quoted <<foo $\rightarrow \pi =$ `bar`>>]]

Unfortunately, it does not do that for the contents of `<span>` which is inside a `<pre>` (I guess rightfully so?).
<<`Noweb` $\rightarrow$ HTML>>=
codechunk_html(nl, '\n').
codechunk_html(text(Text), Text).
codechunk_html(use(Use),
		span([class=nwuse, id=Label], [
				&(lang), &(nbsp),
				a(href="#~a"-Ref, \[F]), % add directly to the output
				&(nbsp), &(rang)])) :-
	use_name(Use, Name),
	use_label(Use, Label),
	use_ref(Use, Ref),
	through_pandoc(html5, Name, F).
@

### Ad-hoc markup with Pandoc
To make the contents of code chunks look like code, they are put inside a `<pre>` tag.
But in a `lire` source file, a code chunk can contain a [[<<Use>>]], which will have to be typeset properly.
Pandoc will not do this automatically for a `<span>` inside raw HTML `<pre>`.
The hacky solution here is to pass the Name in a [[<<Use>>]] (formatted in markdown) to Pandoc, and embed the resulting markup in the output.
Here is how it is done:

#. Turn single quotes -- "`'`" -- into "`'\''`" sequences;
#. Construct the pipe;
#. Call it as a `bash` process;
#. Parse the output, removing the enclosing `<p>`;
#. Convert back to HTML
<<The ugliness called `through_pandoc/3`>>=
:- use_module(library(sgml), [load_structure/3]).
:- use_module(library(sgml_write), [xml_write/3]).

through_pandoc(To, Str, FStr) :-
	atomics_to_string(L, "'", Str), % split
	atomics_to_string(L, "'\\''", EscStr), % put together
	atomics_to_string(["echo '", EscStr, "' | pandoc -t ", To], Pandoc),
	process_create(path(bash), ['-c', Pandoc], [stdout(pipe(Out))]),
	through_pandoc_1(To, Out, FStr),
	close(Out).

through_pandoc_1(html5, Out, FStr) :-
	load_structure(Out, [element(p, [], StrDOM)], [space(preserve)]),
	with_output_to(string(FStr),
		xml_write(current_output, StrDOM, [header(false), layout(false)])).
@

# Layout
The system tries to separate content and layout as much as possible.
It also aims to provide a sensible default layout for the human-readable documentation.

## HTML
For HTML documentation, a CSS file is used. Here it is:
<<lire.css>>=
h1, h2, h3 {
	font-family: sans-serif;
}
p {
	max-width: 17cm;
}
ul {
	max-width: 15cm;
}
body {
	padding-left: 3cm;
}
span.quotedcode {
	font-family: monospace;
}
span.nwquoteduse {
	font-family: serif;
}
span.nwuse {
	font-family: serif;
	font-style: italic;
}
span.nwdefn {
	font-family: serif;
	font-weight: bold;
}
@

# Common definitions
All building block for `noweb` (front ends, filters, back ends) are usually installed in the same place:
<<`Noweb` location>>=
/usr/lib/noweb
@

The portable Bash shebang is, according to [Stack Overflow](http://stackoverflow.com/questions/10376206/preferred-bash-shebang):
<<Bash shebang>>=
#! /usr/bin/env bash
@

# References

