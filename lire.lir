---
title: 'Reproducible computing with `lire`'
author: Boris Vassilev
version: 0.3
rights: (c) 2015 Boris Vassilev, University of Helsinki
...

# Introduction
This is a collection of tools that support programming language-agnostic reproducible computing.

This document is self-hosting: the implementation presented here can be used to produce the final human-readable documentation.
This makes informal validation trivial ("does the final document look right?"), but it also means that the only guarantee is that it implements enough to be able to compile itself.

# Overview
The tools provided by `lire` aim to support two goals:

1. Generating results by running the code defined in a `lire` file.
2. Presenting all code and possibly results in a human-readable form.

The program installed as the bash script [[<<lire>>]] that takes the `lire` source file name as its only argument and performs three steps:
<<lire.sh>>=
<<Bash shebang>>
<<`Lire` setup>>
# step 1:
<<Extract code from the `lire` source file>>
# step 2:
<<Run all code>>
# step 3:
### Weave to HTML ###
make -C .lire html
cp .lire/*.html .
@

`Lire` maintains a "state" in order to avoid unnecessary computation in a subdirectory named `.lire`.
The `--parents` option is used so that `mkdir` does nothing _and_ does not report an error if the directory already exists.
<<`Lire` setup>>=
mkdir --parents .lire
@

# Extract code from the `lire` source file
"Code chunks" inside a `lire` source file form a ["forest"](https://en.wikipedia.org/wiki/Tree_%28graph_theory%29#forest) of trees (directed, acyclic graphs).
The tools provided by `noweb` will not allow circular code chunk definitions, so `lire` does not need to worry about this.
<<Extract code from the `lire` source file>>=
<<Get names of code chunks to be tangled>>
<<Tangle code chunks>>
<<Take in use externally maintained files>>
@
Not everything fits in a single source file.
A bibliography file, for example, is best maintained by a proper reference manager.
Such files can be listed explicitly in a code chunk named `lire::use` like so:
<<lire::use>>=
lire.bib
@

## Tangling code
`Lire` makes the assumption that all root chunks are source files that are meant to be tangled and eventually used.
The list is then sorted; this is necessary for the next step of filtering out explicitly ignored chunk names.
<<Get names of code chunks to be tangled>>=
allnames=$(mktemp)
noroots "$1" \
    | <<Drop double angle brackets>> \
    | <<Remove empty lines>> \
    | <<Remove names starting with `lire::`>> \
    | sort -u \
    > "$allnames"
<<Filter out explicitly ignored chunk names>>
@

One implicitly ignored chunk is the "anonymous", unnamed code chunk as in [[@<<>>=]]; this is interpreted as a continuation of the previous chunk by `lire`, but [[@<<>>=]] will appear in the list of root chunks as listed by `noweb`'s own `noroots`.
A `lire` source file can also contain root chunks that have names beginning with `lire::`.
These are used by the user to customize `lire`'s functionality, as for example the [[<<lire::use>>]] chunk mentioned above.
<<Drop double angle brackets>>=
sed -n 's/^@<<\(.*\)@>>$/\1/p'
@
<<Remove empty lines>>=
sed '/^\s*$/d'
@
<<Remove names starting with `lire::`>>=
sed -n '/^lire::.*/!p'
@

If for some reason a root chunk should not be tangled, it can be listed in a chunk named `lire::ignore`.
<<lire::ignore>>=
@
The chunk `lire::ignore` is tangled to a temporary file and any names in it are subtracted from the list of root chunk names obtained in the previous step.
Since the command line tool used to do this, `comm`, expects sorted input files, it is sorted as well.
<<Filter out explicitly ignored chunk names>>=
ignored=$(mktemp)
notangle -R"lire::ignore" "$1" \
    | <<Remove empty lines>> \
    | sort -u \
    > "$ignored"
tangled=$(mktemp)
comm -23 "$allnames" "$ignored" > "$tangled"
rm "$allnames" "$ignored"
@
At that point, the names of root chunks that are meant to tangled are in the file `$tangled`, one name per line, without empty lines.

It is not feasible to simply re-run all programs in an analysis pipeline every time we change a small part of it.
Similarly, it would be grossly inefficient to re-run code simply because the documentation in the source file has changed.
One established approach to dealing with dependencies is to use the `make` utility and provide the project with a `Makefile`.
<<Makefile>>=
all : nwpipe-pandoc
.PHONY : all

@

However, this approach does not work out of the box with a literate source file:
`make` has no way of knowing that an irrelevant _part_ of a file has changed.
To circumvent this, when code chunks are tangled, their contents are compared to already existing tangled files.
Files are only replaced if their contents have changed.
<<Tangle code chunks>>=
while IFS='' read -r chunkname
do
    f=$(mktemp)
    notangle -t8 -filter emptydefn -R"$chunkname" lire.lir > "$f"
    cmp --quiet "$f" .lire/"$chunkname"
    if [ $? -eq 0 ]; then
        rm "$f"
    else
        mv "$f" .lire/"$chunkname" && echo "Tangling : $chunkname"
    fi
done < "$tangled"
rm "$tangled"
@

## Using externally maintained files
We want to allow for externally maintained files to be part of a project.
There are two reasons why it is better to explicitly mention these files inside the `lire` source file:
1. For posterity;
2. For managing dependencies

An externally maintained file has to be mentioned explicitly by file name, on its own line, in a code chunk called `lire::use`.
A special case is the `lire` source file itself; its name is appended to the list of used files.
<<Take in use externally maintained files>>=
used=$(mktemp)
notangle -R"lire::use" "$1" \
    | <<Remove empty lines>> \
    > "$used"
echo "$1" >> "$used"
while IFS='' read -r usedname
do
    cmp --quiet "$usedname" .lire/"$usedname"
    if [ $? -ne 0 ]; then
        cp "$usedname" .lire/"$usedname" && echo "Using : $usedname"
    fi
done < "$used"
rm "$used"
@

# Run all code
To run all code necessary to create all results we need, we use the `make` utiliy by calling it inside `.lire`:
<<Run all code>>=
make -C .lire all
@
For now, one must define a [[<<Makefile>>]] in the `lire` source file.
It _should_ be possible to generate the `Makefile`.
This is on the TODO list.

# Generate the human-readable documentation
In its first incarnation, it consist only of a [Pandoc](http://johnmacfarlane.net/pandoc/) back end for [Noweb](https://www.cs.tufts.edu/~nr/noweb/) [@Noweb2008].
It is a bash script that will read from standard input.
Any command line arguments passed to the script will be used as additional arguments to the invocation of `pandoc`.
<<Weave to HTML>>=
<<`Lire` source file $\rightarrow$ `noweb`'s pipeline representation>> \
    | <<`Noweb`'s pipeline representation $\rightarrow$ "valid" Pandoc>> \
    | <<Pandoc $\rightarrow$ HTML>>
@

A custom `noweb` pipeline is used, as we definitely want to be able to use "anonymous" chunks as continuations of the previous chunk.
<<`Lire` source file $\rightarrow$ `noweb`'s pipeline representation>>=
<<`Noweb` location>>/markup \
    | <<`Noweb` location>>/emptydefn \
    | <<`Noweb` location>>/noidx -delay
@

To make it marginally easier to deal with the pipeline representation, we completely remove any empty text tokens from it before converting it to Pandoc using [[<<nwpipe-pandoc.pl>>]].
<<`Noweb`'s pipeline representation $\rightarrow$ "valid" Pandoc>>=
sed -n '/^@text $/!p' \
    | <<`Lire` location>>/nwpipe-pandoc
@
<<`Lire` location>>=
"$HOME"/lib/lire
@

<<Pandoc $\rightarrow$ HTML>>=
pandoc \
    --table-of-contents \
    --standalone \
    --self-contained \
    --smart \
    --to=html5 \
    --output=- \
    --bibliography=lire.bib \
    --css=lire.css
@

We use the GNU `make` utility to generate the human readable documentation and generate any files that are required.
<<Makefile>>=
html : lire.lir
	lire-weave lire.lir lire.html
.PHONY : html

@

<<lire-weave>>=
<<lire-weave.sh>>
@
<<lire-weave.sh>>=
<<Bash shebang>>

< "$1" \
    <<Weave to HTML>> \
    > "$2"
@


The program [[<<nwpipe-pandoc.pl>>]] is written in Prolog, using the SWI-Prolog implementation [@Wielemaker2012].
It is written as a stand-alone command line program: once compiled, can be run from the command line as a part of a pipe.
It uses the module [[<<driver.pl>>]] to do the actual work.
<<nwpipe-pandoc.pl>>=
% no messages of type `informational` or `banner`
:- set_prolog_flag(verbose, silent).

:- use_module(driver, [nwpipeline_pandoc/1]).
main :-
    current_prolog_flag(argv, Argv), % command line arguments
    prompt(_Old, ''), %  turn off prompt
    nwpipeline_pandoc(Argv),
    <<Warn about non-determinism>>,
    halt. % exit with success
main :-
    format(user_error, "FAILURE~n", []),
    halt(1). % otherwise, exit with failure (1)
@
<<Makefile>>=
nwpipe-pandoc : nwpipe-pandoc.pl driver.pl nwpipe.pl lirehtml.pl
	swipl --goal=main -o nwpipe-pandoc -c nwpipe-pandoc.pl

@

During development, it is good to know if the program has left behind any unintentional choice points -- they are almost certainly errors.
They would be "hidden" by the `halt/0` used at the end of the main program.
<<Warn about non-determinism>>=
deterministic(D),
(   D == false
->  format(user_error, "DANGLING CHOICE POINT(S)!~n", [])
;   true
)
@

## From `lire` source to documentation

The driver uses [[<<nwpipe.pl>>]] and [[<<lirehtml.pl>>]].
<<driver.pl>>=
:- module(driver, [nwpipeline_pandoc/1]).

:- use_module(nwpipe, [nwtokens_terms/2, lire//1]).
:- use_module(lirehtml, [emit_html/2]).

nwpipeline_pandoc(_) :-
    I = user_input,
    input_to_terms(I, Ts),
    open('terms.txt', write, Terms_file),
    forall(member(T, Ts), format(Terms_file, "~q~n", [T])),
    close(Terms_file),
    phrase(lire(L), Ts),
    setup_call_cleanup(open('doc.txt', write, Doc_file),
            (   maplist(format_to_string("~q~n"), L, LS),
                atomics_to_string(LS, S),
                format(Doc_file, "~s~n", [S])
            ),
            %write_lire(L, Doc_file),
            close(Doc_file)),
    setup_call_cleanup(open('weaved.pandoc', write, Pandoc_file),
            emit_html(Pandoc_file, L),
            close(Pandoc_file)),
    emit_html(user_output, L).

format_to_string(F, T, S) :-
    format(string(S), F, [T]).

write_lire([], _).
write_lire([H|T], Out) :-
    format(Out, "~q~n", [H]),
    write_lire(T, Out).

input_to_terms(I, Ts) :-
    read_string(I, _, S),
    split_string(S, "\r\n", "\r\n", Str_Ls),
    maplist(string_codes, Str_Ls, Ls),
    nwtokens_terms(Ls, Ts).
@

<<lirehtml.pl>>=
:- module(lirehtml, [emit_html/2]).

:- use_module(library(http/html_write)).
:- multifile html_write:expand//1.

emit_html(Out, L) :-
    formatting(L, F),
    phrase(lire_pandoc(P), F),
    phrase(html(P), H),
    print_html(Out, H).
@

Add to the database a table with all code chunks.
<<>>=
formatting(L, L) :-
    code_metas(L, Ms),
    retractall(label_nr_name_fname(_,_,_,_)),
    forall(nth1(N, Ms, M),
            (   through_pandoc(html5, M.name, FN),
                assertz(label_nr_name_fname(M.label, N, M.name, FN))
            )).

code_metas([], []).
code_metas([H|T], Ms) :-
    (   H = code_meta(_, M)
    ->  Ms = [M|Ms0]
    ;   Ms = Ms0
    ),
    code_metas(T, Ms0).
@

Use Pandoc to pre-format chunk names.
<<>>=
:- use_module(library(sgml), [load_structure/3]).
:- use_module(library(sgml_write), [xml_write/3]).

through_pandoc(To, Str, FStr) :-
	atomics_to_string(L, "'", Str), % split
	atomics_to_string(L, "'\\''", EscStr), % put together
	atomics_to_string(["echo '", EscStr, "' | pandoc -t ", To], Pandoc),
	process_create(path(bash), ['-c', Pandoc], [stdout(pipe(Out))]),
	through_pandoc_1(To, Out, FStr),
	close(Out).

through_pandoc_1(html5, Out, FStr) :-
	load_structure(Out, [element(p, [], StrDOM)], [dialect(xml)]),
	with_output_to(string(FStr),
		xml_write(current_output, StrDOM,
			[header(false), layout(false)])).

lire_pandoc(P) -->
    [X], !,
    lire_pandoc(X, P).
lire_pandoc([]) --> [].
lire_pandoc(nl, ["\n"|P]) --> lire_pandoc(P).
lire_pandoc(text(T), [T|P]) --> lire_pandoc(P).
lire_pandoc(quote(Q), [span(class(quote), QC)|P]) -->
    {   phrase(lire_pandoc(QC), Q)
    },
    lire_pandoc(P).
lire_pandoc(quote_use(Name, Label),
            [span(class(quoteduse),
                  [&(lang), \thinnbsp,
                   a(href("#~a"-Label), Name),
                   \thinnbsp, &(rang)])|P]) -->
    lire_pandoc(P).
lire_pandoc(code_meta(C, M),
            [div(class(codechunk),
             [\chunk_defn(M),
              \chunk_uses(M),
              \chunk_defs(M),
              \chunk_prev(M),
              pre(CC),
              \chunk_next(M)])|P]) -->
    {   phrase(lire_pandoc(CC), C)
    },
    lire_pandoc(P).
lire_pandoc(code_use(L, R, N),
            [span(class(embeddeduse),
                  [&(lang), \thinnbsp,
                   a([id(L), href("#~a"-R)], \[FN]),
                   \thinnbsp, &(rang)])|P]) -->
    {   label_nr_name_fname(R, _, N, FN)
    },
    lire_pandoc(P).
lire_pandoc(chunks_list(_Cs), P) -->
/*
            ["# List of Chunks\n\n",
             div(class(listofchunks), \chunks_list(Cs))|P]) -->
*/
    lire_pandoc(P).
lire_pandoc(index(_), P) --> [], lire_pandoc(P).

thinnbsp --> html(span(style("white-space:nowrap"), &(thinsp))).

chunk_defn(M) -->
    {   label_nr_name_fname(M.label, Nr, _, FName)
    },
    html(span([class(defn), id(M.label)],
              [span(class(chunknr), "C~d:"-Nr), &(nbsp),
               &(lang), \thinnbsp,
               a(href("#~a"-M.label), \[FName]),
               \thinnbsp, &(rang), &(equiv)])).
chunk_uses(M) -->
    {   code{uses:Us} :< M
    }, !,
    html(span(class(chunkuses), \chunk_uses_(Us))).
chunk_uses(_) --> [].
chunk_uses_([U|Us]) -->
    {   label_nr_name_fname(U, Nr, _, _)
    }, !,
    html([br([]), "used by ", a(href("#~a"-U),"C~d"-Nr)]),
    chunk_uses_rest(Us).
chunk_uses_(notused) --> html([br([]), "root chunk"]).
chunk_uses_rest([]) --> [].
chunk_uses_rest([U|Us]) -->
    {   label_nr_name_fname(U, Nr, _, _)
    },
    html([", ", a(href("#~a"-U), "C~d"-Nr)]),
    chunk_uses_rest(Us).

chunk_next(M) -->
    {   code{next:L} :< M,
        label_nr_name_fname(L, Nr, _, _)
    }, !,
    html(span(class(chunknext),
         a(href("#~a"-L), [&(8681), \thinnbsp, "C~d"-Nr]))).
chunk_next(_) -->
    html(&(9646)).

chunk_prev(M) -->
    {   code{prev:L} :< M,
        label_nr_name_fname(L, Nr, _, _)
    },
    html([br([]),
          span(class(chunkprev),
               a(href("#~a"-L), [&(8679), \thinnbsp, "C~d"-Nr]))]).
chunk_prev(_) --> [].

chunk_defs(M) -->
    {   code{defs:Ds} :< M
    }, !,
    html(span(class(chunkdefs), \chunk_defs_(Ds))).
chunk_defs(_) --> [].

chunk_defs_([D|Ds]) -->
    {   label_nr_name_fname(D, Nr, _, _)
    },
    html([br([]), "definition continued in ", a(href("#~a"-D), "C~d"-Nr)]),
    chunk_defs_rest(Ds).
chunk_defs_rest([]) --> [].
chunk_defs_rest([D|Ds]) -->
    {   label_nr_name_fname(D, Nr, _, _)
    },
    html(["+", a(href("#~a"-D), "C~d"-Nr)]),
    chunk_defs_rest(Ds).
@

Parsing the pipeline:
<<nwpipe.pl>>=
:- module(nwpipe, [nwtokens_terms/2, lire//1]).

nwtokens_terms([], []).
nwtokens_terms([[0'@|Token]|Tokens], [Term|Terms]) :-
    phrase(token(Back, Term), Token, Back),
    %format(user_error, "~q~n", [Term]),
    nwtokens_terms(Tokens, Terms).

<<`Noweb` tokens $\rightarrow$ Prolog terms>>
<<Build the document structure>>
@

<<`Noweb` tokens $\rightarrow$ Prolog terms>>=
:- use_module(library(dcg/basics), [nonblanks//1, integer//1]).
token(Back, Term) -->
    nonblanks(NB),
    {   atom_codes(Key, NB),
        keyword(Key, Items, Back, Term)
    },
    items(Items).
<<Structural keywords>>
<<Tagging keywords>>

<<Individual items>>
@

<<Structural keywords>>=
keyword(begin, [space, chunk_kind(K)]/*, space, chunk_number(N)]*/, _, K).
keyword(end, []/*[space, chunk_kind(_K), space, chunk_number(N)]*/, _, end).
keyword(text, [space, string_rest(S, Rest)], Rest, text(S)).
keyword(nl, [], [], nl).
keyword(defn, [space, string_rest(S, Rest)], Rest, defn(S)).
keyword(use, [space, string_rest(S, Rest)], Rest, use(S)).
keyword(quote, [], [], quote).
keyword(endquote, [], [], endquote).
@

For the [[<<Tagging keywords>>]], most are represented in `noweb`'s pipeline by a single keyword, `xref`.
There are several categories of cross-referencing concepts.
<<Tagging keywords>>=
keyword(file, [space, atom_rest(File, Rest)], Rest, file(File)).
keyword(xref, [space, xref(XRef, Rest)], Rest, XRef).
keyword(index, [space, index(Index, Rest)], Rest, Index).

<<Basic cross-referencing>>
<<Linking previous and next definitions of a code chunk>>
<<Continued definitions of the current chunk>>
<<The list of all code chunks>>
<<Chunks where the code is used>>

<<The index of identifiers>>
@

<<Basic cross-referencing>>=
xref(label, [space, atom_rest(L, Rest)], Rest, xref_label(L)).
xref(ref, [space, atom_rest(L, Rest)], Rest, xref_ref(L)).
@

<<Linking previous and next definitions of a code chunk>>=
xref(prevdef, [space, atom_rest(L, Rest)], Rest, xref_prevdef(L)).
xref(nextdef, [space, atom_rest(L, Rest)], Rest, xref_nextdef(L)).
@

<<Continued definitions of the current chunk>>=
xref(begindefs, [], [], xref_begindefs).
xref(defitem, [space, atom_rest(L, Rest)], Rest, xref_defitem(L)).
xref(enddefs, [], [], xref_enddefs).
@

<<Chunks where the code is used>>=
xref(beginuses, [], [], xref_beginuses).
xref(useitem, [space, atom_rest(L, Rest)], Rest, xref_useitem(L)).
xref(enduses, [], [], xref_enduses).
xref(notused, [space, string_rest(Name, Rest)], Rest, xref_notused(Name)).
@

<<The list of all code chunks>>=
xref(beginchunks, [], [], xref_beginchunks).
xref(chunkbegin,
    [space, atom(L), space, string_rest(Name, Rest)],
    Rest,
    xref_chunkbegin(L, Name)).
xref(chunkuse, [space, atom_rest(L, Rest)], Rest, xref_chunkuse(L)).
xref(chunkdefn, [space, atom_rest(L, Rest)], Rest, xref_chunkdefn(L)).
xref(chunkend, [], [], xref_chunkend).
xref(endchunks, [], [], xref_endchunks).
@

<<The index of identifiers>>=
index(beginindex, [], [], index_beginindex).
index(endindex, [], [], index_endindex).
@

These are the rules used to transform [[<<Individual items>>]] that appear in the `noweb` tokens to Prolog terms.
<<Individual items>>=
items([]) --> [].
items([I|Is]) -->
    item(I),
    items(Is).

<<Atoms and text>>
<<"Space">>
<<Chunk number>>
<<Chunk kind>>
<<Cross-reference>>
@

Identifiers are converted to atoms, while "text" (text and names) are converted to strings:
<<Atoms and text>>=
item(atom(A)) -->
    nonblanks(Codes),
    {   atom_codes(A, Codes)
    }.

% Using when here is probably an ugly hack.
% Could not figure out a better way to deal with
% "rest of line" situations.
item(atom_rest(A, Rest)) -->
    {   when(ground(Rest), atom_codes(A, Rest))
    }.

item(string_rest(Str, Rest)) -->
    {   when(ground(Rest), string_codes(Str, Rest))
    }.
@

We assume that [[<<"Space">>]] is represented by a single "space" character; the "Hacker's Guide" is not explicit about this, but so far it has always worked.
<<"Space">>=
item(space) --> [0'\s]. % could it be another "white"?...
@

Integers are used to enumerate the code and documentation [[<<Chunk number>>]]:
<<Chunk number>>=
item(chunk_number(N)) --> integer(N).
@

Code and documentation chunks are delimited by the same keywords; the [[<<Chunk kind>>]] is encoded in a secondary keyword:
<<Chunk kind>>=
item(chunk_kind(CK)) -->
    nonblanks(Codes),
    {   atom_codes(CK, Codes)
    }.
@

Finally, [[<<Cross-reference>>]].
Note that it employs quite a few secondary keywords collected in their own look up table.
<<Cross-reference>>=
item(xref(XRef, Rest)) -->
    nonblanks(Codes),
    {   atom_codes(X, Codes),
        xref(X, Items, Rest, XRef)
    },
    items(Items).
item(index(Index, Rest)) -->
    nonblanks(Codes),
    {   atom_codes(X, Codes),
        index(X, Items, Rest, Index)
    },
    items(Items).
@

<<Build the document structure>>=
lire(L) -->
    [file(_)],
    lire_rest(L).

lire_rest(L) -->
    [X], !,
    lire_file(X, L).
lire_rest([]) --> [].

lire_file(docs, L) -->
    [X],
    docs(X, L).
lire_file(code, [code_meta(C, M)|L]) -->
    code(C, M),
    lire_rest(L).
lire_file(nl, [nl|L]) -->
    lire_rest(L).
lire_file(xref_beginchunks, [chunks_list(Cs)|L]) -->
    [X],
    xref_chunks(X, Cs),
    lire_rest(L).
lire_file(index_beginindex, [index(I)|L]) -->
    [X],
    index_list(X, I),
    lire_rest(L).
@

<<Build the document structure>>=
docs(end, L) -->
    lire_rest(L).
docs(text(T), [text(Text)|L]) -->
    docs_text(Ts),
    {   atomics_to_string([T|Ts], Text)
    },
    [X],
    docs(X, L).
docs(nl, [nl|L]) -->
    [X],
    docs(X, L).
docs(quote, [quote(Q)|L]) -->
    [X],
    quote(X, Q),
    [Y],
    docs(Y, L).

quote(text(T), [text(T)|Q]) -->
    [X],
    quote(X, Q).
quote(xref_ref(L), [quote_use(N, L)|Q]) -->
    [use(N), X],
    quote(X, Q).
quote(endquote, []) --> [].
@

<<Build the document structure>>=
code(Cs, M) -->
    [X],
    defn(X, M_pairs),
    {   dict_create(M, code, M_pairs)
    },
    code_content(Cs).

defn(nl, []) --> [].
defn(xref_label(L), [label(L)|M]) -->
    [X],
    defn(X, M).
defn(xref_ref(L), [ref(L)|M]) -->
    [X],
    defn(X, M).
defn(defn(N), [name(N)|M]) -->
    [X],
    defn(X, M).
defn(xref_notused(_N), [uses(notused)|M]) -->
    [X],
    defn(X, M).
defn(xref_beginuses, [uses(Us)|M]) -->
    [X],
    uses(X, Us, Us),
    [Y],
    defn(Y, M).
defn(xref_prevdef(L), [prev(L)|M]) -->
    [X],
    defn(X, M).
defn(xref_nextdef(L), [next(L)|M]) -->
    [X],
    defn(X, M).
defn(language(L), [language(L)|M]) -->
    [X],
    defn(X, M).
defn(xref_begindefs, [defs(Ds)|M]) -->
    [X],
    defs(X, Ds, Ds),
    [Y],
    defn(Y, M).
@

<<Build the document structure>>=
uses(xref_enduses, _, []) --> [].
uses(xref_useitem(L), Us, [L|Us0]) -->
    [X],
    uses(X, Us, Us0).

defs(xref_enddefs, _, []) --> [].
defs(xref_defitem(L), Ds, [L|Ds0]) -->
    [X],
    defs(X, Ds, Ds0).

code_content([text(Text)|Cs]) -->
    text_token(T), !,
    code_text(Ts),
    {   atomics_to_string([T|Ts], Text)
    },
    code_content(Cs).
code_content([code_use(L, R, N)|Cs]) -->
    [xref_label(L), xref_ref(R), use(N)], !,
    code_content(Cs).
code_content([]) --> [end].
@

<<Build the document structure>>=
text_token("\n") --> [nl].
text_token(T) --> [text(T)].
    
code_text([T|Ts]) -->
    text_token(T), !,
    code_text(Ts).
code_text([]) --> [].

docs_text([T|Ts]) -->
    [text(T)], !,
    docs_text(Ts).
docs_text(["\n"|Ts]) -->
    [nl], !,
    eol(Ts).
docs_text([]) --> [].

eol([T|Ts]) -->
    [text(T)], !,
    docs_text(Ts).
eol([]) --> [].
@

<<Build the document structure>>=
xref_chunks(xref_endchunks, []) --> [].
xref_chunks(xref_chunkbegin(L, N), [chunk(L, N, Us, Ds)|Cs]) -->
    [X],
    xref_chunk(X, Us, Ds),
    [Y],
    xref_chunks(Y, Cs).

xref_chunk(xref_chunkend, [], []) --> [].
xref_chunk(xref_chunkuse(U), [chunkuse(U)|Us], Ds) -->
    [X],
    xref_chunk(X, Us, Ds).
xref_chunk(xref_chunkdefn(D), Us, [chunkdefn(D)|Ds]) -->
    [X],
    xref_chunk(X, Us, Ds).

index_list(index_endindex, []) --> [].
% we don't have an index for now
@

## Layout
The system tries to separate content and layout as much as possible.
It also aims to provide a sensible default layout for the human-readable documentation.

### HTML
For HTML documentation, [[<<lire.css>>]] is used.
<<lire.css>>=
<<Text width and margin>>
<<Headers in sans-serif>>
<<Quoted code in monospace>>
<<Use names formatting>>
<<List of chunks formatting>>
@

The text width is limited and a left margin is inserted to improve readability.
<<Text width and margin>>=
p {
    max-width: 14cm;
}
ul {
    max-width: 12cm;
}
body {
    padding-left: 2cm;
}
@

<<Headers in sans-serif>>=
h1,
h2,
h3 {
    font-family: sans-serif;
}
@

<<Use names formatting>>=
span.chunkdefs a,
span.chunkuses a,
span.chunkprev a,
span.chunknext a,
span.defn a,
span.quoteduse a,
span.embeddeduse a {
    text-decoration-line: None;
}
span.chunkdefs,
span.chunkuses,
span.chunkprev,
span.chunknext {
    font-size: small;
}
span.quoteduse {
    font-family: Serif;
}
span.embeddeduse {
    font-family: Serif;
}
span.chunknr {
    font-weight: bold;
}
@

Use names in quoted code and code chunks are typeset in different colors and are high-lighted when the mouse is over them.
<<>>=
span.quoteduse a {
    color: DarkGreen;
}
span.quoteduse a:hover {
    color: LimeGreen;
}
span.embeddeduse a {
    color: DarkBlue;
    font-style: Italic;
}
span.embeddeduse a:hover {
    color: DodgerBlue;
}
span.chunkdefs a,
span.chunkprev a,
span.chunknext a,
span.defn a,
span.chunkuses a {
    color: DarkRed;
}
span.chunkdefs a:hover,
span.chunkprev a:hover,
span.chunknext a:hover,
span.defn a:hover,
span.chunkuses a:hover {
    color: DarkGoldenRod;
}
@

<<Quoted code in monospace>>=
span.quote {
    font-family: Monospace;
}
@

<<List of chunks formatting>>=
ul.chunkslist {
    list-style-type: square;
}
@

# Common definitions
All building block for `noweb` (front ends, filters, back ends) are usually installed in the same [[<<`Noweb` location>>]]:
<<`Noweb` location>>=
/usr/lib/noweb
@

The portable [[<<Bash shebang>>]] is, according to [Stack Overflow](http://stackoverflow.com/questions/10376206/preferred-bash-shebang):
<<Bash shebang>>=
#! /usr/bin/env bash
@

# References

